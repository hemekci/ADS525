{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fec981fc",
   "metadata": {},
   "source": [
    "# Semantic Search & RAG - Medium Tasks\n",
    "\n",
    "Trying out some more advanced search techniques. Building on the basic stuff from earlier.\n",
    "\n",
    "**What we're doing:**\n",
    "- Hybrid search (BM25 + semantic)\n",
    "- Different chunking strategies \n",
    "- Query expansion\n",
    "- Document selection for LLM context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76bb899",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run all cells in this section to set up the environment and load necessary data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee086a5a",
   "metadata": {},
   "source": [
    "### [Optional] - Installing Packages on Google Colab\n",
    "\n",
    "If you are viewing this notebook on Google Colab, uncomment and run the following code to install dependencies.\n",
    "\n",
    "**Note**: Use a GPU for this notebook. In Google Colab, go to Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "2e7d3efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain==0.2.5 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (0.2.5)\n",
      "Requirement already satisfied: faiss-cpu==1.8.0 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (1.8.0)\n",
      "Requirement already satisfied: cohere==5.5.8 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (5.5.8)\n",
      "Requirement already satisfied: langchain-community==0.2.5 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (0.2.5)\n",
      "Requirement already satisfied: rank_bm25==0.2.2 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (0.2.2)\n",
      "Requirement already satisfied: sentence-transformers==3.0.1 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (3.0.1)\n",
      "Requirement already satisfied: pandas in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (2.3.3)\n",
      "Requirement already satisfied: python-dotenv in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (1.2.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from langchain==0.2.5) (6.0.3)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from langchain==0.2.5) (2.0.44)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from langchain==0.2.5) (3.13.2)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from langchain==0.2.5) (4.0.3)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.7 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from langchain==0.2.5) (0.2.43)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from langchain==0.2.5) (0.2.4)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from langchain==0.2.5) (0.1.147)\n",
      "Requirement already satisfied: numpy<2,>=1 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from langchain==0.2.5) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from langchain==0.2.5) (2.12.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from langchain==0.2.5) (2.32.5)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from langchain==0.2.5) (8.5.0)\n",
      "Requirement already satisfied: boto3<2.0.0,>=1.34.0 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from cohere==5.5.8) (1.40.68)\n",
      "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from cohere==5.5.8) (1.12.1)\n",
      "Requirement already satisfied: httpx>=0.21.2 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from cohere==5.5.8) (0.28.1)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from cohere==5.5.8) (0.4.3)\n",
      "Requirement already satisfied: parameterized<0.10.0,>=0.9.0 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from cohere==5.5.8) (0.9.0)\n",
      "Requirement already satisfied: tokenizers<1,>=0.15 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from cohere==5.5.8) (0.22.1)\n",
      "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from cohere==5.5.8) (2.32.4.20250913)\n",
      "Requirement already satisfied: typing_extensions>=4.0.0 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from cohere==5.5.8) (4.15.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from langchain-community==0.2.5) (0.6.7)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from sentence-transformers==3.0.1) (4.57.1)\n",
      "Requirement already satisfied: tqdm in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from sentence-transformers==3.0.1) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from sentence-transformers==3.0.1) (2.9.0)\n",
      "Requirement already satisfied: scikit-learn in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from sentence-transformers==3.0.1) (1.7.2)\n",
      "Requirement already satisfied: scipy in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from sentence-transformers==3.0.1) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from sentence-transformers==3.0.1) (0.36.0)\n",
      "Requirement already satisfied: Pillow in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from sentence-transformers==3.0.1) (12.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.5) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.5) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.5) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.5) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.5) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.5) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.5) (1.22.0)\n",
      "Requirement already satisfied: botocore<1.41.0,>=1.40.68 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from boto3<2.0.0,>=1.34.0->cohere==5.5.8) (1.40.68)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from boto3<2.0.0,>=1.34.0->cohere==5.5.8) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.15.0,>=0.14.0 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from boto3<2.0.0,>=1.34.0->cohere==5.5.8) (0.14.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from botocore<1.41.0,>=1.40.68->boto3<2.0.0,>=1.34.0->cohere==5.5.8) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from botocore<1.41.0,>=1.40.68->boto3<2.0.0,>=1.34.0->cohere==5.5.8) (2.5.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.5) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.5) (0.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.7->langchain==0.2.5) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.7->langchain==0.2.5) (24.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.7->langchain==0.2.5) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.5) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.5) (1.0.0)\n",
      "Requirement already satisfied: anyio in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from httpx>=0.21.2->cohere==5.5.8) (4.11.0)\n",
      "Requirement already satisfied: certifi in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from httpx>=0.21.2->cohere==5.5.8) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from httpx>=0.21.2->cohere==5.5.8) (1.0.9)\n",
      "Requirement already satisfied: idna in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from httpx>=0.21.2->cohere==5.5.8) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.21.2->cohere==5.5.8) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from pydantic<3,>=1->langchain==0.2.5) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from pydantic<3,>=1->langchain==0.2.5) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from pydantic<3,>=1->langchain==0.2.5) (0.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.41.0,>=1.40.68->boto3<2.0.0,>=1.34.0->cohere==5.5.8) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.2.5) (3.4.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.5) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.5) (1.0.0)\n",
      "Requirement already satisfied: anyio in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from httpx>=0.21.2->cohere==5.5.8) (4.11.0)\n",
      "Requirement already satisfied: certifi in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from httpx>=0.21.2->cohere==5.5.8) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from httpx>=0.21.2->cohere==5.5.8) (1.0.9)\n",
      "Requirement already satisfied: idna in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from httpx>=0.21.2->cohere==5.5.8) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.21.2->cohere==5.5.8) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from pydantic<3,>=1->langchain==0.2.5) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from pydantic<3,>=1->langchain==0.2.5) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from pydantic<3,>=1->langchain==0.2.5) (0.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.41.0,>=1.40.68->boto3<2.0.0,>=1.34.0->cohere==5.5.8) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.2.5) (3.4.4)\n",
      "Requirement already satisfied: greenlet>=1 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.2.5) (3.2.4)\n",
      "Requirement already satisfied: filelock in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers==3.0.1) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers==3.0.1) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers==3.0.1) (1.2.0)\n",
      "Requirement already satisfied: greenlet>=1 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.2.5) (3.2.4)\n",
      "Requirement already satisfied: filelock in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers==3.0.1) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers==3.0.1) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers==3.0.1) (1.2.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers==3.0.1) (2025.11.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers==3.0.1) (0.6.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.5) (1.1.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers==3.0.1) (2025.11.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers==3.0.1) (0.6.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.5) (1.1.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.0 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (3.5.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.0 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers==3.0.1) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from anyio->httpx>=0.21.2->cohere==5.5.8) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from anyio->httpx>=0.21.2->cohere==5.5.8) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers==3.0.1) (3.0.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from scikit-learn->sentence-transformers==3.0.1) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from scikit-learn->sentence-transformers==3.0.1) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers==3.0.1) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from anyio->httpx>=0.21.2->cohere==5.5.8) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from anyio->httpx>=0.21.2->cohere==5.5.8) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers==3.0.1) (3.0.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from scikit-learn->sentence-transformers==3.0.1) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from scikit-learn->sentence-transformers==3.0.1) (3.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu124\n",
      "Requirement already satisfied: llama-cpp-python==0.2.78 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (0.2.78)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from llama-cpp-python==0.2.78) (4.15.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from llama-cpp-python==0.2.78) (1.26.4)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from llama-cpp-python==0.2.78) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from llama-cpp-python==0.2.78) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /media/danielcastillo/DanielSSD1/ads525/ads525/lib/python3.10/site-packages (from jinja2>=2.11.3->llama-cpp-python==0.2.78) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "!pip install langchain==0.2.5 faiss-cpu==1.8.0 cohere==5.5.8 langchain-community==0.2.5 rank_bm25==0.2.2 sentence-transformers==3.0.1 pandas python-dotenv\n",
    "!pip install llama-cpp-python==0.2.78  --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu124\n",
    "\n",
    "## IMPORTANT: Make sure to restart the session after installing the packages above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b4a0ef",
   "metadata": {},
   "source": [
    "### Import Libraries and Setup API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "100b5bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get API key from environment variable\n",
    "api_key = os.environ.get('COHERE_API_KEY')\n",
    "\n",
    "# Create Cohere client\n",
    "co = cohere.Client(api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32674c26",
   "metadata": {},
   "source": [
    "## Load Sample Dataset\n",
    "\n",
    "For these tasks, we'll use a dataset with multiple documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "e8e02fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 technical documents\n",
      "Average length: 406 characters\n"
     ]
    }
   ],
   "source": [
    "# Sample technical documents for our search experiments\n",
    "documents = [\n",
    "    \"\"\"API Gateway Architecture: A well-designed API gateway serves as a single entry point \n",
    "    for all client requests. It handles authentication, rate limiting, request routing, and \n",
    "    response transformation. Popular patterns include BFF (Backend for Frontend) where \n",
    "    different gateways serve web vs mobile clients. Circuit breakers prevent cascade \n",
    "    failures when downstream services are unavailable.\"\"\",\n",
    "    \n",
    "    \"\"\"Microservices Security: Securing distributed systems requires defense in depth. \n",
    "    Service-to-service communication should use mutual TLS authentication. JWT tokens \n",
    "    enable stateless authorization with configurable expiration. Zero-trust architecture \n",
    "    assumes no implicit trust between services. Regular security audits identify \n",
    "    vulnerabilities in dependencies and configurations.\"\"\",\n",
    "    \n",
    "    \"\"\"Database Scaling Strategies: Horizontal scaling distributes data across multiple \n",
    "    machines. Sharding partitions data by key ranges or hash functions. Read replicas \n",
    "    handle query workloads while write operations go to primary instances. Eventual \n",
    "    consistency allows systems to remain available during partition events. CQRS \n",
    "    separates read and write models for optimal performance.\"\"\",\n",
    "    \n",
    "    \"\"\"Container Orchestration: Kubernetes automates deployment, scaling, and management \n",
    "    of containerized applications. Pods group related containers that share resources. \n",
    "    Services provide stable networking endpoints for pod communication. ConfigMaps and \n",
    "    Secrets manage configuration data separately from application code. Operators \n",
    "    extend Kubernetes to manage complex stateful applications.\"\"\",\n",
    "    \n",
    "    \"\"\"Cloud Computing Performance: Auto-scaling adjusts resource allocation based on \n",
    "    demand metrics like CPU utilization or request count. Load balancers distribute \n",
    "    traffic across healthy instances. Content delivery networks cache static assets \n",
    "    at edge locations worldwide. Database connection pooling reduces overhead from \n",
    "    frequent connection establishment. Monitoring provides visibility into system health.\"\"\",\n",
    "    \n",
    "    \"\"\"Software Development Lifecycle: Agile methodologies emphasize iterative development \n",
    "    with short feedback cycles. Continuous integration automatically builds and tests \n",
    "    code changes. Feature flags enable gradual rollout of new functionality. Code \n",
    "    reviews improve quality through peer collaboration. Retrospectives help teams \n",
    "    identify process improvements and address technical debt.\"\"\",\n",
    "    \n",
    "    \"\"\"Machine Learning Operations: MLOps bridges development and production for ML systems. \n",
    "    Model versioning tracks changes in algorithms and training data. Automated testing \n",
    "    validates model performance against baseline metrics. Feature stores centralize \n",
    "    data preparation and serve consistent features to training and inference pipelines. \n",
    "    A/B testing compares model performance in production environments.\"\"\",\n",
    "    \n",
    "    \"\"\"Web Application Security: Cross-site scripting (XSS) attacks inject malicious \n",
    "    scripts into web pages. Content Security Policy headers restrict resource loading \n",
    "    to prevent XSS. SQL injection exploits inadequate input validation in database \n",
    "    queries. Parameterized queries safely handle user input. HTTPS encrypts data \n",
    "    in transit while secure session management prevents unauthorized access.\"\"\",\n",
    "    \n",
    "    \"\"\"Data Pipeline Architecture: Extract, Transform, Load (ETL) processes move data \n",
    "    between systems. Stream processing handles real-time data using tools like Apache \n",
    "    Kafka and Apache Flink. Data lakes store raw data in various formats for future \n",
    "    analysis. Data lineage tracking helps understand data flow and impact of changes. \n",
    "    Schema evolution manages changes to data structure over time.\"\"\",\n",
    "    \n",
    "    \"\"\"Mobile Application Development: Native apps provide optimal performance and platform \n",
    "    integration. Cross-platform frameworks like React Native reduce development effort \n",
    "    across iOS and Android. Progressive Web Apps offer app-like experiences using \n",
    "    web technologies. Offline-first design ensures functionality without network \n",
    "    connectivity. Push notifications engage users with timely updates.\"\"\"\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(documents)} technical documents\")\n",
    "print(f\"Average length: {np.mean([len(d) for d in documents]):.0f} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c716aa62",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "\n",
    "These are reusable functions we'll use throughout the tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d993dd68",
   "metadata": {},
   "source": [
    "`bm25_tokenier` prepares the text for BM25 keyword search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3073ea5",
   "metadata": {},
   "source": [
    "Prepare text for BM25 keyword search.\n",
    "    \n",
    "    BM25 needs clean tokens (words) to count matches. This function:\n",
    "    1. Makes everything lowercase (so \"Database\" and \"database\" match)\n",
    "    2. Removes punctuation (so \"cloud,\" becomes \"cloud\")\n",
    "    3. Removes stop words (common words like \"the\", \"is\", \"a\" that don't help search)\n",
    "    4. Only keeps meaningful words\n",
    "    \n",
    "    Example: \"The database is fast\" becomes [\"database\", \"fast\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "8c7767ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_tokenizer(text):\n",
    "    tokenized_doc = []\n",
    "    \n",
    "    # Split text into words and process each one\n",
    "    for token in text.lower().split():\n",
    "        # Remove punctuation from beginning and end\n",
    "        token = token.strip(string.punctuation)\n",
    "        \n",
    "        # Only keep non-empty words that aren't stop words\n",
    "        if len(token) > 0 and token not in _stop_words.ENGLISH_STOP_WORDS:\n",
    "            tokenized_doc.append(token)\n",
    "    \n",
    "    return tokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "d13c7b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(query, results, show_scores=True):\n",
    "    \"\"\"Display search results in a readable format.\"\"\"\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Loop through results and print each one\n",
    "    for i, (doc, score) in enumerate(results, 1):\n",
    "        if show_scores:\n",
    "            print(f\"{i}. [Score: {score:.4f}] {doc}\")\n",
    "        else:\n",
    "            print(f\"{i}. {doc}\")\n",
    "    \n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80760017",
   "metadata": {},
   "source": [
    "## Medium Tasks\n",
    "\n",
    "Complete the following tasks to build production-ready search systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f07977",
   "metadata": {},
   "source": [
    "### Task 1: Hybrid Search \n",
    "\n",
    "Let's try combining keyword search (BM25) with semantic search. Sometimes you need exact keyword matches, sometimes you want meaning-based results.\n",
    "\n",
    "The idea: combine both and see if we get better results than either alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "c6e6cbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings...\n",
      "Got embeddings: (10, 4096)\n",
      "Got embeddings: (10, 4096)\n"
     ]
    }
   ],
   "source": [
    "# Start by getting embeddings for all our docs\n",
    "print(\"Creating embeddings...\")\n",
    "doc_embeddings = co.embed(\n",
    "    texts=documents,\n",
    "    input_type=\"search_document\"\n",
    ").embeddings\n",
    "doc_embeddings = np.array(doc_embeddings)\n",
    "\n",
    "print(f\"Got embeddings: {doc_embeddings.shape}\")\n",
    "# Each doc is now a 4096-dim vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "0ba8d9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape looks right: (10, 4096)\n",
      "Values look normalized: min=-8.125, max=8.141\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape looks right: {doc_embeddings.shape}\")\n",
    "print(f\"Values look normalized: min={doc_embeddings.min():.3f}, max={doc_embeddings.max():.3f}\")\n",
    "# print(doc_embeddings[0][:5])  # peek at first few dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "3f167ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index ready - 10 vectors\n"
     ]
    }
   ],
   "source": [
    "# Build the FAISS index for semantic search\n",
    "dim = doc_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index.add(np.float32(doc_embeddings))\n",
    "\n",
    "print(f\"FAISS index ready - {index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7c85ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "1a84f07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 ready\n"
     ]
    }
   ],
   "source": [
    "# Now set up BM25 for keyword matching\n",
    "tokenized_docs = [bm25_tokenizer(doc) for doc in documents]\n",
    "bm25 = BM25Okapi(tokenized_docs)\n",
    "\n",
    "print(\"BM25 ready\")\n",
    "# This will find exact word matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "4bb86639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test - how many docs have \"security\" keyword?\n",
    "# print(sum(['security' in doc.lower() for doc in documents]))\n",
    "# print(sum(['scale' in doc.lower() for doc in documents]))\n",
    "\n",
    "# Just checking distribution of keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "e4fd445b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: API gateway rate limiting authentication\n",
      "\n",
      "Semantic search results:\n",
      "1. Distance: 10498.3184\n",
      "   API Gateway Architecture: A well-designed API gateway serves as a single entry point \n",
      "    for all cl...\n",
      "\n",
      "2. Distance: 12875.8115\n",
      "   Microservices Security: Securing distributed systems requires defense in depth. \n",
      "    Service-to-serv...\n",
      "\n",
      "3. Distance: 13333.7549\n",
      "   Web Application Security: Cross-site scripting (XSS) attacks inject malicious \n",
      "    scripts into web ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try semantic search first\n",
    "query = \"API gateway rate limiting authentication\"\n",
    "\n",
    "q_emb = co.embed(\n",
    "    texts=[query],\n",
    "    input_type=\"search_query\"\n",
    ").embeddings[0]\n",
    "\n",
    "# Search\n",
    "dists, idxs = index.search(np.float32([q_emb]), 3)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(\"\\nSemantic search results:\")\n",
    "for i, (idx, dist) in enumerate(zip(idxs[0], dists[0]), 1):\n",
    "    print(f\"{i}. Distance: {dist:.4f}\")\n",
    "    print(f\"   {documents[idx][:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "ee17f666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BM25 results:\n",
      "1. Score: 10.1701\n",
      "   API Gateway Architecture: A well-designed API gateway serves as a single entry point \n",
      "    for all cl...\n",
      "\n",
      "2. Score: 1.3230\n",
      "   Microservices Security: Securing distributed systems requires defense in depth. \n",
      "    Service-to-serv...\n",
      "\n",
      "3. Score: 0.0000\n",
      "   Mobile Application Development: Native apps provide optimal performance and platform \n",
      "    integratio...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now try BM25\n",
    "tokenized_q = bm25_tokenizer(query)\n",
    "scores = bm25.get_scores(tokenized_q)\n",
    "\n",
    "top_idxs = np.argsort(scores)[-3:][::-1]\n",
    "\n",
    "print(f\"\\nBM25 results:\")\n",
    "for i, idx in enumerate(top_idxs, 1):\n",
    "    print(f\"{i}. Score: {scores[idx]:.4f}\")\n",
    "    print(f\"   {documents[idx][:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "2440ec6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized BM25: min=0.000, max=1.000\n",
      "Normalized semantic: min=0.000, max=1.000\n"
     ]
    }
   ],
   "source": [
    "# Normalize both scores to 0-1 range so we can combine them\n",
    "bm25_norm = (scores - scores.min()) / (scores.max() - scores.min())\n",
    "\n",
    "# Get all semantic scores\n",
    "all_dists, _ = index.search(np.float32([q_emb]), len(documents))\n",
    "sem_scores = 1 / (1 + all_dists[0])  # convert distance to score\n",
    "sem_norm = (sem_scores - sem_scores.min()) / (sem_scores.max() - sem_scores.min())\n",
    "\n",
    "print(f\"Normalized BM25: min={bm25_norm.min():.3f}, max={bm25_norm.max():.3f}\")\n",
    "print(f\"Normalized semantic: min={sem_norm.min():.3f}, max={sem_norm.max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "516daaf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hybrid (alpha=0.5):\n",
      "1. Score: 1.0000\n",
      "   API Gateway Architecture: A well-designed API gateway serves as a single entry point \n",
      "    for all cl...\n",
      "\n",
      "2. Score: 0.3210\n",
      "   Microservices Security: Securing distributed systems requires defense in depth. \n",
      "    Service-to-serv...\n",
      "\n",
      "3. Score: 0.2190\n",
      "   Database Scaling Strategies: Horizontal scaling distributes data across multiple \n",
      "    machines. Shar...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combine - 50/50 mix\n",
    "alpha = 0.5\n",
    "combined = {}\n",
    "for i, doc in enumerate(documents):\n",
    "    combined[doc] = (1 - alpha) * bm25_norm[i] + alpha * sem_norm[i]\n",
    "\n",
    "sorted_results = sorted(combined.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\nHybrid (alpha={alpha}):\")\n",
    "for i, (doc, score) in enumerate(sorted_results[:3], 1):\n",
    "    print(f\"{i}. Score: {score:.4f}\")\n",
    "    print(f\"   {doc[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "5e342998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: how to make software more secure\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Try different alpha values\n",
    "# Let's see how changing alpha affects results\n",
    "\n",
    "# Let's try different alpha values with a new query\n",
    "query2 = \"how to make software more secure\"\n",
    "\n",
    "print(f\"Query: {query2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "de987a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings and scores for this query\n",
    "q2_emb = co.embed(texts=[query2], input_type=\"search_query\").embeddings[0]\n",
    "q2_tok = bm25_tokenizer(query2)\n",
    "\n",
    "# BM25 scores\n",
    "bm25_sc = bm25.get_scores(q2_tok)\n",
    "bm25_n = (bm25_sc - bm25_sc.min()) / (bm25_sc.max() - bm25_sc.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "d96e967b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense scores\n",
    "d2, _ = index.search(np.float32([q2_emb]), len(documents))\n",
    "sem_sc = 1 / (1 + d2[0])\n",
    "sem_n = (sem_sc - sem_sc.min()) / (sem_sc.max() - sem_sc.min())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "7550fb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "alpha=0 (keywords only):\n",
      "1. Software Development Lifecycle: Agile methodologies emphasize iterative developm...\n",
      "2. Web Application Security: Cross-site scripting (XSS) attacks inject malicious \n",
      " ...\n",
      "3. Mobile Application Development: Native apps provide optimal performance and plat...\n"
     ]
    }
   ],
   "source": [
    "# Try pure keyword (alpha=0)\n",
    "print(\"\\nalpha=0 (keywords only):\")\n",
    "for i, idx in enumerate(np.argsort(bm25_n)[-3:][::-1], 1):\n",
    "    print(f\"{i}. {documents[idx][:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "57325714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "alpha=1 (semantic only):\n",
      "1. API Gateway Architecture: A well-designed API gateway serves as a single entry p...\n",
      "2. Microservices Security: Securing distributed systems requires defense in depth. ...\n",
      "3. Database Scaling Strategies: Horizontal scaling distributes data across multiple...\n"
     ]
    }
   ],
   "source": [
    "# Try pure semantic (alpha=1)\n",
    "print(\"\\nalpha=1 (semantic only):\")\n",
    "for i, idx in enumerate(np.argsort(sem_n)[-3:][::-1], 1):\n",
    "    print(f\"{i}. {documents[idx][:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "7fd2a8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "alpha=0.5 (balanced):\n",
      "1. Software Development Lifecycle: Agile methodologies emphasize iterative developm...\n",
      "2. Web Application Security: Cross-site scripting (XSS) attacks inject malicious \n",
      " ...\n",
      "3. API Gateway Architecture: A well-designed API gateway serves as a single entry p...\n"
     ]
    }
   ],
   "source": [
    "# Balanced approach\n",
    "print(\"\\nalpha=0.5 (balanced):\")\n",
    "for i, idx in enumerate(np.argsort(0.5*bm25_n + 0.5*sem_n)[-3:][::-1], 1):\n",
    "    print(f\"{i}. {documents[idx][:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "a3eae512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with different query type?\n",
    "# q3 = \"make systems faster\"\n",
    "# q3_emb = co.embed(texts=[q3], input_type=\"search_query\").embeddings[0]\n",
    "# q3_tok = bm25_tokenizer(q3)\n",
    "\n",
    "# Not sure if worth running"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3684cdc1",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "1. For the queries, does keyword matching or semantic search work better?\n",
    "2. What happens if you change alpha to 0.3 or 0.7?\n",
    "3. For really technical queries with specific terms, do you need higher keyword weight?\n",
    "4. For conceptual questions, is semantic search better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f28478",
   "metadata": {},
   "source": [
    "### Task 2: Chunking Experiments\n",
    "\n",
    "When you have long docs, you need to split them up. But how?\n",
    "\n",
    "**Questions:**\n",
    "- Sentence-based vs word-based chunks?\n",
    "- Does overlap between chunks help?\n",
    "- What size works best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "d13ae737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc length: 936 chars, 119 words\n"
     ]
    }
   ],
   "source": [
    "# Let's work with a longer document to test chunking\n",
    "long_document = \"\"\"\n",
    "Artificial intelligence is transforming how businesses operate. Machine learning algorithms can now \n",
    "analyze vast amounts of data to identify patterns that humans might miss. Natural language processing \n",
    "enables computers to understand and generate human language with remarkable accuracy.\n",
    "\n",
    "Deep learning models, particularly neural networks with multiple layers, have achieved breakthrough \n",
    "results in image recognition, speech synthesis, and language translation. These models learn hierarchical \n",
    "representations of data, capturing both low-level features and high-level abstractions.\n",
    "\n",
    "The deployment of AI systems in production environments requires careful consideration of several factors. \n",
    "Model inference latency must be minimized to ensure responsive user experiences. Scalability is crucial \n",
    "as request volumes can vary dramatically. Monitoring and observability help detect model drift and \n",
    "performance degradation over time.\"\"\"\n",
    "\n",
    "print(f\"Doc length: {len(long_document)} chars, {len(long_document.split())} words\")\n",
    "# Pretty long - definitely need to chunk this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "754ffe20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc length: 1553 chars, 204 words\n"
     ]
    }
   ],
   "source": [
    "# Continue the document...\n",
    "long_document += \"\"\"\n",
    "Ethical considerations are paramount when developing AI applications. Bias in training data can lead to \n",
    "unfair outcomes for certain groups. Privacy concerns arise when models are trained on sensitive personal \n",
    "information. Transparency and explainability help build trust with users and stakeholders.\n",
    "\n",
    "The future of AI likely involves more efficient models that require less computational resources. \n",
    "Few-shot and zero-shot learning techniques enable models to adapt to new tasks with minimal examples. \n",
    "Multimodal models that process text, images, and audio together will unlock new applications and capabilities.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Doc length: {len(long_document)} chars, {len(long_document.split())} words\")\n",
    "# Pretty long - definitely need to chunk this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "848c119b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 8 sentence chunks\n",
      "\n",
      "First one:\n",
      "Artificial intelligence is transforming how businesses operate. Machine learning algorithms can now  analyze vast amounts of data to identify patterns that humans might miss.\n"
     ]
    }
   ],
   "source": [
    "# Approach 1: split by sentences\n",
    "sents = [s.strip() for s in long_document.replace('\\n', ' ').split('.') if s.strip()]\n",
    "\n",
    "# Group into 2-sent chunks\n",
    "chunks_sent = []\n",
    "for i in range(0, len(sents), 2):\n",
    "    chunk = '. '.join(sents[i:i + 2]) + '.'\n",
    "    chunks_sent.append(chunk)\n",
    "\n",
    "print(f\"Got {len(chunks_sent)} sentence chunks\")\n",
    "print(\"\\nFirst one:\")\n",
    "print(chunks_sent[0])\n",
    "# print(chunks_sent[1])  # uncomment to see more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "308604de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 7 word chunks (40 words, 10 overlap)\n",
      "\n",
      "First: Artificial intelligence is transforming how businesses operate. Machine learning algorithms can now ...\n",
      "\n",
      "Second: understand and generate human language with remarkable accuracy. Deep learning models, particularly ...\n"
     ]
    }
   ],
   "source": [
    "# Approach 2: word-based with overlap\n",
    "words = long_document.split()\n",
    "chunk_size = 40\n",
    "overlap = 10\n",
    "\n",
    "chunks_word = []\n",
    "i = 0\n",
    "while i < len(words):\n",
    "    chunk = ' '.join(words[i:i + chunk_size])\n",
    "    chunks_word.append(chunk)\n",
    "    i += (chunk_size - overlap)\n",
    "\n",
    "print(f\"Got {len(chunks_word)} word chunks ({chunk_size} words, {overlap} overlap)\")\n",
    "print(\"\\nFirst:\", chunks_word[0][:100] + \"...\")\n",
    "print(\"\\nSecond:\", chunks_word[1][:100] + \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "d409defe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we try different overlap values?\n",
    "# for ovlp in [0, 5, 10, 20]:\n",
    "#     chunks_test = []\n",
    "#     i = 0\n",
    "#     while i < len(words):\n",
    "#         chunks_test.append(' '.join(words[i:i+40]))\n",
    "#         i += (40 - ovlp)\n",
    "#     print(f\"overlap={ovlp}: {len(chunks_test)} chunks\")\n",
    "\n",
    "# TODO: test if 20 overlap is actually better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "838b1bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding sentence chunks...\n",
      "Embedding word chunks...\n",
      "Embedding word chunks...\n",
      "\n",
      "Sentence: (8, 4096)\n",
      "Word: (7, 4096)\n",
      "\n",
      "Sentence: (8, 4096)\n",
      "Word: (7, 4096)\n"
     ]
    }
   ],
   "source": [
    "# Embed both chunk types\n",
    "print(\"Embedding sentence chunks...\")\n",
    "sent_embs = np.array(co.embed(texts=chunks_sent, input_type=\"search_document\").embeddings)\n",
    "\n",
    "print(\"Embedding word chunks...\")\n",
    "word_embs = np.array(co.embed(texts=chunks_word, input_type=\"search_document\").embeddings)\n",
    "\n",
    "print(f\"\\nSentence: {sent_embs.shape}\")\n",
    "print(f\"Word: {word_embs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "8e2f74a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices built! Now let's search...\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Build FAISS indices\n",
    "idx_sent = faiss.IndexFlatL2(sent_embs.shape[1])\n",
    "idx_sent.add(np.float32(sent_embs))\n",
    "\n",
    "idx_word = faiss.IndexFlatL2(word_embs.shape[1])\n",
    "idx_word.add(np.float32(word_embs))\n",
    "\n",
    "print(\"Indices built! Now let's search...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "66d4f11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: ethical concerns with AI\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Try a query about ethics\n",
    "test_q = \"ethical concerns with AI\"\n",
    "\n",
    "# Get query embedding\n",
    "q_emb = co.embed(texts=[test_q], input_type=\"search_query\").embeddings[0]\n",
    "\n",
    "print(f\"\\nQuery: {test_q}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "980ca3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence chunks:\n",
      "1. dist=7059.3555\n",
      "   Monitoring and observability help detect model drift and  performance degradation over time. Ethical considerations are ...\n",
      "\n",
      "2. dist=7389.8052\n",
      "   Transparency and explainability help build trust with users and stakeholders. The future of AI likely involves more effi...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Search sentence-based chunks\n",
    "d_sent, i_sent = idx_sent.search(np.float32([q_emb]), 2)\n",
    "print(\"\\nSentence chunks:\")\n",
    "for i, (idx, d) in enumerate(zip(i_sent[0], d_sent[0]), 1):\n",
    "    print(f\"{i}. dist={d:.4f}\")\n",
    "    print(f\"   {chunks_sent[idx][:120]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "39eb51f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word chunks:\n",
      "1. dist=6042.2891\n",
      "   considerations are paramount when developing AI applications. Bias in training data can lead to unfair outcomes for cert...\n",
      "\n",
      "2. dist=8005.3174\n",
      "   Transparency and explainability help build trust with users and stakeholders. The future of AI likely involves more effi...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Search word-based chunks  \n",
    "d_word, i_word = idx_word.search(np.float32([q_emb]), 2)\n",
    "print(\"Word chunks:\")\n",
    "for i, (idx, d) in enumerate(zip(i_word[0], d_word[0]), 1):\n",
    "    print(f\"{i}. dist={d:.4f}\")\n",
    "    print(f\"   {chunks_word[idx][:120]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191f7040",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "- Which chunking strategy gave better results? \n",
    "- Try changing words_per_chunk to 20 or 60 - does it matter?\n",
    "- What about overlap of 0 vs 20?\n",
    "- For specific questions, are smaller chunks better?\n",
    "- For broad questions, are larger chunks better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b94ac36",
   "metadata": {},
   "source": [
    "### Task 3: Query Expansion\n",
    "\n",
    "Sometimes you don't find what you need because you asked the \"wrong\" way. What if we let an LLM rewrite the query in different ways and search with all versions?\n",
    "\n",
    "**Goal:** Use an LLM to rewrite queries and combine the results.\n",
    "\n",
    "**What you'll learn:**\n",
    "- How to expand queries automatically\n",
    "- How to merge results from multiple searches\n",
    "- When this helps (and when it doesn't)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "43778e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get LLM to generate query variations\n",
    "original_q = \"improving software security\"\n",
    "\n",
    "prompt = f\"\"\"Generate 2 alternative ways to phrase this search query. \n",
    "Each variation should mean the same thing but use different words.\n",
    "\n",
    "Original query: {original_q}\n",
    "\n",
    "Return only the alternative queries, one per line.\"\"\"\n",
    "\n",
    "resp = co.chat(message=prompt, max_tokens=100, temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "306d23c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: improving software security\n",
      "\n",
      "Generated:\n",
      "1. enhancing application protection\n",
      "2. strengthening program safety measures\n"
     ]
    }
   ],
   "source": [
    "# Parse the generated variations\n",
    "variations = [v.strip() for v in resp.text.strip().split('\\n') if v.strip()]\n",
    "\n",
    "print(f\"Original: {original_q}\")\n",
    "print(\"\\nGenerated:\")\n",
    "for i, v in enumerate(variations, 1):\n",
    "    print(f\"{i}. {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "753a1952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First attempt used temperature=0.3 but variations were too similar\n",
    "# Bumped to 0.7 - better diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "e926ca33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Searching with each query...\n"
     ]
    }
   ],
   "source": [
    "# Search with all query variations\n",
    "all_qs = [original_q] + variations\n",
    "\n",
    "print(\"\\nSearching with each query...\")\n",
    "all_res = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "43ed29c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "improving software security:\n",
      "  Top 2:\n",
      "  1. Web Application Security: Cross-site scripting (XSS) attacks...\n",
      "  2. Microservices Security: Securing distributed systems require...\n",
      "\n",
      "enhancing application protection:\n",
      "  Top 2:\n",
      "  1. Web Application Security: Cross-site scripting (XSS) attacks...\n",
      "  2. Microservices Security: Securing distributed systems require...\n",
      "\n",
      "enhancing application protection:\n",
      "  Top 2:\n",
      "  1. Web Application Security: Cross-site scripting (XSS) attacks...\n",
      "  2. Microservices Security: Securing distributed systems require...\n",
      "\n",
      "strengthening program safety measures:\n",
      "  Top 2:\n",
      "  1. Web Application Security: Cross-site scripting (XSS) attacks...\n",
      "  2. Microservices Security: Securing distributed systems require...\n",
      "\n",
      "strengthening program safety measures:\n",
      "  Top 2:\n",
      "  1. Web Application Security: Cross-site scripting (XSS) attacks...\n",
      "  2. Microservices Security: Securing distributed systems require...\n",
      "  Top 2:\n",
      "  1. Web Application Security: Cross-site scripting (XSS) attacks...\n",
      "  2. Microservices Security: Securing distributed systems require...\n"
     ]
    }
   ],
   "source": [
    "# Run search for each query variant\n",
    "for q in all_qs:\n",
    "    print(f\"\\n{q}:\")\n",
    "    \n",
    "    qe = co.embed(texts=[q], input_type=\"search_query\").embeddings[0]\n",
    "    dists, idxs = index.search(np.float32([qe]), 5)\n",
    "    \n",
    "    q_results = [(documents[idx], rank) for rank, (idx, _) in enumerate(zip(idxs[0], dists[0]), 1)]\n",
    "    all_res.append(q_results)\n",
    "    \n",
    "    # Top 2\n",
    "    print(\"  Top 2:\")\n",
    "    for i, (doc, _) in enumerate(q_results[:2], 1):\n",
    "        print(f\"  {i}. {doc[:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "44c13f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged 7 documents\n"
     ]
    }
   ],
   "source": [
    "# Merge using RRF (Reciprocal Rank Fusion)\n",
    "# Docs that show up in multiple results get boosted\n",
    "\n",
    "doc_scores = {}\n",
    "k = 60\n",
    "\n",
    "for results in all_res:\n",
    "    for doc, rank in results:\n",
    "        if doc not in doc_scores:\n",
    "            doc_scores[doc] = 0\n",
    "        doc_scores[doc] += 1 / (k + rank)  # RRF formula\n",
    "\n",
    "merged = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "print(f\"Merged {len(merged)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "4032c5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Merged results:\n",
      "================================================================================\n",
      "1. RRF=0.0492\n",
      "   Web Application Security: Cross-site scripting (XSS) attacks inject malicious \n",
      " ...\n",
      "\n",
      "2. RRF=0.0484\n",
      "   Microservices Security: Securing distributed systems requires defense in depth. ...\n",
      "\n",
      "3. RRF=0.0469\n",
      "   Cloud Computing Performance: Auto-scaling adjusts resource allocation based on \n",
      "...\n",
      "\n",
      "4. RRF=0.0315\n",
      "   Machine Learning Operations: MLOps bridges development and production for ML sys...\n",
      "\n",
      "5. RRF=0.0313\n",
      "   Software Development Lifecycle: Agile methodologies emphasize iterative developm...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display merged results\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Merged results:\")\n",
    "print(\"=\" * 80)\n",
    "for i, (doc, sc) in enumerate(merged[:5], 1):\n",
    "    print(f\"{i}. RRF={sc:.4f}\")\n",
    "    print(f\"   {doc[:80]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "79e16590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 slots, 7 unique docs\n",
      "Some docs appeared multiple times\n"
     ]
    }
   ],
   "source": [
    "# Debug: check if docs repeated\n",
    "from collections import Counter\n",
    "top_docs = [doc for doc, _ in merged[:10]]\n",
    "print(f\"\\nTop 10 slots, {len(set(top_docs))} unique docs\")\n",
    "if len(set(top_docs)) < 10:\n",
    "    print(\"Some docs appeared multiple times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4374e1a1",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "- Try this with a completely different query - do we get different docs?\n",
    "- Does query expansion actually help or just add noise?\n",
    "- When would generating variations be useful vs harmful?\n",
    "- For ambiguous queries does it help, but for specific technical terms does it hurt?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939eea3d",
   "metadata": {},
   "source": [
    "### Medium Task 4: Fitting Documents Into Limited Space\n",
    "\n",
    "LLMs have limited context windows. You might retrieve 20 documents but only fit 5. How do you choose which ones?\n",
    "\n",
    "**Goal:** Pick the best documents that fit within a token budget.\n",
    "\n",
    "**What you'll learn:**\n",
    "- How to estimate token counts\n",
    "- How to rerank documents by relevance\n",
    "- Different strategies for ordering documents in the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "48ddbaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a local embedding model\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name='BAAI/bge-small-en-v1.5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "499444d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 448 chars of docs\n"
     ]
    }
   ],
   "source": [
    "# Some technical documentation\n",
    "tech_docs = \"\"\"\n",
    "Cloud Computing: Our platform provides scalable computing on demand. Virtual machines \n",
    "scale automatically based on load.\n",
    "\n",
    "Machine Learning: Pre-trained models handle common tasks. Custom models can be trained \n",
    "using distributed infrastructure.\n",
    "\n",
    "Databases: We offer SQL and NoSQL databases. Automatic sharding distributes data across nodes.\n",
    "\n",
    "Security: End-to-end encryption protects data. Multi-factor authentication prevents unauthorized access.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Loaded {len(tech_docs)} chars of docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "99f6a9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 6 chunks\n",
      "\n",
      "Chunk 1: Cloud Computing: Our platform provides scalable computing on demand. Virtual mac...\n",
      "\n",
      "Chunk 2: Machine Learning: Pre-trained models handle common tasks. Custom models can be t...\n",
      "\n",
      "Chunk 3: Databases: We offer SQL and NoSQL databases. Automatic sharding distributes data...\n"
     ]
    }
   ],
   "source": [
    "# More docs...\n",
    "tech_docs += \"\"\"\n",
    "Developer Tools: SDK supports Python, JavaScript, Java. Documentation includes code examples.\n",
    "\n",
    "Monitoring: Real-time metrics track performance. Distributed tracing helps debug microservices.\n",
    "\"\"\"\n",
    "\n",
    "# Split into small chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=150, chunk_overlap=30)\n",
    "chunks = splitter.split_text(tech_docs)\n",
    "\n",
    "print(f\"Created {len(chunks)} chunks\")\n",
    "for i, chunk in enumerate(chunks[:3], 1):\n",
    "    print(f\"\\nChunk {i}: {chunk[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "1c3e75f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create vector store and retrieve candidates\n",
    "# Create vector store\n",
    "vectorstore = FAISS.from_texts(chunks, embedding_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "d378f649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: How can I scale my application?\n",
      "Retrieved 6 candidates\n"
     ]
    }
   ],
   "source": [
    "# Retrieve candidates\n",
    "query = \"How can I scale my application?\"\n",
    "\n",
    "# TODO: maybe try k=20 and see if reranking helps more?\n",
    "candidates = vectorstore.similarity_search(query, k=10)\n",
    "candidate_texts = [doc.page_content for doc in candidates]\n",
    "\n",
    "print(f\"\\nQuery: {query}\")\n",
    "print(f\"Retrieved {len(candidate_texts)} candidates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "65c046af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 3:\n",
      "1. Cloud Computing: Our platform provides scalable computing on demand. Virtual mac...\n",
      "2. Monitoring: Real-time metrics track performance. Distributed tracing helps debug...\n",
      "3. Developer Tools: SDK supports Python, JavaScript, Java. Documentation includes c...\n"
     ]
    }
   ],
   "source": [
    "# Quick look at what we got\n",
    "print(\"\\nFirst 3:\")\n",
    "for i, txt in enumerate(candidate_texts[:3], 1):\n",
    "    print(f\"{i}. {txt[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "dfa974a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After reranking:\n",
      "1. Relevance: 0.2476\n",
      "   Cloud Computing: Our platform provides scalable computing on demand. Virtual mac...\n",
      "\n",
      "2. Relevance: 0.1010\n",
      "   Databases: We offer SQL and NoSQL databases. Automatic sharding distributes data...\n",
      "\n",
      "3. Relevance: 0.0649\n",
      "   Machine Learning: Pre-trained models handle common tasks. Custom models can be t...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Rerank using Cohere to get the BEST candidates\n",
    "# Reranking scores documents based on how well they answer the question\n",
    "\n",
    "# Rerank with Cohere\n",
    "# Should score by relevance not just distance\n",
    "\n",
    "reranked = co.rerank(\n",
    "    query=query,\n",
    "    documents=candidate_texts,\n",
    "    top_n=10,\n",
    "    return_documents=True\n",
    ")\n",
    "\n",
    "print(\"\\nAfter reranking:\")\n",
    "for i, res in enumerate(reranked.results[:3], 1):\n",
    "    print(f\"{i}. Relevance: {res.relevance_score:.4f}\")\n",
    "    print(f\"   {res.document.text[:80]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "30c4d5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick docs that fit in token budget\n",
    "# Quick estimate: ~4 chars per token\n",
    "\n",
    "max_tok = 300\n",
    "selected = []\n",
    "total_tok = 0\n",
    "\n",
    "for res in reranked.results:\n",
    "    txt = res.document.text\n",
    "    tok = len(txt) // 4\n",
    "    \n",
    "    if total_tok + tok <= max_tok:\n",
    "        selected.append({'text': txt, 'rel': res.relevance_score, 'tok': tok})\n",
    "        total_tok += tok\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "11cfe096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit 6 docs in 155 tokens (budget: 300)\n",
      "\n",
      "1. rel=0.248, tok=30\n",
      "   Cloud Computing: Our platform provides scalable co...\n",
      "\n",
      "2. rel=0.101, tok=23\n",
      "   Databases: We offer SQL and NoSQL databases. Autom...\n",
      "\n",
      "3. rel=0.065, tok=30\n",
      "   Machine Learning: Pre-trained models handle common...\n",
      "\n",
      "4. rel=0.043, tok=23\n",
      "   Developer Tools: SDK supports Python, JavaScript, ...\n",
      "\n",
      "5. rel=0.041, tok=23\n",
      "   Monitoring: Real-time metrics track performance. D...\n",
      "\n",
      "6. rel=0.028, tok=26\n",
      "   Security: End-to-end encryption protects data. Mul...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# What did we select?\n",
    "print(f\"Fit {len(selected)} docs in {total_tok} tokens (budget: {max_tok})\\n\")\n",
    "\n",
    "for i, d in enumerate(selected, 1):\n",
    "    print(f\"{i}. rel={d['rel']:.3f}, tok={d['tok']}\")\n",
    "    print(f\"   {d['text'][:50]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "527d9c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best doc first:\n",
      "Doc 1: Cloud Computing: Our platform provides scalable computing on demand. Virtual machines \n",
      "scale automatically based on load.\n",
      "Doc 2: Databases: We ...\n",
      "\n",
      "Best doc last:\n",
      "Doc 1: Security: End-to-end encryption protects data. Multi-factor authentication prevents unauthorized access.\n",
      "Doc 2: Monitoring: Real-time metrics t...\n"
     ]
    }
   ],
   "source": [
    "# Try different orderings\n",
    "print(\"\\nBest doc first:\")\n",
    "ctx1 = \"\\n\".join([f\"Doc {i+1}: {d['text']}\" for i, d in enumerate(selected)])\n",
    "print(ctx1[:150] + \"...\\n\")\n",
    "\n",
    "print(\"Best doc last:\")\n",
    "ctx2 = \"\\n\".join([f\"Doc {i+1}: {d['text']}\" for i, d in enumerate(reversed(selected))])\n",
    "print(ctx2[:150] + \"...\")\n",
    "\n",
    "# Research says end position sometimes better (recency bias)\n",
    "# But need to test with actual LLM to know for sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "d54e36a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 by relevance (no token limit):\n",
      "1. 0.248 : Cloud Computing: Our platform provides scalable computing on demand. Virtual machines \n",
      "scale automat...\n",
      "2. 0.101 : Databases: We offer SQL and NoSQL databases. Automatic sharding distributes data across nodes....\n",
      "3. 0.065 : Machine Learning: Pre-trained models handle common tasks. Custom models can be trained \n",
      "using distri...\n",
      "4. 0.043 : Developer Tools: SDK supports Python, JavaScript, Java. Documentation includes code examples....\n",
      "5. 0.041 : Monitoring: Real-time metrics track performance. Distributed tracing helps debug microservices....\n"
     ]
    }
   ],
   "source": [
    "# Quick check: what if we ignored tokens?\n",
    "print(\"\\nTop 5 by relevance (no token limit):\")\n",
    "for i, res in enumerate(reranked.results[:5], 1):\n",
    "    print(f\"{i}. {res.relevance_score:.3f} : {res.document.text[:100]}...\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "7e37e2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Try different document orderings\n",
    "# Research shows: best document LAST often works better!\n",
    "\n",
    "# Note: tried both orderings above\n",
    "# Normally you'd test with actual LLM to see which works better\n",
    "# Context window positioning can affect answer quality!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc170fb",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "- What happens with max_tokens=500 instead of 300?\n",
    "- Try putting best doc first vs last. Which works better?\n",
    "- What if a single doc is bigger than max_tokens?\n",
    "- Could we summarize long docs instead of truncating?\n",
    "- Could we use doc importance scores for selection?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ads525",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
