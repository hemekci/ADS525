{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ETtu9CvVMDR"
   },
   "source": [
    "<h1>Chapter 7 - Advanced Text Generation Techniques and Tools</h1>\n",
    "<i>Going beyond prompt engineering.</i>\n",
    "\n",
    "<a href=\"https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961\"><img src=\"https://img.shields.io/badge/Buy%20the%20Book!-grey?logo=amazon\"></a>\n",
    "<a href=\"https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/\"><img src=\"https://img.shields.io/badge/O'Reilly-white.svg?logo=data:image/svg%2bxml;base64,PHN2ZyB3aWR0aD0iMzQiIGhlaWdodD0iMjciIHZpZXdCb3g9IjAgMCAzNCAyNyIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGNpcmNsZSBjeD0iMTMiIGN5PSIxNCIgcj0iMTEiIHN0cm9rZT0iI0Q0MDEwMSIgc3Ryb2tlLXdpZHRoPSI0Ii8+CjxjaXJjbGUgY3g9IjMwLjUiIGN5PSIzLjUiIHI9IjMuNSIgZmlsbD0iI0Q0MDEwMSIvPgo8L3N2Zz4K\"></a>\n",
    "<a href=\"https://github.com/HandsOnLLM/Hands-On-Large-Language-Models\"><img src=\"https://img.shields.io/badge/GitHub%20Repository-black?logo=github\"></a>\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter07/Chapter%207%20-%20Advanced%20Text%20Generation%20Techniques%20and%20Tools.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "This notebook is for Chapter 7 of the [Hands-On Large Language Models](https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961) book by [Jay Alammar](https://www.linkedin.com/in/jalammar) and [Maarten Grootendorst](https://www.linkedin.com/in/mgrootendorst/).\n",
    "\n",
    "---\n",
    "\n",
    "<a href=\"https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961\">\n",
    "<img src=\"https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/images/book_cover.png\" width=\"350\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [OPTIONAL] - Installing Packages on <img src=\"https://colab.google/static/images/icons/colab.png\" width=100>\n",
    "\n",
    "If you are viewing this notebook on Google Colab (or any other cloud vendor), you need to **uncomment and run** the following codeblock to install the dependencies for this chapter:\n",
    "\n",
    "---\n",
    "\n",
    "💡 **NOTE**: We will want to use a GPU to run the examples in this notebook. In Google Colab, go to\n",
    "**Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: llama-cpp-python==0.2.69 in /home/danielcastillo/Desktop/ADS525/ads525/lib/python3.10/site-packages (0.2.69)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /home/danielcastillo/Desktop/ADS525/ads525/lib/python3.10/site-packages (from llama-cpp-python==0.2.69) (1.26.4)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /home/danielcastillo/Desktop/ADS525/ads525/lib/python3.10/site-packages (from llama-cpp-python==0.2.69) (5.6.3)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/danielcastillo/Desktop/ADS525/ads525/lib/python3.10/site-packages (from llama-cpp-python==0.2.69) (4.15.0)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /home/danielcastillo/Desktop/ADS525/ads525/lib/python3.10/site-packages (from llama-cpp-python==0.2.69) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/danielcastillo/Desktop/ADS525/ads525/lib/python3.10/site-packages (from jinja2>=2.11.3->llama-cpp-python==0.2.69) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain>=0.1.17 openai>=1.13.3 langchain_openai>=0.1.6 transformers>=4.40.1 datasets>=2.18.0 accelerate>=0.27.2 sentence-transformers>=2.5.1 duckduckgo-search>=5.2.2 langchain_community\n",
    "%pip install llama-cpp-python==0.2.69"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rerbJgwAigbK"
   },
   "source": [
    "# Loading an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf\n",
    "\n",
    "# If this command does not work for you, you can use the link directly to download the model\n",
    "# https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-cpp-python in /home/danielcastillo/Desktop/ADS525/ads525/lib/python3.10/site-packages (0.2.69)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /home/danielcastillo/Desktop/ADS525/ads525/lib/python3.10/site-packages (from llama-cpp-python) (1.26.4)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /home/danielcastillo/Desktop/ADS525/ads525/lib/python3.10/site-packages (from llama-cpp-python) (3.1.6)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /home/danielcastillo/Desktop/ADS525/ads525/lib/python3.10/site-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/danielcastillo/Desktop/ADS525/ads525/lib/python3.10/site-packages (from llama-cpp-python) (4.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/danielcastillo/Desktop/ADS525/ads525/lib/python3.10/site-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LQcht_ZFijW7"
   },
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"Phi-3-mini-4k-instruct-fp16.gguf\",\n",
    "    n_gpu_layers=-1,\n",
    "    max_tokens=500,\n",
    "    n_ctx=2048,\n",
    "    seed=42,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 854,
     "status": "ok",
     "timestamp": 1724338298709,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "3SNhQF9WthzV",
    "outputId": "fd062b8a-4643-43a3-afc1-cce9dc338708"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Hi! My name is Maarten. What is 1 + 1?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wwx2AIuGfCoP"
   },
   "source": [
    "### Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "kF--Q5me_-X1"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Create a prompt template with the \"input_prompt\" variable\n",
    "template = \"\"\"<s><|user|>\n",
    "{input_prompt}<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"input_prompt\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ogWsGeg6hElt"
   },
   "outputs": [],
   "source": [
    "basic_chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 894,
     "status": "ok",
     "timestamp": 1724338313078,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "KINQxKAINXgG",
    "outputId": "682d6b12-a4aa-4992-8abe-b23334b2a524"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Hello Maarten, the answer to 1 + 1 is 2.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the chain\n",
    "basic_chain.invoke(\n",
    "    {\n",
    "        \"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSMBMRxB8gFW"
   },
   "source": [
    "### Multiple Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 292,
     "status": "ok",
     "timestamp": 1724338320681,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "wrUKuHt_OLpe",
    "outputId": "844043b3-51ad-4de8-dada-a7b290c2e5c0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_116285/1360148921.py:8: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use `RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  title = LLMChain(llm=llm, prompt=title_prompt, output_key=\"title\")\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.chains import LLMChain\n",
    "\n",
    "# Create a chain for the title of our story\n",
    "template = \"\"\"<s><|user|>\n",
    "Create a title for a story about {summary}. Only return the title.<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "title_prompt = PromptTemplate(template=template, input_variables=[\"summary\"])\n",
    "title = LLMChain(llm=llm, prompt=title_prompt, output_key=\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 805,
     "status": "ok",
     "timestamp": 1724338321745,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "igFIyg73OtaL",
    "outputId": "9d7610f3-8fc1-429d-bf5f-f1a4697fad40"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'a girl that lost her mother',\n",
       " 'title': ' \"Whispers of Love: A Journey Through Grief\"'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title.invoke({\"summary\": \"a girl that lost her mother\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "zTtFEmANOhyE"
   },
   "outputs": [],
   "source": [
    "# Create a chain for the character description using the summary and title\n",
    "template = \"\"\"<s><|user|>\n",
    "Describe the main character of a story about {summary} with the title {title}. Use only two sentences.<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "character_prompt = PromptTemplate(\n",
    "    template=template, input_variables=[\"summary\", \"title\"]\n",
    ")\n",
    "character = LLMChain(llm=llm, prompt=character_prompt, output_key=\"character\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Xjf-avW8NAqZ"
   },
   "outputs": [],
   "source": [
    "# Create a chain for the story using the summary, title, and character description\n",
    "template = \"\"\"<s><|user|>\n",
    "Create a story about {summary} with the title {title}. The main charachter is: {character}. Only return the story and it cannot be longer than one paragraph<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "story_prompt = PromptTemplate(\n",
    "    template=template, input_variables=[\"summary\", \"title\", \"character\"]\n",
    ")\n",
    "story = LLMChain(llm=llm, prompt=story_prompt, output_key=\"story\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "epNudKyyPClO"
   },
   "outputs": [],
   "source": [
    "# Combine all three components to create the full chain\n",
    "llm_chain = title | character | story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6677,
     "status": "ok",
     "timestamp": 1715331735693,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "b44ZR0vXRaAo",
    "outputId": "f73d9bbc-d126-4aed-8f1c-7ff14d5afa59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'a girl that lost her mother',\n",
       " 'title': ' \"Finding Warmth in Grief: A Tale of Lily\\'s Journey\"',\n",
       " 'character': \" Lily is an empathetic and resilient young girl who, after losing her beloved mother to illness, embarks on a transformative journey to find solace and healing amidst the depths of grief. Her kind heart and unyielding spirit lead her to unexpected friendships and self-discoveries that help her navigate through life's challenges with grace and newfound strength.\\n\\nLily, an introverted yet determined girl, is forced to grow beyond her comfort zone as she seeks out ways to cope with the loss of her mother; this quest unveils both heartbreaking vulnerabilities and inspiring moments of courage that gradually shape her into a pillar of support for others in their own grief.\",\n",
       " 'story': \" Finding Warmth in Grief: A Tale of Lily's Journey tells the poignant tale of an empathetic and resilient young girl named Lily, who after experiencing the tragic loss of her beloved mother to illness, embarks on a transformative journey that leads her through the darkest corridors of grief in search of solace. With each step forward, she encounters individuals from all walks of life whose unwavering kindness and support begin to heal her wounded heart. Through their stories, Lily finds courage within herself as she learns that love endures beyond death; thus, blossoming into a beacon of strength and compassion for others navigating their own paths through loss.\"}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke(\"a girl that lost her mother\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UQ-DZ71P-D-"
   },
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 841,
     "status": "ok",
     "timestamp": 1715331767433,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "-15Eoey5EJUO",
    "outputId": "e475493c-ede1-4932-b954-ade7be05c79a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Hello Maarten, the answer to 1 + 1 is 2.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's give the LLM our name\n",
    "basic_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "executionInfo": {
     "elapsed": 1385,
     "status": "ok",
     "timestamp": 1715331769763,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "N42wQRl-Lykt",
    "outputId": "d1019050-017d-4dcb-a008-7029e9ebd9fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" I'm unable to determine your name as I don't have access to personal data of individuals. If you need help with something specific, feel free to ask!\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next, we ask the LLM to reproduce the name\n",
    "basic_chain.invoke({\"input_prompt\": \"What is my name?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PfqATEZjMgET"
   },
   "source": [
    "## ConversationBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Zoo0PA1fUs70"
   },
   "outputs": [],
   "source": [
    "# Create an updated prompt template to include a chat history\n",
    "template = \"\"\"<s><|user|>Current conversation:{chat_history}\n",
    "\n",
    "{input_prompt}<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"input_prompt\", \"chat_history\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "bgGMS1S9saLi"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_116285/3768838688.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\")\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "\n",
    "# Define the type of Memory we will use\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "# Chain the LLM, Prompt, and Memory together\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=llm,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 887,
     "status": "ok",
     "timestamp": 1715331790905,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "mltR_GtkiqDZ",
    "outputId": "15161d8e-2520-4ffc-e104-6147e52bd5f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'Hi! My name is Maarten. What is 1 + 1?',\n",
       " 'chat_history': '',\n",
       " 'text': \" Hello Maarten! The answer to what 1 + 1 equals is 2. It's a basic arithmetic operation in mathematics where you add the two numbers together.\"}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a conversation and ask a basic question\n",
    "llm_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 821,
     "status": "ok",
     "timestamp": 1715331794689,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "h-je1rmy3dx4",
    "outputId": "5a25e2dd-5fb4-4cd8-8a5d-5da423bd3d33"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'What is my name?',\n",
       " 'chat_history': \"Human: Hi! My name is Maarten. What is 1 + 1?\\nAI:  Hello Maarten! The answer to what 1 + 1 equals is 2. It's a basic arithmetic operation in mathematics where you add the two numbers together.\",\n",
       " 'text': \" Your name is mentioned as Maarten by the human in this conversation.\\n\\nAs for your identity, I'm an AI digital assistant and don't have a personal name, but you can refer to me as the AI.\"}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Does the LLM remember the name we gave it?\n",
    "llm_chain.invoke({\"input_prompt\": \"What is my name?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sw3ELCg6Rpsk"
   },
   "source": [
    "## ConversationBufferMemoryWindow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "G0DRT7kjRtiC"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_116285/1769901941.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\")\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# Retain only the last 2 conversations in memory\n",
    "memory = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\")\n",
    "\n",
    "# Chain the LLM, Prompt, and Memory together\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=llm,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2270,
     "status": "ok",
     "timestamp": 1715331894039,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "CBY69vvcR1Qq",
    "outputId": "76dbe9cc-3161-486c-b5b1-c8ce713ab1ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'What is 3 + 3?',\n",
       " 'chat_history': \"Human: Hi! My name is Maarten and I am 33 years old. What is 1 + 1?\\nAI:  Hello Maarten, my name isn't programmed to remember personal details, but I can certainly help with the math question! 1 + 1 equals 2.\",\n",
       " 'text': ' 3 + 3 equals 6.'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ask two questions and generate two conversations in its memory\n",
    "llm_chain.invoke({\"input_prompt\":\"Hi! My name is Maarten and I am 33 years old. What is 1 + 1?\"})\n",
    "llm_chain.invoke({\"input_prompt\":\"What is 3 + 3?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 455,
     "status": "ok",
     "timestamp": 1715331894493,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "nvSLfKWpR5h5",
    "outputId": "6ce15789-1eae-4817-c676-0282f22b5d40"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'What is my name?',\n",
       " 'chat_history': \"Human: Hi! My name is Maarten and I am 33 years old. What is 1 + 1?\\nAI:  Hello Maarten, my name isn't programmed to remember personal details, but I can certainly help with the math question! 1 + 1 equals 2.\\nHuman: What is 3 + 3?\\nAI:  3 + 3 equals 6.\",\n",
       " 'text': ' Your name, as mentioned in the conversation, is Maarten.\\n\\nAnd to answer your math question again, 3 + 3 equals 6.'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check whether it knows the name we gave it\n",
    "llm_chain.invoke({\"input_prompt\":\"What is my name?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1455,
     "status": "ok",
     "timestamp": 1715331896303,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "YW7qEyctcqeJ",
    "outputId": "54e196e3-b1de-4269-c31d-2ec369efce4b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'What is my age?',\n",
       " 'chat_history': 'Human: What is 3 + 3?\\nAI:  3 + 3 equals 6.\\nHuman: What is my name?\\nAI:  Your name, as mentioned in the conversation, is Maarten.\\n\\nAnd to answer your math question again, 3 + 3 equals 6.',\n",
       " 'text': \" I'm unable to determine your age as I don't have access to personal information.\\nAnswer: I cannot answer that question.\"}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check whether it knows the age we gave it\n",
    "llm_chain.invoke({\"input_prompt\":\"What is my age?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSb5OnANMhu2"
   },
   "source": [
    "## ConversationSummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "lWHZlJUbwpqE"
   },
   "outputs": [],
   "source": [
    "# Create a summary prompt template\n",
    "summary_prompt_template = \"\"\"<s><|user|>Summarize the conversations and update with the new lines.\n",
    "\n",
    "Current summary:\n",
    "{summary}\n",
    "\n",
    "new lines of conversation:\n",
    "{new_lines}\n",
    "\n",
    "New summary:<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "summary_prompt = PromptTemplate(\n",
    "    input_variables=[\"new_lines\", \"summary\"],\n",
    "    template=summary_prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "qg1HAgxZMkbO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_116285/1883484148.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryMemory(\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.memory import ConversationSummaryMemory\n",
    "\n",
    "# Define the type of memory we will use\n",
    "memory = ConversationSummaryMemory(\n",
    "    llm=llm,\n",
    "    memory_key=\"chat_history\",\n",
    "    prompt=summary_prompt\n",
    ")\n",
    "\n",
    "# Chain the LLM, prompt, and memory together\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=llm,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6468,
     "status": "ok",
     "timestamp": 1715332131212,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "2klIk9CpVSH0",
    "outputId": "1edddb31-7703-4a54-c758-03a6d459e783"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'What is my name?',\n",
       " 'chat_history': ' Maarten introduces himself and asks a simple math question about the sum of 1 + 1, to which the AI confirms that it equals 2, providing an explanation on addition as a basic arithmetic operation.',\n",
       " 'text': ' Your name was not mentioned in our previous conversation. You introduced yourself as Maarten. How may I assist you further with math or any other queries?'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a conversation and ask for the name\n",
    "llm_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})\n",
    "llm_chain.invoke({\"input_prompt\": \"What is my name?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4499,
     "status": "ok",
     "timestamp": 1715332139542,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "_VdOH_I-V-Fy",
    "outputId": "b20dc2a4-1be8-40dd-d683-955b32900875"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'What was the first question I asked?',\n",
       " 'chat_history': ' Maarten introduces himself and inquires about the sum of 1 + 1, to which the AI confirms it equals 2, explaining addition as a basic arithmetic operation. The AI also reminds Maarten that his name was not provided earlier but recognizes him from the introduction. It stands ready to assist with any math problems or additional queries he may have.',\n",
       " 'text': ' The first question you asked was, \"what is 1+1?\"'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check whether it has summarized everything thus far\n",
    "llm_chain.invoke({\"input_prompt\": \"What was the first question I asked?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 211,
     "status": "ok",
     "timestamp": 1715332142602,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "n1_LlvrVX9HL",
    "outputId": "f1c989a2-76e2-4348-cbe5-8a4f5a551dc0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': ' Maarten introduces himself and asks about the sum of 1 + 1, to which the AI confirms it equals 2. The AI also reminds Maarten that his name wasn\\'t mentioned in their previous interaction but recognizes him from this introduction. It stands ready to assist with any math problems or additional queries he may have. Additionally, when asked about the first question he posed, the human inquires and the AI confirms it was \"what is 1+1?\". The AI explains addition as a fundamental arithmetic operation before offering further assistance.'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check what the summary is thus far\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BG5sJa1qvS4N"
   },
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rcBt8bZM56dM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Load OpenAI's LLMs with LangChain\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your_api_key_here\"\n",
    "openai_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ddgs\n",
      "  Using cached ddgs-9.6.1-py3-none-any.whl (41 kB)\n",
      "Requirement already satisfied: httpx[brotli,http2,socks]>=0.28.1 in /home/danielcastillo/Desktop/ADS525/ads525/lib/python3.10/site-packages (from ddgs) (0.28.1)\n",
      "Requirement already satisfied: httpx[brotli,http2,socks]>=0.28.1 in /home/danielcastillo/Desktop/ADS525/ads525/lib/python3.10/site-packages (from ddgs) (0.28.1)\n",
      "Collecting click>=8.1.8\n",
      "Collecting click>=8.1.8\n",
      "  Downloading click-8.3.0-py3-none-any.whl (107 kB)\n",
      "\u001b[2K     \u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/107.3 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m0m  Downloading click-8.3.0-py3-none-any.whl (107 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.3/107.3 KB\u001b[0m \u001b[31m711.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m803.9 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.3/107.3 KB\u001b[0m \u001b[31m711.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting lxml>=6.0.0\n",
      "Collecting lxml>=6.0.0\n",
      "  Downloading lxml-6.0.2-cp310-cp310-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl (5.3 MB)\n",
      "\u001b[?25l     \u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/5.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m  Downloading lxml-6.0.2-cp310-cp310-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl (5.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m340.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m340.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting primp>=0.15.0\n",
      "Collecting primp>=0.15.0\n",
      "  Downloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m0m  Downloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m389.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: idna in /home/danielcastillo/Desktop/ADS525/ads525/lib/python3.10/site-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (3.11)\n",
      "Requirement already satisfied: anyio in /home/danielcastillo/Desktop/ADS525/ads525/lib/python3.10/site-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.11.0)\n",
      "Requirement already satisfied: certifi in /home/danielcastillo/Desktop/ADS525/ads525/lib/python3.10/site-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /home/danielcastillo/Desktop/ADS525/ads525/lib/python3.10/site-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.0.9)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m389.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: idna in /home/danielcastillo/Desktop/ADS525/ads525/lib/python3.10/site-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (3.11)\n",
      "Requirement already satisfied: anyio in /home/danielcastillo/Desktop/ADS525/ads525/lib/python3.10/site-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.11.0)\n",
      "Requirement already satisfied: certifi in /home/danielcastillo/Desktop/ADS525/ads525/lib/python3.10/site-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /home/danielcastillo/Desktop/ADS525/ads525/lib/python3.10/site-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.0.9)\n",
      "Collecting socksio==1.*\n",
      "  Using cached socksio-1.0.0-py3-none-any.whl (12 kB)\n",
      "Collecting socksio==1.*\n",
      "  Using cached socksio-1.0.0-py3-none-any.whl (12 kB)\n",
      "Collecting h2<5,>=3\n",
      "  Using cached h2-4.3.0-py3-none-any.whl (61 kB)\n",
      "Collecting h2<5,>=3\n",
      "  Using cached h2-4.3.0-py3-none-any.whl (61 kB)\n",
      "Collecting brotli\n",
      "  Using cached Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
      "Requirement already satisfied: h11>=0.16 in /home/danielcastillo/Desktop/ADS525/ads525/lib/python3.10/site-packages (from httpcore==1.*->httpx[brotli,http2,socks]>=0.28.1->ddgs) (0.16.0)\n",
      "Collecting brotli\n",
      "  Using cached Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
      "Requirement already satisfied: h11>=0.16 in /home/danielcastillo/Desktop/ADS525/ads525/lib/python3.10/site-packages (from httpcore==1.*->httpx[brotli,http2,socks]>=0.28.1->ddgs) (0.16.0)\n",
      "Collecting hpack<5,>=4.1\n",
      "  Using cached hpack-4.1.0-py3-none-any.whl (34 kB)\n",
      "Collecting hpack<5,>=4.1\n",
      "  Using cached hpack-4.1.0-py3-none-any.whl (34 kB)\n",
      "Collecting hyperframe<7,>=6.1\n",
      "  Using cached hyperframe-6.1.0-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/danielcastillo/Desktop/ADS525/ads525/lib/python3.10/site-packages (from anyio->httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/danielcastillo/Desktop/ADS525/ads525/lib/python3.10/site-packages (from anyio->httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5 in /home/danielcastillo/Desktop/ADS525/ads525/lib/python3.10/site-packages (from anyio->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.15.0)\n",
      "Collecting hyperframe<7,>=6.1\n",
      "  Using cached hyperframe-6.1.0-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/danielcastillo/Desktop/ADS525/ads525/lib/python3.10/site-packages (from anyio->httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/danielcastillo/Desktop/ADS525/ads525/lib/python3.10/site-packages (from anyio->httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5 in /home/danielcastillo/Desktop/ADS525/ads525/lib/python3.10/site-packages (from anyio->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.15.0)\n",
      "Installing collected packages: brotli, socksio, primp, lxml, hyperframe, hpack, click, h2, ddgs\n",
      "Installing collected packages: brotli, socksio, primp, lxml, hyperframe, hpack, click, h2, ddgs\n",
      "Successfully installed brotli-1.1.0 click-8.3.0 ddgs-9.6.1 h2-4.3.0 hpack-4.1.0 hyperframe-6.1.0 lxml-6.0.2 primp-0.15.0 socksio-1.0.0\n",
      "Successfully installed brotli-1.1.0 click-8.3.0 ddgs-9.6.1 h2-4.3.0 hpack-4.1.0 hyperframe-6.1.0 lxml-6.0.2 primp-0.15.0 socksio-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ddgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "lmRZu8DO2p6k"
   },
   "outputs": [],
   "source": [
    "# Create the ReAct template\n",
    "react_template = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=react_template,\n",
    "    input_variables=[\"tools\", \"tool_names\", \"input\", \"agent_scratchpad\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "NV-ssNa-4zOK"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import ddgs python package. Please install it with `pip install -U ddgs`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/ADS525/ads525/lib/python3.10/site-packages/langchain_community/utilities/duckduckgo_search.py:49\u001b[0m, in \u001b[0;36mDuckDuckGoSearchAPIWrapper.validate_environment\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mddgs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DDGS  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ddgs'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DuckDuckGoSearchResults\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# You can create the tool to pass to an agent\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m search \u001b[38;5;241m=\u001b[39m \u001b[43mDuckDuckGoSearchResults\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m search_tool \u001b[38;5;241m=\u001b[39m Tool(\n\u001b[1;32m      7\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mduckduck\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA web search engine. Use this to as a search engine for general queries.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     func\u001b[38;5;241m=\u001b[39msearch\u001b[38;5;241m.\u001b[39mrun,\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Prepare tools|\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/ADS525/ads525/lib/python3.10/site-packages/langchain_core/tools/base.py:508\u001b[0m, in \u001b[0;36mBaseTool.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs_schema must be a subclass of pydantic BaseModel or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma JSON schema dict. Got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124margs_schema\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    506\u001b[0m     )\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 508\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ADS525/ads525/lib/python3.10/site-packages/langchain_core/load/serializable.py:113\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419  # Intentional blank docstring\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/ADS525/ads525/lib/python3.10/site-packages/langchain_community/utilities/duckduckgo_search.py:51\u001b[0m, in \u001b[0;36mDuckDuckGoSearchAPIWrapper.validate_environment\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mddgs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DDGS  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import ddgs python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install -U ddgs`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     54\u001b[0m     )\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "\u001b[0;31mImportError\u001b[0m: Could not import ddgs python package. Please install it with `pip install -U ddgs`."
     ]
    }
   ],
   "source": [
    "from langchain_classic.agents import load_tools, Tool\n",
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "\n",
    "# You can create the tool to pass to an agent\n",
    "search = DuckDuckGoSearchResults()\n",
    "search_tool = Tool(\n",
    "    name=\"duckduck\",\n",
    "    description=\"A web search engine. Use this to as a search engine for general queries.\",\n",
    "    func=search.run,\n",
    ")\n",
    "\n",
    "# Prepare tools|\n",
    "tools = load_tools([\"llm-math\"], llm=openai_llm)\n",
    "tools.append(search_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6tAr1962vS4T"
   },
   "outputs": [],
   "source": [
    "from langchain_classic.agents import AgentExecutor, create_react_agent\n",
    "\n",
    "# Construct the ReAct agent\n",
    "agent = create_react_agent(openai_llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, tools=tools, verbose=True, handle_parsing_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5841,
     "status": "ok",
     "timestamp": 1712135814912,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "QSU6ECdYBOOm",
    "outputId": "b6cf304c-c0f2-4939-b682-2f72d7e5a078"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use a web search engine to find the current price of a MacBook Pro in USD and then use a calculator to convert it to EUR.\n",
      "Action: duckduck\n",
      "Action Input: \"current price of MacBook Pro in USD\"\u001b[0m\u001b[33;1m\u001b[1;3msnippet: Apple's 2024 MacBook Pro 16-inch can be equipped with an M4 Pro or M4 Max chip. Retail prices start at $2,499 USD , but every configuration is on sale, with the best prices in this guide knocking hundred of dollars off the laptop of your choosing. You can also browse a selection of the top 16-inch MacBook Pro deals in our dedicated roundup., title: MacBook Pro 16-inch 2024 M4 Best Sale Price Deals, link: https://prices.appleinsider.com/macbook-pro-16-inch-m4, snippet: Apple is refreshing the 14-inch MacBook Pro with its new M5 chip, which promises to further enhance graphics and AI workload performance. The updated MacBook is for students, business users, and ..., title: Apple's New 14-Inch MacBook Pro Arrives With M5 Chip, $1,599 Starting Price, link: https://www.pcmag.com/news/apples-new-14-inch-macbook-pro-arrives-with-m5-chip-1599-starting-price, snippet: With power, performance, and premium style, the MacBook Pro represents the pinnacle of Apple's laptop offerings. This comprehensive guide breaks down current MacBook Pro pricing across available sizes, processors, storage capacities, and more. You'll learn how choices at checkout impact final ..., title: How Much Does A MacBook Pro Cost? - The Pricer, link: https://www.thepricer.org/how-much-does-a-macbook-pro-cost/, snippet: Best Buy is knocking up to 50% off the new MacBook Pro M5. Here's how to get Apple's latest Macs for less and save an extra 10% in the process., title: Get the new MacBook Pro M5 from $799 at Best Buy - Tom's Guide, link: https://www.tomsguide.com/computing/macbooks/get-the-new-macbook-pro-m5-from-usd799-at-best-buy-heres-how-to-save-50-percent-right-now\u001b[0m\u001b[32;1m\u001b[1;3mI found the current price of a MacBook Pro in USD. Now I need to use a calculator to convert it to EUR.\n",
      "Action: Calculator\n",
      "Action Input: 2499 * 0.85\u001b[0m\u001b[36;1m\u001b[1;3mAnswer: 2124.15\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: The current price of a MacBook Pro in USD is $2,499. It would cost approximately €2,124.15 in EUR with an exchange rate of 0.85 EUR for 1 USD.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD?',\n",
       " 'output': 'The current price of a MacBook Pro in USD is $2,499. It would cost approximately €2,124.15 in EUR with an exchange rate of 0.85 EUR for 1 USD.'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the Price of a MacBook Pro?\n",
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD?\"\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ads525",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
