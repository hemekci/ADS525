{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bffe54e0",
   "metadata": {},
   "source": [
    "# Semantic Search & RAG - Hard Tasks\n",
    "\n",
    "Advanced RAG stuff that actually gets used in production.\n",
    "\n",
    "**Topics:**\n",
    "- Multi-vector retrieval (ColBERT-style)\n",
    "- Cross-encoder reranking\n",
    "- HyDE (Hypothetical Document Embeddings)\n",
    "- Self-correcting RAG\n",
    "- Adaptive retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aa2520",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run all cells in this section to set up the environment and load necessary data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7df3e6b",
   "metadata": {},
   "source": [
    "### [Optional] - Installing Packages on Google Colab\n",
    "\n",
    "If you are viewing this notebook on Google Colab, uncomment and run the following code to install dependencies.\n",
    "\n",
    "**Note**: Use a GPU for this notebook. In Google Colab, go to Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6266d1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install langchain==0.2.5 faiss-cpu==1.8.0 cohere==5.5.8 langchain-community==0.2.5 rank_bm25==0.2.2 sentence-transformers==3.0.1 pandas python-dotenv\n",
    "# !pip install llama-cpp-python==0.2.78  --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu124\n",
    "\n",
    "# ## IMPORTANT: Make sure to restart the session after installing the packages above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf1dd8c",
   "metadata": {},
   "source": [
    "### Import Libraries and Setup API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "42febddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded\n"
     ]
    }
   ],
   "source": [
    "import cohere\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "import string\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.environ.get('COHERE_API_KEY')\n",
    "\n",
    "if api_key is None:\n",
    "    print(\"API key not found.\")\n",
    "else:\n",
    "    print(\"API key loaded\")\n",
    "\n",
    "co = cohere.Client(api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301f096b",
   "metadata": {},
   "source": [
    "### Load Sample Dataset\n",
    "\n",
    "More complex technical documents for testing advanced techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8d3dbf5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8 technical documents\n",
      "Average length: 400 characters\n"
     ]
    }
   ],
   "source": [
    "# Technical documentation corpus\n",
    "documents = [\n",
    "    \"\"\"RAG Architecture: Retrieval-Augmented Generation combines information retrieval with \n",
    "    language generation. The retrieval component searches a knowledge base for relevant documents. \n",
    "    The generation component uses an LLM to synthesize information from retrieved documents into \n",
    "    coherent responses. This architecture reduces hallucinations by grounding generation in \n",
    "    retrieved facts.\"\"\",\n",
    "    \n",
    "    \"\"\"Dense Retrieval Methods: Dense retrieval uses neural networks to encode queries and documents \n",
    "    as dense vectors. These vectors capture semantic meaning beyond keyword matching. Models like \n",
    "    BERT and sentence transformers learn these representations through contrastive learning on \n",
    "    question-answer pairs. The main advantage is finding semantically similar content even when \n",
    "    exact words don't match.\"\"\",\n",
    "    \n",
    "    \"\"\"Cross-Encoder Reranking: Unlike bi-encoders that encode query and document separately, \n",
    "    cross-encoders process query-document pairs jointly. This allows attention mechanisms to \n",
    "    model interactions between query terms and document content. Cross-encoders are more accurate \n",
    "    but computationally expensive, making them ideal for reranking top candidates from fast \n",
    "    first-stage retrieval.\"\"\",\n",
    "    \n",
    "    \"\"\"Embedding Model Training: High-quality embeddings require careful training. Contrastive \n",
    "    learning optimizes embeddings so relevant pairs are close in vector space while irrelevant \n",
    "    pairs are far apart. Hard negative mining selects challenging examples that are similar but \n",
    "    not relevant, improving model discrimination. Training data quality matters more than \n",
    "    quantity for specialized domains.\"\"\",\n",
    "    \n",
    "    \"\"\"Context Window Management: LLMs have fixed context windows measured in tokens. Effective RAG \n",
    "    systems must fit relevant information within this limit. Strategies include selecting top-k \n",
    "    documents by relevance, truncating individual documents, and reranking to prioritize most \n",
    "    useful content. Some systems use sliding windows or hierarchical summarization for very long \n",
    "    documents.\"\"\",\n",
    "    \n",
    "    \"\"\"Query Understanding: Effective search requires understanding user intent. Query expansion \n",
    "    adds related terms to improve recall. Query rewriting reformulates ambiguous queries for \n",
    "    clarity. Some systems generate multiple query variations and merge results. Intent \n",
    "    classification routes queries to specialized retrievers. These preprocessing steps \n",
    "    significantly impact retrieval quality.\"\"\",\n",
    "    \n",
    "    \"\"\"Vector Databases: Specialized databases optimize similarity search over high-dimensional \n",
    "    vectors. FAISS uses techniques like product quantization and HNSW graphs to enable \n",
    "    sub-linear search time. Other systems like Pinecone and Weaviate add features like \n",
    "    filtering, hybrid search, and managed infrastructure. Choosing the right index type \n",
    "    balances speed, accuracy, and memory usage.\"\"\",\n",
    "    \n",
    "    \"\"\"Evaluation Metrics: RAG systems require careful evaluation. Retrieval metrics include \n",
    "    recall@k (are relevant docs retrieved?) and MRR (how highly ranked?). Generation metrics \n",
    "    assess answer quality through accuracy, completeness, and faithfulness to sources. End-to-end \n",
    "    metrics like answer correctness and user satisfaction capture overall system performance.\"\"\",\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(documents)} technical documents\")\n",
    "print(f\"Average length: {np.mean([len(d) for d in documents]):.0f} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792961f1",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2dda21dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_tokenizer(text):\n",
    "    \"\"\"Tokenize text for BM25\"\"\"\n",
    "    tokenized_doc = []\n",
    "    for token in text.lower().split():\n",
    "        token = token.strip(string.punctuation)\n",
    "        if len(token) > 0 and token not in _stop_words.ENGLISH_STOP_WORDS:\n",
    "            tokenized_doc.append(token)\n",
    "    return tokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6b5e5816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(query, results, title=\"Results\"):\n",
    "    \"\"\"Pretty print search results\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{title}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    for i, (doc, score) in enumerate(results, 1):\n",
    "        print(f\"\\n{i}. Score: {score:.4f}\")\n",
    "        print(f\"   {doc[:150]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9700003",
   "metadata": {},
   "source": [
    "## Hard Tasks\n",
    "\n",
    "Advanced RAG techniques used in production systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f0b86e",
   "metadata": {},
   "source": [
    "### Task 1: Multi-Vector Retrieval (ColBERT-style)\n",
    "\n",
    "Instead of one embedding per document, use MULTIPLE embeddings. Each sentence gets its own vector.\n",
    "\n",
    "**The idea:**\n",
    "- Split docs into sentences\n",
    "- Embed each sentence separately  \n",
    "- At search: find best matching sentence from each doc\n",
    "- Doc score = score of its best sentence\n",
    "\n",
    "Why? Single vectors lose details. Multiple vectors capture more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "73a5447f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 8 docs into 33 sentences\n"
     ]
    }
   ],
   "source": [
    "# Split each doc into sentences\n",
    "doc_sentences = []\n",
    "doc_ids = []\n",
    "\n",
    "for doc_idx, doc in enumerate(documents):\n",
    "    # Simple split on '. '\n",
    "    sents = [s.strip() for s in doc.split('. ') if len(s.strip()) > 20]\n",
    "    doc_sentences.append(sents)\n",
    "    \n",
    "    for sent in sents:\n",
    "        doc_ids.append(doc_idx)\n",
    "\n",
    "print(f\"Split {len(documents)} docs into {len(doc_ids)} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2029c6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Doc 0 has 4 sentences:\n",
      "1. RAG Architecture: Retrieval-Augmented Generation combines information retrieval ...\n",
      "2. The retrieval component searches a knowledge base for relevant documents...\n"
     ]
    }
   ],
   "source": [
    "# Check first doc\n",
    "print(f\"\\nDoc 0 has {len(doc_sentences[0])} sentences:\")\n",
    "for i, sent in enumerate(doc_sentences[0][:2], 1):\n",
    "    print(f\"{i}. {sent[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8c902f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total sentences to embed: 33\n"
     ]
    }
   ],
   "source": [
    "# Flatten for embedding\n",
    "all_sents = []\n",
    "for sents in doc_sentences:\n",
    "    all_sents.extend(sents)\n",
    "\n",
    "print(f\"\\nTotal sentences to embed: {len(all_sents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ffbc4fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 33 embeddings\n"
     ]
    }
   ],
   "source": [
    "# Embed all sentences\n",
    "sent_embs = co.embed(\n",
    "    texts=all_sents,\n",
    "    input_type=\"search_document\"\n",
    ").embeddings\n",
    "sent_embs = np.array(sent_embs)\n",
    "\n",
    "print(f\"Created {sent_embs.shape[0]} embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "45832ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Single-vector: 8 embeddings\n",
      "Multi-vector: 33 embeddings\n"
     ]
    }
   ],
   "source": [
    "# Storage cost comparison\n",
    "print(f\"\\nSingle-vector: {len(documents)} embeddings\")\n",
    "print(f\"Multi-vector: {len(all_sents)} embeddings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3629acb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index ready: 33 vectors\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Build FAISS index for all sentences\n",
    "\n",
    "# Build FAISS index\n",
    "dim = sent_embs.shape[1]\n",
    "idx_multi = faiss.IndexFlatL2(dim)\n",
    "idx_multi.add(np.float32(sent_embs))\n",
    "\n",
    "print(f\"Index ready: {idx_multi.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9c9a2fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How do cross-encoders differ from bi-encoders?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Search with multi-vector retrieval\n",
    "# For each document, we find its BEST matching sentence\n",
    "\n",
    "# Search all sentences\n",
    "query = \"How do cross-encoders differ from bi-encoders?\"\n",
    "\n",
    "q_emb = co.embed(texts=[query], input_type=\"search_query\").embeddings[0]\n",
    "dists, idxs = idx_multi.search(np.float32([q_emb]), len(all_sents))\n",
    "\n",
    "print(f\"Query: {query}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5241d26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top sentences:\n",
      "\n",
      "1. dist=5495.3833 from doc 2\n",
      "   Cross-Encoder Reranking: Unlike bi-encoders that encode query and document separately, \n",
      "    cross-en...\n",
      "\n",
      "2. dist=7663.7764 from doc 2\n",
      "   Cross-encoders are more accurate \n",
      "    but computationally expensive, making them ideal for reranking...\n",
      "\n",
      "3. dist=8085.0264 from doc 3\n",
      "   Contrastive \n",
      "    learning optimizes embeddings so relevant pairs are close in vector space while irr...\n"
     ]
    }
   ],
   "source": [
    "# Top matching sentences\n",
    "print(\"Top sentences:\")\n",
    "for i in range(3):\n",
    "    sent_idx = idxs[0][i]\n",
    "    sent = all_sents[sent_idx]\n",
    "    doc_id = doc_ids[sent_idx]\n",
    "    print(f\"\\n{i+1}. dist={dists[0][i]:.4f} from doc {doc_id}\")\n",
    "    print(f\"   {sent[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f40273cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated 8 docs\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Aggregate sentence scores to document scores\n",
    "# For each document, use the score of its BEST matching sentence\n",
    "\n",
    "# Aggregate sentence scores to doc scores\n",
    "doc_best_sc = {}\n",
    "doc_best_sent = {}\n",
    "\n",
    "for sent_idx, dist in zip(idxs[0], dists[0]):\n",
    "    doc_id = doc_ids[sent_idx]\n",
    "    sim = 1 / (1 + dist)  # convert distance to similarity\n",
    "    \n",
    "    if doc_id not in doc_best_sc or sim > doc_best_sc[doc_id]:\n",
    "        doc_best_sc[doc_id] = sim\n",
    "        doc_best_sent[doc_id] = all_sents[sent_idx]\n",
    "\n",
    "print(f\"Aggregated {len(doc_best_sc)} docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ea16a10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank by best sentence\n",
    "ranked = sorted(doc_best_sc.items(), key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a7380d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Multi-vector results:\n",
      "\n",
      "1. Doc 2 - score: 0.0002\n",
      "   Best: Cross-Encoder Reranking: Unlike bi-encoders that encode query and document separately, \n",
      "    cross-en...\n",
      "   Full: Cross-Encoder Reranking: Unlike bi-encoders that encode query and document separately, \n",
      "    cross-en...\n",
      "\n",
      "2. Doc 3 - score: 0.0001\n",
      "   Best: Contrastive \n",
      "    learning optimizes embeddings so relevant pairs are close in vector space while irr...\n",
      "   Full: Embedding Model Training: High-quality embeddings require careful training. Contrastive \n",
      "    learnin...\n",
      "\n",
      "3. Doc 1 - score: 0.0001\n",
      "   Best: Models like \n",
      "    BERT and sentence transformers learn these representations through contrastive lear...\n",
      "   Full: Dense Retrieval Methods: Dense retrieval uses neural networks to encode queries and documents \n",
      "    a...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display top 3\n",
    "print(f\"\\nMulti-vector results:\\n\")\n",
    "for i, (doc_id, sc) in enumerate(ranked[:3], 1):\n",
    "    print(f\"{i}. Doc {doc_id} - score: {sc:.4f}\")\n",
    "    print(f\"   Best: {doc_best_sent[doc_id][:100]}...\")\n",
    "    print(f\"   Full: {documents[doc_id][:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c33f02",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "1. Try a query matching specific details - does multi-vector help?\n",
    "2. What if you average all sentence scores instead of taking max?\n",
    "3. Storage tradeoff - is it worth it?\n",
    "4. When would multi-vector NOT be worth the cost?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd6d26c",
   "metadata": {},
   "source": [
    "### Task 2: Cross-Encoder Reranking\n",
    "\n",
    "Most retrieval: embed query and docs separately, compare vectors (**bi-encoder**).\n",
    "\n",
    "Cross-encoders: process query+doc TOGETHER. Model sees interactions between them.\n",
    "\n",
    "**Two-stage pattern:**\n",
    "1. Fast bi-encoder -> top 100 candidates\n",
    "2. Slow cross-encoder -> rerank top 10\n",
    "\n",
    "Why? Cross-encoders are accurate but slow. Use them only on top candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6eaa1700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading bi-encoder...\n",
      "Loaded\n",
      "Bi-encoder index built with 8 documents\n",
      "Loaded\n",
      "Bi-encoder index built with 8 documents\n"
     ]
    }
   ],
   "source": [
    "# Step 1: First stage - Fast bi-encoder retrieval\n",
    "# Get top candidates quickly\n",
    "\n",
    "# Load bi-encoder\n",
    "print(\"Loading bi-encoder...\")\n",
    "bi_enc = SentenceTransformer('BAAI/bge-small-en-v1.5')\n",
    "print(\"Loaded\")\n",
    "\n",
    "# Embed documents\n",
    "doc_embeds = bi_enc.encode(documents, convert_to_numpy=True)\n",
    "\n",
    "# Build FAISS index\n",
    "dim = doc_embeds.shape[1]\n",
    "index_bi = faiss.IndexFlatL2(dim)\n",
    "index_bi.add(np.float32(doc_embeds))\n",
    "\n",
    "print(f\"Bi-encoder index built with {index_bi.ntotal} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7c16acf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 8 docs\n"
     ]
    }
   ],
   "source": [
    "# Embed docs\n",
    "doc_embs = bi_enc.encode(documents, convert_to_numpy=True)\n",
    "print(f\"Encoded {len(documents)} docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b60e57cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index ready: 8 docs\n"
     ]
    }
   ],
   "source": [
    "# Build index\n",
    "dim = doc_embs.shape[1]\n",
    "idx_bi = faiss.IndexFlatL2(dim)\n",
    "idx_bi.add(np.float32(doc_embs))\n",
    "print(f\"Index ready: {idx_bi.ntotal} docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c7c1d712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 5 candidates\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Search with bi-encoder\n",
    "\n",
    "query = \"What are the advantages of cross-encoders over other methods?\"\n",
    "\n",
    "q_emb = bi_enc.encode([query], convert_to_numpy=True)\n",
    "dists, idxs = index_bi.search(np.float32(q_emb), 5)\n",
    "\n",
    "# Build candidates list\n",
    "candidates = []\n",
    "for idx, dist in zip(idxs[0], dists[0]):\n",
    "    candidates.append({\n",
    "        'doc_id': idx,\n",
    "        'text': documents[idx],\n",
    "        'bi_dist': dist\n",
    "    })\n",
    "print(f\"Built {len(candidates)} candidates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "cdd8d6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 5 candidates\n"
     ]
    }
   ],
   "source": [
    "# Build candidates\n",
    "candidates = []\n",
    "for idx, dist in zip(idxs[0], dists[0]):\n",
    "    candidates.append({\n",
    "        'doc_id': idx,\n",
    "        'text': documents[idx],\n",
    "        'bi_dist': dist\n",
    "    })\n",
    "\n",
    "print(f\"Built {len(candidates)} candidates\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "be6fecd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the advantages of cross-encoders over other methods?\n",
      "\n",
      "Got top 5 candidates\n",
      "Bi-encoder results:\n",
      "1. dist=0.4058\n",
      "   Cross-Encoder Reranking: Unlike bi-encoders that encode query and document separ...\n",
      "\n",
      "2. dist=0.6338\n",
      "   Embedding Model Training: High-quality embeddings require careful training. Cont...\n",
      "\n",
      "3. dist=0.6877\n",
      "   Dense Retrieval Methods: Dense retrieval uses neural networks to encode queries ...\n",
      "\n",
      "4. dist=0.7459\n",
      "   Vector Databases: Specialized databases optimize similarity search over high-dim...\n",
      "\n",
      "5. dist=0.7490\n",
      "   RAG Architecture: Retrieval-Augmented Generation combines information retrieval ...\n",
      "\n",
      "Loading cross-encoder...\n",
      "Loaded\n",
      "Loaded\n"
     ]
    }
   ],
   "source": [
    "# Search with bi-encoder\n",
    "query = \"What are the advantages of cross-encoders over other methods?\"\n",
    "\n",
    "q_emb = bi_enc.encode([query], convert_to_numpy=True)\n",
    "dists, idxs = idx_bi.search(np.float32(q_emb), 5)\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Got top 5 candidates\")\n",
    "\n",
    "# Display bi-encoder results\n",
    "print(\"Bi-encoder results:\")\n",
    "for i, c in enumerate(candidates, 1):\n",
    "    print(f\"{i}. dist={c['bi_dist']:.4f}\")\n",
    "    print(f\"   {c['text'][:80]}...\\n\")\n",
    "\n",
    "# Load cross-encoder\n",
    "print(\"Loading cross-encoder...\")\n",
    "cross_enc = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "print(\"Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "16737226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bi-encoder results:\n",
      "1. dist=0.4058\n",
      "   Cross-Encoder Reranking: Unlike bi-encoders that encode query and document separ...\n",
      "\n",
      "2. dist=0.6338\n",
      "   Embedding Model Training: High-quality embeddings require careful training. Cont...\n",
      "\n",
      "3. dist=0.6877\n",
      "   Dense Retrieval Methods: Dense retrieval uses neural networks to encode queries ...\n",
      "\n",
      "4. dist=0.7459\n",
      "   Vector Databases: Specialized databases optimize similarity search over high-dim...\n",
      "\n",
      "5. dist=0.7490\n",
      "   RAG Architecture: Retrieval-Augmented Generation combines information retrieval ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show results\n",
    "print(\"Bi-encoder results:\")\n",
    "for i, c in enumerate(candidates, 1):\n",
    "    print(f\"{i}. dist={c['bi_dist']:.4f}\")\n",
    "    print(f\"   {c['text'][:80]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ff316f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cross-encoder...\n",
      "Loaded\n",
      "\n",
      "Key difference:\n",
      "- Bi-encoder: Embeds query and doc separately, then compares vectors\n",
      "- Cross-encoder: Feeds [query, doc] together, model sees interactions\n",
      "\n",
      "This means cross-encoder can understand:\n",
      "  - Which query words match which doc parts\n",
      "  - Context and relationships between terms\n",
      "  - But it's much slower (must process each pair)\n",
      "Loaded\n",
      "\n",
      "Key difference:\n",
      "- Bi-encoder: Embeds query and doc separately, then compares vectors\n",
      "- Cross-encoder: Feeds [query, doc] together, model sees interactions\n",
      "\n",
      "This means cross-encoder can understand:\n",
      "  - Which query words match which doc parts\n",
      "  - Context and relationships between terms\n",
      "  - But it's much slower (must process each pair)\n"
     ]
    }
   ],
   "source": [
    "# Load cross-encoder\n",
    "print(\"Loading cross-encoder...\")\n",
    "cross_enc = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "print(\"Loaded\")\n",
    "\n",
    "print(\"\\nKey difference:\")\n",
    "print(\"- Bi-encoder: Embeds query and doc separately, then compares vectors\")\n",
    "print(\"- Cross-encoder: Feeds [query, doc] together, model sees interactions\")\n",
    "print(\"\\nThis means cross-encoder can understand:\")\n",
    "print(\"  - Which query words match which doc parts\")\n",
    "print(\"  - Context and relationships between terms\")\n",
    "print(\"  - But it's much slower (must process each pair)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bdee719b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  5.4480724 -10.7306     -1.4447366  -7.91539    -9.924572 ]\n"
     ]
    }
   ],
   "source": [
    "# Rerank with cross-encoder\n",
    "pairs = [[query, cand['text']] for cand in candidates]\n",
    "\n",
    "# Get scores (higher = more relevant)\n",
    "sc = cross_enc.predict(pairs)\n",
    "\n",
    "for cand, score in zip(candidates, sc):\n",
    "    cand['cross_sc'] = score\n",
    "\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f42e94ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After reranking:\n",
      "\n",
      "1. cross_sc=5.4481 (bi_dist was 0.4058)\n",
      "   Cross-Encoder Reranking: Unlike bi-encoders that encode query and document separ...\n",
      "\n",
      "2. cross_sc=-1.4447 (bi_dist was 0.6877)\n",
      "   Dense Retrieval Methods: Dense retrieval uses neural networks to encode queries ...\n",
      "\n",
      "3. cross_sc=-7.9154 (bi_dist was 0.7459)\n",
      "   Vector Databases: Specialized databases optimize similarity search over high-dim...\n",
      "\n",
      "4. cross_sc=-9.9246 (bi_dist was 0.7490)\n",
      "   RAG Architecture: Retrieval-Augmented Generation combines information retrieval ...\n",
      "\n",
      "5. cross_sc=-10.7306 (bi_dist was 0.6338)\n",
      "   Embedding Model Training: High-quality embeddings require careful training. Cont...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show reranked results\n",
    "reranked = sorted(candidates, key=lambda x: x['cross_sc'], reverse=True)\n",
    "\n",
    "print(\"\\nAfter reranking:\\n\")\n",
    "for i, c in enumerate(reranked, 1):\n",
    "    print(f\"{i}. cross_sc={c['cross_sc']:.4f} (bi_dist was {c['bi_dist']:.4f})\")\n",
    "    print(f\"   {c['text'][:80]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "71e2f28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ranking comparison:\n",
      "\n",
      "Pos | Bi-Encoder | Cross-Encoder\n",
      "----------------------------------------\n",
      " 1  |    Doc 2   |    Doc 2\n",
      " 2  |    Doc 3   |    Doc 1 <- changed\n",
      " 3  |    Doc 1   |    Doc 6 <- changed\n",
      " 4  |    Doc 6   |    Doc 0 <- changed\n",
      " 5  |    Doc 0   |    Doc 3 <- changed\n"
     ]
    }
   ],
   "source": [
    "# Compare rankings\n",
    "print(\"\\nRanking comparison:\\n\")\n",
    "print(\"Pos | Bi-Encoder | Cross-Encoder\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for i in range(len(candidates)):\n",
    "    bi_doc = candidates[i]['doc_id']\n",
    "    cross_doc = reranked[i]['doc_id']\n",
    "    changed = \" <- changed\" if bi_doc != cross_doc else \"\"\n",
    "    print(f\" {i+1}  |    Doc {bi_doc}   |    Doc {cross_doc}{changed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caacb41",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "1. Try different queries - does reranking always help?\n",
    "2. What about very specific technical queries?\n",
    "3. Retrieve 10 candidates instead of 5 - does reranking improve more?\n",
    "4. When is the extra cost NOT worth it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a27b8cc",
   "metadata": {},
   "source": [
    "### Task 3: Hypothetical Document Embeddings (HyDE)\n",
    "\n",
    "Weird idea: instead of searching with your query, generate a FAKE answer first, then search for docs similar to that fake answer.\n",
    "\n",
    "**Why it works:**\n",
    "- Questions are short and abstract\n",
    "- Answers are detailed and concrete  \n",
    "- Real docs contain answer-like text\n",
    "- Fake answer is more similar to real answers than question is\n",
    "\n",
    "**Process:**\n",
    "1. User: \"What is RAG?\"\n",
    "2. LLM: generates fake answer\n",
    "3. Embed fake answer, search\n",
    "4. Find real docs about RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4f7d3ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating doc embeddings...\n",
      "Created 8 embeddings\n",
      "Created 8 embeddings\n"
     ]
    }
   ],
   "source": [
    "# Create doc embeddings\n",
    "print(\"Creating doc embeddings...\")\n",
    "doc_embs = co.embed(texts=documents, input_type=\"search_document\").embeddings\n",
    "doc_embs = np.array(doc_embs)\n",
    "\n",
    "print(f\"Created {len(doc_embs)} embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "97736c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index ready: 8 docs\n"
     ]
    }
   ],
   "source": [
    "# Build index\n",
    "dim = doc_embs.shape[1]\n",
    "idx_hyde = faiss.IndexFlatL2(dim)\n",
    "idx_hyde.add(np.float32(doc_embs))\n",
    "\n",
    "print(f\"Index ready: {idx_hyde.ntotal} docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "67a3dce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What makes dense retrieval better than keyword search?\n",
      "\n",
      "Baseline (direct query):\n",
      "1. dist=4093.2412\n",
      "   Dense Retrieval Methods: Dense retrieval uses neural networks to encode queries ...\n",
      "\n",
      "2. dist=7746.4697\n",
      "   Query Understanding: Effective search requires understanding user intent. Query ...\n",
      "\n",
      "3. dist=7979.4609\n",
      "   Cross-Encoder Reranking: Unlike bi-encoders that encode query and document separ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Baseline - search with query directly\n",
    "query = \"What makes dense retrieval better than keyword search?\"\n",
    "\n",
    "q_emb = co.embed(texts=[query], input_type=\"search_query\").embeddings[0]\n",
    "dists, idxs = idx_hyde.search(np.float32([q_emb]), 3)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Baseline (direct query):\")\n",
    "for i, (idx, dist) in enumerate(zip(idxs[0], dists[0]), 1):\n",
    "    print(f\"{i}. dist={dist:.4f}\")\n",
    "    print(f\"   {documents[idx][:80]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "fd564b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Generate hypothetical answer with LLM\n",
    "# This is the key step - we create a fake answer\n",
    "\n",
    "# Generate fake answer\n",
    "prompt = f\"\"\"Answer this in 2-3 sentences. Be technical.\\n\\nQuestion: {query}\\n\\nAnswer:\"\"\"\n",
    "\n",
    "resp = co.chat(message=prompt, max_tokens=150, temperature=0.3)\n",
    "hypo_doc = resp.text.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8695c03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense retrieval leverages deep learning models to embed queries and documents into a continuous vector space, capturing semantic relationships that keyword search cannot. This allows dense retrieval to match queries with relevant documents based on contextual meaning rather than exact term overlap, improving recall and precision, especially for complex or ambiguous queries. Additionally, dense retrieval can handle synonyms, polysemy, and contextual nuances more effectively than traditional keyword-based methods.\n"
     ]
    }
   ],
   "source": [
    "print(hypo_doc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f598dc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed fake answer\n",
    "hyde_emb = co.embed(\n",
    "    texts=[hypo_doc],\n",
    "    input_type=\"search_query\"\n",
    ").embeddings[0]\n",
    "\n",
    "dists_h, idxs_h = idx_hyde.search(np.float32([hyde_emb]), 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "159ce70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed fake answer\n",
    "hyde_emb = co.embed(\n",
    "    texts=[hypo_doc],\n",
    "    input_type=\"search_query\"\n",
    ").embeddings[0]\n",
    "\n",
    "dists_h, idxs_h = idx_hyde.search(np.float32([hyde_emb]), 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "23a9a5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "With fake answer:\n",
      "\n",
      "1. dist=2555.9355\n",
      "   Dense Retrieval Methods: Dense retrieval uses neural networks to encode queries ...\n",
      "\n",
      "2. dist=7315.6992\n",
      "   Cross-Encoder Reranking: Unlike bi-encoders that encode query and document separ...\n",
      "\n",
      "3. dist=7566.8037\n",
      "   Query Understanding: Effective search requires understanding user intent. Query ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show results\n",
    "print(\"\\nWith fake answer:\\n\")\n",
    "for i, (idx, dist) in enumerate(zip(idxs_h[0], dists_h[0]), 1):\n",
    "    print(f\"{i}. dist={dist:.4f}\")\n",
    "    print(f\"   {documents[idx][:80]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "772e9a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nComparison:\\n\n",
      "Direct query:\n",
      "  1. Doc 1\n",
      "  2. Doc 5\n",
      "  3. Doc 2\n",
      "\\nHyDE:\n",
      "  1. Doc 1\n",
      "  2. Doc 2\n",
      "  3. Doc 5\n"
     ]
    }
   ],
   "source": [
    "# Compare results\n",
    "print(\"\\\\nComparison:\\\\n\")\n",
    "print(\"Direct query:\")\n",
    "for i, idx in enumerate(idxs[0], 1):\n",
    "    print(f\"  {i}. Doc {idx}\")\n",
    "\n",
    "print(\"\\\\nHyDE:\")\n",
    "for i, idx in enumerate(idxs_h[0], 1):\n",
    "    print(f\"  {i}. Doc {idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a32b3a",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "1. Try a specific technical question - does HyDE help more?\n",
    "2. Generate multiple hypothetical answers and combine results?\n",
    "3. Change temperature to 0.7 - does more creative fake answer help?\n",
    "4. When might HyDE give WORSE results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7775087",
   "metadata": {},
   "source": [
    "### Task 4: Self-Correcting RAG\n",
    "\n",
    "RAG system that checks its own answers.\n",
    "\n",
    "**Process:**\n",
    "1. Retrieve docs\n",
    "2. Generate answer\n",
    "3. Verify: is answer supported by docs?\n",
    "4. If not: retrieve more OR say \"I don't know\"\n",
    "\n",
    "Self-verification catches hallucinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b631dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up retrieval system...\n",
      "Ready to retrieve from 8 documents\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Set up retrieval (reuse embeddings from earlier)\n",
    "\n",
    "print(\"Setting up retrieval system...\")\n",
    "\n",
    "# We already have doc_embeddings and index_hyde from previous task\n",
    "print(f\"Ready to retrieve from {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f17f44ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How does hard negative mining improve embedding quality?\n",
      "\n",
      "Retrieved:\n",
      "1. Embedding Model Training: High-quality embeddings require careful training. Cont...\n",
      "\n",
      "2. Dense Retrieval Methods: Dense retrieval uses neural networks to encode queries ...\n",
      "\n",
      "3. Cross-Encoder Reranking: Unlike bi-encoders that encode query and document separ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Retrieve documents and generate answer\n",
    "\n",
    "# Retrieve docs\n",
    "query = \"How does hard negative mining improve embedding quality?\"\n",
    "\n",
    "q_emb = co.embed(texts=[query], input_type=\"search_query\").embeddings[0]\n",
    "dists, idxs = idx_hyde.search(np.float32([q_emb]), 3)\n",
    "\n",
    "ret_docs = [documents[idx] for idx in idxs[0]]\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Retrieved:\")\n",
    "for i, doc in enumerate(ret_docs, 1):\n",
    "    print(f\"{i}. {doc[:80]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "150b275c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated initial answer\n"
     ]
    }
   ],
   "source": [
    "# Generate initial answer\n",
    "ctx = \"\\n\\n\".join([f\"Doc {i+1}: {doc}\" for i, doc in enumerate(ret_docs)])\n",
    "\n",
    "prompt = f\"\"\"Answer based on these docs.\\n\\nDocs:\\n{ctx}\\n\\nQuestion: {query}\\n\\nAnswer:\"\"\"\n",
    "\n",
    "resp = co.chat(message=prompt, max_tokens=150, temperature=0.3)\n",
    "initial_ans = resp.text.strip()\n",
    "\n",
    "print(\"Generated initial answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856e3923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verdict: SUPPORTED\n",
      "\n",
      "The answer is fully supported by the information provided in Doc 1, which explicitly states that hard negative mining selects challenging examples that are similar but not relevant, thereby improving the model's discrimination ability. The answer accurately reflects this mechanism and its impact on embedding quality, including the emphasis on training data quality in specialized domains.\n",
      "\n",
      "Answer supported!\n",
      "{'Final': 'Hard negative mining improves embedding quality by selecting challenging examples that are similar to the relevant pairs but not actually relevant. This process enhances the model\\'s ability to discriminate between relevant and irrelevant pairs in the vector space. By focusing on these \"hard negatives,\" the model learns to better distinguish subtle differences, leading to more accurate and robust embeddings. This technique is particularly effective in specialized domains where training data quality is more critical than quantity.'}\n"
     ]
    }
   ],
   "source": [
    "# Verify answer\n",
    "verif_prompt = f\"\"\"Is this answer supported by the docs?\\n\\nDocs:\\n{ctx}\\n\\nQuestion: {query}\\nAnswer: {initial_ans}\\n\\nRespond: SUPPORTED, UNSUPPORTED, or PARTIAL\\n\\nVerification:\"\"\"\n",
    "\n",
    "verif = co.chat(message=verif_prompt, max_tokens=100, temperature=0.0)\n",
    "verdict = verif.text.strip()\n",
    "\n",
    "print(f\"Verdict: {verdict}\")\n",
    "\n",
    "# Decide what to do\n",
    "if \"UNSUPPORTED\" in verdict.upper() or \"PARTIAL\" in verdict.upper():\n",
    "    print(\"\\n  Not fully supported. Retrieving more...\\n\")\n",
    "    \n",
    "    dists_m, idxs_m = idx_hyde.search(np.float32([q_emb]), 6)\n",
    "    add_docs = [documents[idx] for idx in idxs_m[0][3:]]\n",
    "    \n",
    "    print(f\"Retrieved {len(add_docs)} more docs\")\n",
    "else:\n",
    "    print(\"\\nAnswer supported\")\n",
    "    print({\"Final\": initial_ans})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "93f84e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate revised answer\n",
    "if \"UNSUPPORTED\" in verdict.upper() or \"PARTIAL\" in verdict.upper():\n",
    "    all_docs = ret_docs + add_docs\n",
    "    exp_ctx = \"\\n\\n\".join([f\"Doc {i+1}: {doc}\" for i, doc in enumerate(all_docs)])\n",
    "    \n",
    "    rev_prompt = f\"\"\"Answer with ALL docs.\\n\\nDocs:\\n{exp_ctx}\\n\\nQuestion: {query}\\n\\nAnswer:\"\"\"\n",
    "    \n",
    "    rev_resp = co.chat(message=rev_prompt, max_tokens=200, temperature=0.3)\n",
    "    final_ans = rev_resp.text.strip()\n",
    "    \n",
    "    print(\"\\nRevised answer:\")\n",
    "    print(final_ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561f5260",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "1. Try a question that can't be answered from these docs - what happens?\n",
    "2. What if verification is too strict?\n",
    "3. Add a 3rd iteration?\n",
    "4. How to handle cases where more docs don't help?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ads525",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
