{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IyffKxPbyhq"
      },
      "source": "# Chapter 4: Text Classification - Medium Tasks (Solutions)\n\nComplete working solutions."
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOxEdnfDbyhu"
      },
      "source": "---\n\n## Setup\n\nRun all cells in this section to set up the environment and load necessary data.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LGW2SD-c864"
      },
      "source": "### [Optional] - Installing Packages on <img src=\"https://colab.google/static/images/icons/colab.png\" width=100>\n\nIf you are viewing this notebook on Google Colab (or any other cloud vendor), you need to **uncomment and run** the following codeblock to install the dependencies for this chapter:\n\n---\n\n **Note**: We will want to use a GPU to run the examples in this notebook. In Google Colab, go to\n**Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4**.\n\n---\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "N-PxmOIhc865"
      },
      "outputs": [],
      "source": " %%capture\n!pip install transformers sentence-transformers openai\n!pip install -U datasets"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APb1rVC1byhy"
      },
      "source": "### Data Loading\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5phRS_z2U_3T",
        "outputId": "1a0a97f1-625f-4b1d-dbd7-d9c2f7375225"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 8530\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 1066\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 1066\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": "from datasets import load_dataset\n# Load our data\ndata = load_dataset(\"rotten_tomatoes\")\ndata"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30nUV21qbyh0"
      },
      "source": "### Helper Functions\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "X0KyKHtqyjn3"
      },
      "outputs": [],
      "source": "from sklearn.metrics import classification_report\ndef evaluate_performance(y_true, y_pred):\n    \"\"\"Create and print the classification report\"\"\"\n    performance = classification_report(\n        y_true, y_pred,\n        target_names=[\"Negative Review\", \"Positive Review\"]\n    )\n    print(performance)"
    },
    {
      "cell_type": "markdown",
      "source": "## Your Turn - Text Classification Experiments\n\nRun each task first to see the baseline results. Follow the instructions to modify and experiment.",
      "metadata": {
        "id": "NKYNfoaVC4hU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlVd1GVpbyh5"
      },
      "source": "---\n\n## Medium Tasks\n"
    },
    {
      "cell_type": "markdown",
      "source": "### Medium Tasks - Building Real Classifiers\n\nThese tasks require more modification and experimentation. You'll build complete classification systems.",
      "metadata": {
        "id": "2ipz0pJYEne6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uLZBrWMbyh8"
      },
      "source": "Run the code to see how 5-level classification works. Then try adding a 6th category.\n"
    },
    {
      "cell_type": "markdown",
      "source": "#### Medium Task 1: Multi-Class Sentiment Classification\nIn this task, you'll build a sentiment classifier with 5 different categories (from extremely negative to extremely positive) instead of just binary positive/negative.\n**What to do:**\n1. Run the cells below to see baseline 5-level classification\n2. Observe which reviews are uncertain (low margin between top predictions)\n3. Try uncommenting the 6-level version to add more granularity\n4. Compare how predictions change with more categories",
      "metadata": {
        "id": "3CZEijHVcMGI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PXpGfijbyh-"
      },
      "source": "Set up the 5 sentiment categories and compute embeddings:"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7ceixsfybyh-"
      },
      "outputs": [],
      "source": "from sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nmodel = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3B5rf4Dbyh_"
      },
      "source": "Classify each review and show confidence scores:"
    },
    {
      "cell_type": "code",
      "source": "# Define 5 sentiment categories\nsentiment_labels = [\n    \"extremely negative review\",\n    \"somewhat negative review\",\n    \"neutral review\",\n    \"somewhat positive review\",\n    \"extremely positive review\"\n]\n\n# Create embeddings for each category\nlabel_embeddings = model.encode(sentiment_labels)\n\nprint(\"Sentiment categories:\")\nfor i, label in enumerate(sentiment_labels):\n    print(f\"  {i}: {label}\")",
      "metadata": {
        "id": "x7dk3IHDbyh_",
        "outputId": "39b1f9f2-73d4-439e-fa6c-55d9471ac31a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment categories:\n",
            "  0: extremely negative review\n",
            "  1: somewhat negative review\n",
            "  2: neutral review\n",
            "  3: somewhat positive review\n",
            "  4: extremely positive review\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "# Test on some sample reviews\ntest_reviews = [\n    \"This is the best movie I have ever seen! Absolute masterpiece!\",\n    \"Pretty good film, I enjoyed it\",\n    \"It was okay, nothing special\",\n    \"Not very good, quite boring\",\n    \"Terrible movie, waste of time\"\n]\n\nreview_embeddings = model.encode(test_reviews)\nsimilarities = cosine_similarity(review_embeddings, label_embeddings)\n\nprint(\"Classification results:\")\nfor i, review in enumerate(test_reviews):\n    predicted_category = sentiment_labels[similarities[i].argmax()]\n    confidence = similarities[i].max()\n    top_two_scores = sorted(similarities[i], reverse=True)[:2]\n    margin = top_two_scores[0] - top_two_scores[1]\n\n    print(f\"\\n'{review}'\")\n    print(f\"  \u2192 {predicted_category}\")\n    print(f\"  Confidence: {confidence:.3f} | Margin: {margin:.3f}\")",
      "metadata": {
        "id": "SlF4O-VVbyiA",
        "outputId": "55bcff11-fff2-48ee-8dda-5d8f560466e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification results:\n",
            "\n",
            "'This is the best movie I have ever seen! Absolute masterpiece!'\n",
            "  \u2192 extremely positive review\n",
            "  Confidence: 0.256 | Margin: 0.099\n",
            "\n",
            "'Pretty good film, I enjoyed it'\n",
            "  \u2192 somewhat positive review\n",
            "  Confidence: 0.428 | Margin: 0.073\n",
            "\n",
            "'It was okay, nothing special'\n",
            "  \u2192 somewhat negative review\n",
            "  Confidence: 0.439 | Margin: 0.004\n",
            "\n",
            "'Not very good, quite boring'\n",
            "  \u2192 somewhat positive review\n",
            "  Confidence: 0.378 | Margin: 0.000\n",
            "\n",
            "'Terrible movie, waste of time'\n",
            "  \u2192 extremely negative review\n",
            "  Confidence: 0.426 | Margin: 0.009\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZPl8Fu08byiB",
        "outputId": "010508bf-0d43-4622-9435-0236662f931b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CATEGORY CONFUSION ANALYSIS\n",
            "Category Pair                                                Similarity  \n",
            " extremely negative review <-> somewhat negative review       0.952\n",
            " somewhat negative review <-> somewhat positive review        0.863\n",
            " somewhat positive review <-> extremely positive review       0.818\n",
            " somewhat negative review <-> neutral review                  0.794\n",
            " extremely negative review <-> somewhat positive review       0.766\n",
            " extremely negative review <-> neutral review                 0.748\n",
            " neutral review <-> somewhat positive review                  0.748\n",
            "somewhat negative review <-> extremely positive review       0.674\n",
            "extremely negative review <-> extremely positive review      0.664\n",
            "neutral review <-> extremely positive review                 0.616\n"
          ]
        }
      ],
      "source": "print()\nprint(\"Category Confusion Analysis\")\nlabel_similarity = cosine_similarity(label_embeddings)\nprint(f\"{'Category Pair':<60s} {'Similarity':<12s}\")\nconfusions = []\nfor i in range(len(sentiment_labels)):\n    for j in range(i+1, len(sentiment_labels)):\n        sim = label_similarity[i][j]\n        confusions.append((i, j, sim))\nfor i, j, sim in sorted(confusions, key=lambda x: x[2], reverse=True)[:10]:\n    pair_name = f\"{sentiment_labels[i]} <-> {sentiment_labels[j]}\"\n    marker = \" \" if sim > 0.7 else \"\"\n    print(f\"{marker}{pair_name:<60s} {sim:.3f}\")"
    },
    {
      "cell_type": "markdown",
      "source": "As you can see, the classifier assigns each review to one of the 5 sentiment categories. The **margin** (difference between top 2 predictions) indicates confidence - large margins (>0.15) mean the model is confident, while small margins (<0.05) indicate uncertainty. Reviews with extreme language (\"best ever\", \"terrible\") have higher confidence, while moderate reviews (\"pretty good\", \"quite bad\") show more uncertainty.",
      "metadata": {
        "id": "fECdQPWjbyiC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFy4lcn9byiD"
      },
      "source": "Analyze which categories are most similar to each other:"
    },
    {
      "cell_type": "markdown",
      "source": "Notice that adjacent categories (like \"somewhat negative\" and \"neutral\") tend to have higher similarity scores, which explains why the model sometimes confuses them. Categories with similarity > 0.7 are particularly prone to confusion.",
      "metadata": {
        "id": "EV2pv7pbbyiD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zL_xe6AabyiE"
      },
      "source": "#### Medium Task 2: Classifier Performance with Limited Training Data\n\nTry different training sizes (100, 500, 1000, 2000) and fill in the results table.\n"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JeFbrbUobyiE",
        "outputId": "54740af4-1f3a-4922-d26e-300e53705cc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EXPERIMENT: Training Size = 1000\n"
          ]
        }
      ],
      "source": "from transformers import pipeline\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score, classification_report\nfrom datasets import load_dataset\nimport numpy as np\ndata = load_dataset(\"rotten_tomatoes\")\n# Try different values: 100, 500, 1000, 2000, 5000\ntrain_size = 1000\ntest_size = 300\ntrain_subset = data[\"train\"].shuffle(seed=42).select(range(min(train_size, len(data[\"train\"]))))\ntest_subset = data[\"test\"].shuffle(seed=42).select(range(test_size))\nprint(f\"Experiment: Training Size = {train_size}\")"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9_26CEFobyiF",
        "outputId": "9b387913-a338-42bc-c254-dc49d65a5e04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[1/2] Testing Task-Specific Model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Task-Specific Model F1: 0.7709\n"
          ]
        }
      ],
      "source": "print(\"\\n[1/2] Testing Task-Specific Model...\")\ntask_model = pipeline(\n    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n    tokenizer=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n    return_all_scores=True,\n    device=-1\n)\ny_pred_task = []\nfor text in test_subset[\"text\"]:\n    output = task_model(text)[0]\n    neg_score = output[0][\"score\"]\n    pos_score = output[2][\"score\"]\n    y_pred_task.append(1 if pos_score > neg_score else 0)\ntask_f1 = f1_score(test_subset[\"label\"], y_pred_task, average='weighted')\nprint(f\" Task-Specific Model F1: {task_f1:.4f}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYLELJ-hbyiF"
      },
      "source": "Train the embedding-based classifier on your labeled data:"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "q7Jgae8WbyiG",
        "outputId": "357918b4-8dfd-47a9-ed4c-0c48c64f97b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[2/2] Training Embedding Classifier on 1000 samples...\n",
            " Embedding Classifier F1: 0.8699\n"
          ]
        }
      ],
      "source": "print(f\"\\n[2/2] Training Embedding Classifier on {train_size} samples...\")\nembedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\ntrain_embeddings = embedding_model.encode(train_subset[\"text\"], show_progress_bar=False)\ntest_embeddings = embedding_model.encode(test_subset[\"text\"], show_progress_bar=False)\nclf = LogisticRegression(random_state=42, max_iter=1000)\nclf.fit(train_embeddings, train_subset[\"label\"])\ny_pred_embed = clf.predict(test_embeddings)\nembed_f1 = f1_score(test_subset[\"label\"], y_pred_embed, average='weighted')\nprint(f\" Embedding Classifier F1: {embed_f1:.4f}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acpiODYAbyiH"
      },
      "source": "Compare the two approaches and show example predictions:"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KKZ7v0nvbyiH",
        "outputId": "ea37226c-2749-45ef-c812-b21d530dee54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "RESULTS SUMMARY\n",
            "Training samples used: 1000\n",
            "\n",
            "Task-Specific (pre-trained):  F1 = 0.7709\n",
            "Embedding + Classifier:       F1 = 0.8699\n",
            "Difference:                       +0.0990\n",
            "\n",
            " Embedding approach WINS with 1000 samples!\n",
            "\n",
            "EXAMPLE PREDICTIONS (first 5)\n",
            "\n",
            "1. 'unpretentious , charming , quirky , original...'\n",
            "   True: Positive\n",
            "   Task-Specific: Positive \n",
            "   Embedding:     Positive \n",
            "\n",
            "2. 'a film really has to be exceptional to justify a three hour ...'\n",
            "   True: Negative\n",
            "   Task-Specific: Negative \n",
            "   Embedding:     Negative \n",
            "\n",
            "3. 'working from a surprisingly sensitive script co-written by g...'\n",
            "   True: Positive\n",
            "   Task-Specific: Positive \n",
            "   Embedding:     Positive \n",
            "\n",
            "4. 'it may not be particularly innovative , but the film's crisp...'\n",
            "   True: Positive\n",
            "   Task-Specific: Positive \n",
            "   Embedding:     Positive \n",
            "\n",
            "5. 'such a premise is ripe for all manner of lunacy , but kaufma...'\n",
            "   True: Negative\n",
            "   Task-Specific: Negative \n",
            "   Embedding:     Negative \n",
            "\n",
            "TODO: Record your results\n",
            "Current: | 1000       | 0.7709  | 0.8699       | Embed       |\n"
          ]
        }
      ],
      "source": "print(\"\\nResults Summary\")\nprint(f\"Training samples: {train_size}\")\nprint(f\"Task-Specific F1: {task_f1:.4f}\")\nprint(f\"Embedding F1: {embed_f1:.4f}\")\nprint(f\"Difference: {embed_f1 - task_f1:+.4f}\")\n\nprint(\"\\nExample predictions:\")\nfor i in range(3):\n    print(f\"\\n'{test_subset['text'][i][:50]}...'\")\n    print(f\"Task-Specific: {y_pred_task[i]} | Embedding: {y_pred_embed[i]}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCWBW3qObyiH"
      },
      "source": "### Questions\n\n1. At what training size did embedding classifier match the task-specific model?\n\n2. Were there cases where one model was correct and the other wrong?\n\n3. Is 100 samples enough labeled data?\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKfJlPDtbyiI"
      },
      "source": "#### Medium Task 3: Confidence-Based Classifier with Uncertainty Handling\n\nIn production, refusing a prediction beats making a wrong one. Here's the key insight: when your model is uncertain, it should say \"I don't know\" rather than guessing. This creates a trade-off between coverage (how many predictions you make) and accuracy (how often you're right).\n\nTry this:\n- Run with threshold of 0.15 first\n- Test 0.05, 0.30, and 0.50 to see how the trade-off shifts\n- Check the uncertain cases (typically have hedging language)\n- Experiment with the alternative uncertainty measure"
    },
    {
      "cell_type": "code",
      "source": "from sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np",
      "metadata": {
        "id": "UrlfyjrabyiJ"
      },
      "outputs": [],
      "execution_count": 12
    },
    {
      "cell_type": "code",
      "source": "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n\n# Reviews with varying levels of clarity\ntest_reviews = [\n    \"Absolutely fantastic! Best movie ever!\",           # Clear positive\n    \"Pretty good, I liked it\",                          # Weak positive\n    \"It was fine, nothing special\",                     # Ambiguous\n    \"Not bad but not great either\",                     # Very ambiguous\n    \"Quite disappointing\",                              # Weak negative\n    \"Terrible! Complete waste of time!\",                # Clear negative\n    \"The movie had some interesting moments\",           # Ambiguous positive\n    \"Outstanding performances all around!\",             # Clear positive\n]\n\n# True labels (for evaluation)\ny_true = [1, 1, 0, 0, 0, 0, 1, 1]  # 1=positive, 0=negative",
      "metadata": {
        "id": "9sf1K6sibyiJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "labels = [\"negative\", \"positive\"]\n\n# Experiment WITH THIS - Try: 0.05, 0.15, 0.30, 0.50\nconfidence_threshold = 0.15",
      "metadata": {
        "id": "jLZjsRNVbyiK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def calculate_margin(similarities):\n    \"\"\"\n    Margin = difference between top two predictions\n    Small margin = uncertain (predictions are close)\n    \"\"\"\n    sorted_sims = np.sort(similarities)[::-1]\n    margin = sorted_sims[0] - sorted_sims[1]\n    return margin\n\n# After first run, uncomment this alternative uncertainty measure:\n# def calculate_margin(similarities):\n#     \"\"\"\n#     Alternative: Use absolute confidence in top prediction\n#     Low confidence = uncertain\n#     \"\"\"\n#     max_confidence = np.max(similarities)\n#     # Convert to margin-like score (higher = more certain)\n#     # If max is 0.6, margin = 0.6 - 0.5 = 0.1 (uncertain)\n#     # If max is 0.9, margin = 0.9 - 0.5 = 0.4 (certain)\n#     margin = max_confidence - 0.5\n#     return margin",
      "metadata": {
        "id": "Ss-J6mVsbyiL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "label_embeddings = model.encode(labels)\nreview_embeddings = model.encode(test_reviews)\nsim_matrix = cosine_similarity(review_embeddings, label_embeddings)",
      "metadata": {
        "id": "htw33fKTbyiL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Classify with confidence threshold\nresults = []\npredictions = []\n\nprint(f\"Confidence-Based Classification (threshold={confidence_threshold})\")\n\nfor i, review in enumerate(test_reviews):\n    similarities = sim_matrix[i]\n    predicted_idx = np.argmax(similarities)\n    margin = calculate_margin(similarities)\n    \n    # Predict only if confident enough\n    if margin >= confidence_threshold:\n        prediction = predicted_idx\n        predictions.append(prediction)\n    else:\n        prediction = None\n        predictions.append(None)\n    \n    pred_label = labels[predicted_idx] if prediction is not None else \"uncertain\"\n    print(f\"{i+1}. '{review}' -> {pred_label} (margin: {margin:.3f})\")\n    \n    results.append({'pred': prediction, 'true': y_true[i], 'margin': margin})",
      "metadata": {
        "id": "oDGa7ACXbyiL",
        "outputId": "1efd9f11-2ffb-4e0b-e9d4-facdca0d0599",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CONFIDENCE-BASED CLASSIFICATION (threshold=0.15)\n",
            "================================================================================\n",
            "\n",
            "1. 'Absolutely fantastic! Best movie ever!'\n",
            "   True label: Positive\n",
            "   Prediction: UNCERTAIN\n",
            "   Top confidence: 0.173\n",
            "   Margin: 0.047 \u2717 Below threshold\n",
            "   Status: UNCERTAIN\n",
            "\n",
            "2. 'Pretty good, I liked it'\n",
            "   True label: Positive\n",
            "   Prediction: UNCERTAIN\n",
            "   Top confidence: 0.217\n",
            "   Margin: 0.033 \u2717 Below threshold\n",
            "   Status: UNCERTAIN\n",
            "\n",
            "3. 'It was fine, nothing special'\n",
            "   True label: Negative\n",
            "   Prediction: UNCERTAIN\n",
            "   Top confidence: 0.254\n",
            "   Margin: 0.055 \u2717 Below threshold\n",
            "   Status: UNCERTAIN\n",
            "\n",
            "4. 'Not bad but not great either'\n",
            "   True label: Negative\n",
            "   Prediction: UNCERTAIN\n",
            "   Top confidence: 0.209\n",
            "   Margin: 0.003 \u2717 Below threshold\n",
            "   Status: UNCERTAIN\n",
            "\n",
            "5. 'Quite disappointing'\n",
            "   True label: Negative\n",
            "   Prediction: UNCERTAIN\n",
            "   Top confidence: 0.276\n",
            "   Margin: 0.061 \u2717 Below threshold\n",
            "   Status: UNCERTAIN\n",
            "\n",
            "6. 'Terrible! Complete waste of time!'\n",
            "   True label: Negative\n",
            "   Prediction: UNCERTAIN\n",
            "   Top confidence: 0.324\n",
            "   Margin: 0.062 \u2717 Below threshold\n",
            "   Status: UNCERTAIN\n",
            "\n",
            "7. 'The movie had some interesting moments'\n",
            "   True label: Positive\n",
            "   Prediction: UNCERTAIN\n",
            "   Top confidence: 0.169\n",
            "   Margin: 0.054 \u2717 Below threshold\n",
            "   Status: UNCERTAIN\n",
            "\n",
            "8. 'Outstanding performances all around!'\n",
            "   True label: Positive\n",
            "   Prediction: UNCERTAIN\n",
            "   Top confidence: 0.141\n",
            "   Margin: 0.068 \u2717 Below threshold\n",
            "   Status: UNCERTAIN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "# Calculate metrics\nmade_predictions = [r for r in results if r['pred'] is not None]\ncorrect = [r for r in made_predictions if r['pred'] == r['true']]\n\ncoverage = len(made_predictions) / len(results)\naccuracy = len(correct) / len(made_predictions) if made_predictions else 0\n\nprint(f\"\\nPerformance:\")\nprint(f\"Coverage: {len(made_predictions)}/{len(results)} = {coverage:.1%}\")\nprint(f\"Accuracy: {len(correct)}/{len(made_predictions)} = {accuracy:.1%}\")",
      "metadata": {
        "id": "Oc38LloRbyiM",
        "outputId": "4e586ec6-6114-4736-85d9-7e44561c9331",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "PERFORMANCE ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "Coverage: 0/8 = 0.0%\n",
            "  \u2192 Made predictions for 0 reviews\n",
            "  \u2192 Refused to predict on 8 reviews\n",
            "\n",
            "Accuracy (on predictions made): 0/0 = 0.0%\n",
            "  \u2192 Of the 0 predictions, 0 were correct\n",
            "\n",
            "Trade-off Analysis:\n",
            "  Threshold = 0.15\n",
            "  \u2192 Higher threshold = fewer predictions but higher accuracy\n",
            "  \u2192 Lower threshold = more predictions but lower accuracy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "### Questions\n\n1. What do uncertain reviews have in common? Are they using hedging language like \"kind of\" or \"somewhat\"?\n\n2. Compare results at threshold=0.05 vs 0.30. Describe the coverage vs accuracy trade-off. When would you want high coverage vs high accuracy?\n\n3. How could you use confidence-based prediction in production? What should a system do when the model is uncertain?",
      "metadata": {
        "id": "H7-OE7H9F6vx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDEBU0cLbyiN"
      },
      "source": "#### Medium Task 4: Classifier Failure Analysis\n\nTrain the classifier and see what kinds of reviews it gets wrong. Then add your own test cases.\n"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UDR7v_0byiN"
      },
      "source": "from sentence_transformers import SentenceTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom datasets import load_dataset\nimport numpy as np",
      "outputs": [],
      "execution_count": 19
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgPRrIxLbyiO"
      },
      "source": "Load data and train a classifier."
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkW9_JNLbyiO",
        "outputId": "d55ce6e5-4b4a-473c-acf5-32189814b740",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": "data = load_dataset(\"rotten_tomatoes\")\nmodel = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n\ntrain_subset = data[\"train\"].shuffle(seed=42).select(range(1000))\ntest_subset = data[\"test\"].shuffle(seed=42).select(range(200))\n\nprint(\"Training classifier...\")\ntrain_embeddings = model.encode(train_subset[\"text\"], show_progress_bar=False)\ntest_embeddings = model.encode(test_subset[\"text\"], show_progress_bar=False)\n\nclf = LogisticRegression(random_state=42, max_iter=1000)\nclf.fit(train_embeddings, train_subset[\"label\"])\n\npredictions = clf.predict(test_embeddings)\nprobabilities = clf.predict_proba(test_embeddings)",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training classifier...\n"
          ]
        }
      ],
      "execution_count": 20
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wm3p9MfHbyiO"
      },
      "source": "Analyze the errors."
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpSZkDVobyiP",
        "outputId": "13daf6ba-a1bf-4c0b-a55d-17c092799c25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": "errors = []\nfor i in range(len(test_subset)):\n    if predictions[i] != test_subset[\"label\"][i]:\n        confidence = probabilities[i][predictions[i]]\n        errors.append({\n            'index': i,\n            'text': test_subset[\"text\"][i],\n            'true_label': test_subset[\"label\"][i],\n            'predicted_label': predictions[i],\n            'confidence': confidence,\n            'length': len(test_subset[\"text\"][i].split())\n        })\n\ntotal_errors = len(errors)\ntotal_samples = len(test_subset)\naccuracy = (total_samples - total_errors) / total_samples\n\nprint(f\"Overall: {total_samples - total_errors}/{total_samples} correct ({accuracy:.1%})\")\nprint(f\"Errors: {total_errors}\")",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall: 174/200 correct (87.0%)\n",
            "Errors: 26\n"
          ]
        }
      ],
      "execution_count": 21
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOX4A0LtbyiP"
      },
      "source": "Look at high-confidence errors (the most surprising mistakes)."
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kY0cgAP-byiP",
        "outputId": "0e5c361d-05c0-4e1d-ecfc-2f5909d9523b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": "high_conf_errors = [e for e in errors if e['confidence'] > 0.7]\n\nprint(\"High-Confidence Errors:\")\nfor error in high_conf_errors[:5]:\n    true_sent = \"positive\" if error['true_label'] == 1 else \"negative\"\n    pred_sent = \"positive\" if error['predicted_label'] == 1 else \"negative\"\n    print(f\"\\n'{error['text']}'\")\n    print(f\"True: {true_sent} | Predicted: {pred_sent} (conf: {error['confidence']:.3f})\")",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HIGH-CONFIDENCE ERRORS:\n",
            "\n",
            "1. 'an uneasy mix of run-of-the-mill raunchy humor and seemingly sincere personal reflection .'\n",
            "   True: Negative | Predicted: Positive\n",
            "   Confidence: 0.701\n",
            "\n",
            "2. 'the stunt work is top-notch ; the dialogue and drama often food-spittingly funny .'\n",
            "   True: Negative | Predicted: Positive\n",
            "   Confidence: 0.867\n",
            "\n",
            "3. 'goldmember is funny enough to justify the embarrassment of bringing a barf bag to the moviehouse .'\n",
            "   True: Positive | Predicted: Negative\n",
            "   Confidence: 0.710\n",
            "\n",
            "4. 'steven soderbergh doesn't remake andrei tarkovsky's solaris so much as distill it .'\n",
            "   True: Positive | Predicted: Negative\n",
            "   Confidence: 0.730\n",
            "\n",
            "5. '\" what really happened ? \" is a question for philosophers , not filmmakers ; all the filmmakers need to do is engage an audience .'\n",
            "   True: Positive | Predicted: Negative\n",
            "   Confidence: 0.717\n"
          ]
        }
      ],
      "execution_count": 22
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQmT2SO8byiQ"
      },
      "source": "Test on edge cases like sarcasm and mixed sentiment."
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbH3zcFMbyiQ",
        "outputId": "d26bed3a-ac6a-413c-d7b5-18b6080e1992",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": "edge_cases = [\n    (\"Sarcastic\", \"Oh great, another masterpiece. not!\", 0),\n    (\"Mixed\", \"The acting was great but the plot was terrible\", 0),\n    (\"Ambiguous\", \"It was a movie\", 0),\n]\n\nedge_embeddings = model.encode([text for _, text, _ in edge_cases])\nedge_predictions = clf.predict(edge_embeddings)\n\nprint(\"\\nEdge Cases:\")\ncorrect = 0\nfor i, (category, text, true_label) in enumerate(edge_cases):\n    pred = edge_predictions[i]\n    match = \"correct\" if pred == true_label else \"wrong\"\n    if pred == true_label:\n        correct += 1\n    print(f\"{category}: '{text}' -> {pred} ({match})\")\n\nprint(f\"\\nEdge case accuracy: {correct}/{len(edge_cases)}\")",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "EDGE CASES:\n",
            "\n",
            "Sarcastic: 'Oh great, another masterpiece. NOT!'\n",
            "  Predicted: 1 | Actual: 0 | WRONG\n",
            "\n",
            "Mixed: 'The acting was great but the plot was terrible'\n",
            "  Predicted: 0 | Actual: 0 | CORRECT\n",
            "\n",
            "Backhanded: 'Not as bad as I expected'\n",
            "  Predicted: 0 | Actual: 1 | WRONG\n",
            "\n",
            "Double negative: 'Not unwatchable'\n",
            "  Predicted: 0 | Actual: 1 | WRONG\n",
            "\n",
            "Very short: 'Boring'\n",
            "  Predicted: 0 | Actual: 0 | CORRECT\n",
            "\n",
            "Ambiguous: 'It was a movie'\n",
            "  Predicted: 0 | Actual: 0 | CORRECT\n",
            "\n",
            "Edge case accuracy: 3/6\n"
          ]
        }
      ],
      "execution_count": 23
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgJCrepxbyiR"
      },
      "source": "### Questions\n\n1. What do high-confidence errors have in common?\n\n2. Do errors tend to be shorter or longer than correct predictions?\n\n3. Which edge cases failed most - sarcasm, mixed sentiment, or double negatives?\n"
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "id": "0yNuuGK5cFj7"
      },
      "execution_count": 23,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}