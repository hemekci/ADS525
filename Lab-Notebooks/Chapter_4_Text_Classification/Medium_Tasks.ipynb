{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Text Classification - Medium Tasks\n",
    "\n",
    "This notebook focuses on building practical text classifiers. You'll create custom multi-class sentiment classifiers, evaluate performance with limited training data, implement confidence-based classification with uncertainty handling, and perform systematic failure analysis. These skills are crucial for real-world NLP applications where data and perfect accuracy are often limited.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "Run all cells in this section to set up the environment and load necessary data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LGW2SD-c864"
   },
   "source": [
    "### [OPTIONAL] - Installing Packages on <img src=\"https://colab.google/static/images/icons/colab.png\" width=100>\n",
    "\n",
    "\n",
    "If you are viewing this notebook on Google Colab (or any other cloud vendor), you need to **uncomment and run** the following codeblock to install the dependencies for this chapter:\n",
    "\n",
    "---\n",
    "\n",
    " **NOTE**: We will want to use a GPU to run the examples in this notebook. In Google Colab, go to\n",
    "**Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4**.\n",
    "\n",
    "---\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "N-PxmOIhc865"
   },
   "outputs": [],
   "source": [
    " %%capture\n",
    "!pip install transformers sentence-transformers openai\n",
    "!pip install -U datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 784,
     "referenced_widgets": [
      "169816892c8646e3888f213295349f00",
      "4a590f6ceb104873b97b9620d6107017",
      "2d14e5528bbd446e8a34c235201f88ac",
      "f4c9e362c7ff40559a1a5632a8b6e907",
      "3c4c633f0af84d099ffc74bac8901b07",
      "35226b53943b4db8a5d46aae09720818",
      "1a6faf9a5ed748f0809834cc52435b3d",
      "4fe179f7fab44513aeaa33cedf44f1bb",
      "5befa9362a09459695f4489c7173c34d",
      "458ad67bb385497889eab5bbdd977de5",
      "027a7c3f8a9e4114ac6d4e64fb62d440",
      "0cce924a44f94cbfaa69a78735ece1d5",
      "28ac2aeffcb6424f97fb47bfaee8e66f",
      "ab02cefe39774db3844d914b65a48790",
      "6bf6e7d710eb4924a3144e6e5d0679ca",
      "b7b774839b134ee687d9d89a0e15d166",
      "c583fda17dd44217875e86209fbad80a",
      "cd4eeedee4fc4f28acf650785bbaa5f3",
      "e5138fd5028f48f9ad8eea2b41b1592a",
      "82f4409e3f854415a871e3da31e9393e",
      "aeb337caa109443a9fc4f93e9f92753d",
      "e7e6399dec234052b7f30e3135a21b00",
      "44a764c1267042259b2c9597a69f6f76",
      "05e8f57e23ff47f0be24d3f9c18554d4",
      "616321e6e87a4d8589d9ed939e19c7b2",
      "8d7284d945004b98b28b6c631a2c4726",
      "97d864e9c07149aeb22a844ced3909d5",
      "dbadb6dfe23141ec8f38ffb747ea882e",
      "82d0c0a18606421f92ed77d8d4c61e10",
      "0c64a3bc4d4d461cbdc3c3c0acae2f94",
      "cf707e36593b4071b8253fd33d18b665",
      "079412e5345a4a09875ed0c86dde93e3",
      "1e2ffd31336946cf952abb5f62651c16",
      "8466edc3103c45d8a5d3fe8166508648",
      "f8d0d8f24551457e862d2e041f3699ab",
      "426852cccf704c58bdc7decbb1c583e6",
      "6b2663befc574a8e8154631c3ac95ff3",
      "091bd22cb6d94ae5b7c962d18e528ca2",
      "5ba5883cc2144929b9d7ddd980e4508d",
      "971d17ca015446b4bbf8a908835295cd",
      "c7086b545eae4b09af3b2d77b78af4b9",
      "5f47aeddde3449a5b5ed280d0ec9bc18",
      "43791444ad3e49f993d17887e6c15dbf",
      "c2c3403eb38b420cba7390d385f325be",
      "9097dd47971f445ea0fd55e7c42c15a0",
      "18cbf6cbaa4c436d9074224f83472863",
      "831180b810da4b549a8ca75536607e31",
      "d9779ccf530e49b1ac52172252ec5ef4",
      "78d69219fad44722a9bad3a12a3ccb61",
      "f08fa19e07644f4d9fff30d02040e315",
      "e1a2a1e765f042a7b9b081688688dab5",
      "76d299a97ab54dbe96f4525095afedcb",
      "73ac94bdcaf8427bbbcdd808f1428dff",
      "6f9a5d0dfe2f4aa49c57e942b927521b",
      "2d7810d299f649a2ac84541e7215d29a",
      "40dc8a185cf34e99a427f7c81bc76540",
      "bcac3e31740e4a2894b4e8bc9656a0c5",
      "87f29bce5b5d4df08b5337d62ffa9568",
      "3129b232f1e3411ead0d913654a11eaa",
      "4e7c3e87552640018f89c6cf9e070642",
      "01ccc633ad5d4350ad04651e360cc478",
      "735737e7c1eb477fae9d06d7324e81e8",
      "882b69fb84034d2dace625ef267f51cb",
      "8c77be8e5b74452bb028b97fd9edba36",
      "55a6296e28394f41b2a6d1f3d76e5944",
      "8f6c7a732bb6498db1de4d66f4b3c623",
      "22541d211626493e87c168143671e5ce",
      "111b9c6182994e2e85a323d07980b1ab",
      "3c53665ac233434fa899399da13650f0",
      "c4867626e6eb41bfaade350790de9f40",
      "a11866f208c4411dbc3d627ef6dbd74f",
      "3485c0ebc7f24965a0fadd3a6570d51e",
      "d111b1f753554eeda57cdbe420335fda",
      "8fb77e0530e946d38c007c9012c37a79",
      "8d669f0719d04980a6f04ffc2f65cf7c",
      "d79a649275a14fdeb2cb01e5ab75021f",
      "14186c7dd75e43d99183d5c8b308c0de"
     ]
    },
    "id": "5phRS_z2U_3T",
    "outputId": "e51c9c23-a48a-4c61-e7ee-cd0ef03915a4"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "169816892c8646e3888f213295349f00"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "train.parquet:   0%|          | 0.00/699k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0cce924a44f94cbfaa69a78735ece1d5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "validation.parquet:   0%|          | 0.00/90.0k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "44a764c1267042259b2c9597a69f6f76"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "test.parquet:   0%|          | 0.00/92.2k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8466edc3103c45d8a5d3fe8166508648"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9097dd47971f445ea0fd55e7c42c15a0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating validation split:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "40dc8a185cf34e99a427f7c81bc76540"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating test split:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "22541d211626493e87c168143671e5ce"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 8530\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load our data\n",
    "data = load_dataset(\"rotten_tomatoes\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "X0KyKHtqyjn3"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_performance(y_true, y_pred):\n",
    "    \"\"\"Create and print the classification report\"\"\"\n",
    "    performance = classification_report(\n",
    "        y_true, y_pred,\n",
    "        target_names=[\"Negative Review\", \"Positive Review\"]\n",
    "    )\n",
    "    print(performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Your Turn - Text Classification Experiments\n",
    "\n",
    "Run each task first to see the baseline results. Follow the instructions to modify and experiment."
   ],
   "metadata": {
    "id": "NKYNfoaVC4hU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This section is divided into EASY, MEDIUM, & HARD."
   ],
   "metadata": {
    "id": "hHVONn85DElL"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Medium Tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Medium Tasks - Building Real Classifiers\n",
    "\n",
    "These tasks require more modification and experimentation. You'll build complete classification systems."
   ],
   "metadata": {
    "id": "2ipz0pJYEne6"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Run the code to see how 5-level classification works. Then try adding a 6th category.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#### Medium Task 1: Multi-Class Sentiment Classification\n\nIn this task, you'll build a sentiment classifier with 5 different categories (from extremely negative to extremely positive) instead of just binary positive/negative.\n\n**What to do:**\n1. Run the cells below to see baseline 5-level classification\n2. Observe which reviews are uncertain (low margin between top predictions)\n3. Try uncommenting the 6-level version to add more granularity\n4. Compare how predictions change with more categories"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Set up the 5 sentiment categories and compute embeddings:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import pipeline\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score, classification_report\nfrom datasets import load_dataset\nimport numpy as np\n\ndata = load_dataset(\"rotten_tomatoes\")\n\ntrain_size = 1000  # TODO: Try different values: 100, 500, 1000, 2000, 5000\ntest_size = 300\n\ntrain_subset = data[\"train\"].shuffle(seed=42).select(range(min(train_size, len(data[\"train\"]))))\ntest_subset = data[\"test\"].shuffle(seed=42).select(range(test_size))\n\nprint(\"=\"*80)\nprint(f\"EXPERIMENT: Training Size = {train_size}\")\nprint(\"=\"*80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Classify each review and show confidence scores:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#### Medium Task 2: Classifier Performance with Limited Training Data\n\nHow much training data do you really need? This task compares two approaches: a task-specific model pre-trained on sentiment versus an embedding classifier trained from scratch on your data. Start by running with 1000 samples, then experiment with different training sizes like 100, 500, or 2000. Watch for the crossover point where the embedding classifier catches up to the pre-trained model and record your results in the table at the end."
  },
  {
   "cell_type": "markdown",
   "source": "As you can see, the classifier assigns each review to one of the 5 sentiment categories. The **margin** (difference between top 2 predictions) indicates confidence - large margins (>0.15) mean the model is confident, while small margins (<0.05) indicate uncertainty. Reviews with extreme language (\"best ever\", \"terrible\") have higher confidence, while moderate reviews (\"pretty good\", \"quite bad\") show more uncertainty.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Analyze which categories are most similar to each other:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*80)\nprint(\"CATEGORY CONFUSION ANALYSIS\")\nprint(\"=\"*80)\n\nlabel_similarity = cosine_similarity(label_embeddings)\n\nprint(f\"{'Category Pair':<60s} {'Similarity':<12s}\")\nprint(\"-\"*75)\n\nconfusions = []\nfor i in range(len(sentiment_labels)):\n    for j in range(i+1, len(sentiment_labels)):\n        sim = label_similarity[i][j]\n        confusions.append((i, j, sim))\n\nfor i, j, sim in sorted(confusions, key=lambda x: x[2], reverse=True)[:10]:\n    pair_name = f\"{sentiment_labels[i]} <-> {sentiment_labels[j]}\"\n    marker = \" \" if sim > 0.7 else \"\"\n    print(f\"{marker}{pair_name:<60s} {sim:.3f}\")"
  },
  {
   "cell_type": "markdown",
   "source": "Notice that adjacent categories (like \"somewhat negative\" and \"neutral\") tend to have higher similarity scores, which explains why the model sometimes confuses them. Categories with similarity > 0.7 are particularly prone to confusion.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from transformers import pipeline\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score, classification_report\nfrom datasets import load_dataset\nimport numpy as np\n\ndata = load_dataset(\"rotten_tomatoes\")\n\ntrain_size = 1000  # TODO: Try different values: 100, 500, 1000, 2000, 5000\ntest_size = 300\n\ntrain_subset = data[\"train\"].shuffle(seed=42).select(range(min(train_size, len(data[\"train\"]))))\ntest_subset = data[\"test\"].shuffle(seed=42).select(range(test_size))\n\nprint(\"=\"*80)\nprint(f\"EXPERIMENT: Training Size = {train_size}\")\nprint(\"=\"*80)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Medium Task 2: Classifier Performance with Limited Training Data\n\nTry different training sizes (100, 500, 1000, 2000) and fill in the results table.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import pipeline\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score, classification_report\nfrom datasets import load_dataset\nimport numpy as np\n\ndata = load_dataset(\"rotten_tomatoes\")\n\n# TODO: Try different values: 100, 500, 1000, 2000, 5000\ntrain_size = 1000\ntest_size = 300\n\ntrain_subset = data[\"train\"].shuffle(seed=42).select(range(min(train_size, len(data[\"train\"]))))\ntest_subset = data[\"test\"].shuffle(seed=42).select(range(test_size))\n\nprint(\"=\"*80)\nprint(f\"EXPERIMENT: Training Size = {train_size}\")\nprint(\"=\"*80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Medium Task 3: Confidence-Based Classifier with Uncertainty Handling\n\nIn production, refusing a prediction beats making a wrong one. Here's the key insight: when your model is uncertain, it should say \"I don't know\" rather than guessing. This creates a trade-off between coverage (how many predictions you make) and accuracy (how often you're right).\n\nTry this:\n- Run with threshold of 0.15 first\n- Test 0.05, 0.30, and 0.50 to see how the trade-off shifts\n- Check the uncertain cases (typically have hedging language)\n- Experiment with the alternative uncertainty measure"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n[1/2] Testing Task-Specific Model...\")\n\ntask_model = pipeline(\n    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n    tokenizer=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n    return_all_scores=True,\n    device=-1\n)\n\ny_pred_task = []\nfor text in test_subset[\"text\"]:\n    output = task_model(text)[0]\n    neg_score = output[0][\"score\"]\n    pos_score = output[2][\"score\"]\n    y_pred_task.append(1 if pos_score > neg_score else 0)\n\ntask_f1 = f1_score(test_subset[\"label\"], y_pred_task, average='weighted')\nprint(f\" Task-Specific Model F1: {task_f1:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Train the embedding-based classifier on your labeled data:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(f\"\\n[2/2] Training Embedding Classifier on {train_size} samples...\")\n\nembedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n\ntrain_embeddings = embedding_model.encode(train_subset[\"text\"], show_progress_bar=False)\ntest_embeddings = embedding_model.encode(test_subset[\"text\"], show_progress_bar=False)\n\nclf = LogisticRegression(random_state=42, max_iter=1000)\nclf.fit(train_embeddings, train_subset[\"label\"])\n\ny_pred_embed = clf.predict(test_embeddings)\nembed_f1 = f1_score(test_subset[\"label\"], y_pred_embed, average='weighted')\n\nprint(f\" Embedding Classifier F1: {embed_f1:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Compare the two approaches and show example predictions:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*80)\nprint(\"RESULTS SUMMARY\")\nprint(\"=\"*80)\nprint(f\"Training samples used: {train_size}\")\nprint(f\"\\nTask-Specific (pre-trained):  F1 = {task_f1:.4f}\")\nprint(f\"Embedding + Classifier:       F1 = {embed_f1:.4f}\")\nprint(f\"Difference:                       {embed_f1 - task_f1:+.4f}\")\n\nif embed_f1 > task_f1:\n    print(f\"\\n Embedding approach WINS with {train_size} samples!\")\nelif embed_f1 > task_f1 - 0.01:\n    print(f\"\\n Essentially TIED\")\nelse:\n    print(f\"\\n Task-specific model wins\")\n\n# Show example predictions\nprint(\"\\n\" + \"=\"*80)\nprint(\"EXAMPLE PREDICTIONS (first 5)\")\nprint(\"=\"*80)\n\nfor i in range(5):\n    true_label = \"Positive\" if test_subset[\"label\"][i] == 1 else \"Negative\"\n    task_pred = \"Positive\" if y_pred_task[i] == 1 else \"Negative\"\n    embed_pred = \"Positive\" if y_pred_embed[i] == 1 else \"Negative\"\n\n    task_correct = \"\" if y_pred_task[i] == test_subset[\"label\"][i] else \"\"\n    embed_correct = \"\" if y_pred_embed[i] == test_subset[\"label\"][i] else \"\"\n\n    print(f\"\\n{i+1}. '{test_subset['text'][i][:60]}...'\")\n    print(f\"   True: {true_label}\")\n    print(f\"   Task-Specific: {task_pred} {task_correct}\")\n    print(f\"   Embedding:     {embed_pred} {embed_correct}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"TODO: Record your results\")\nprint(\"=\"*80)\nprint(f\"Current: | {train_size:<10} | {task_f1:.4f}  | {embed_f1:.4f}       | {'Embed' if embed_f1 > task_f1 else 'Task':<11} |\")"
  },
  {
   "cell_type": "code",
   "source": "from sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\nmodel = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n\n# Reviews with varying clarity levels\ntest_reviews = [\n    \"Absolutely fantastic! Best movie ever!\",\n    \"Pretty good, I liked it\",\n    \"It was fine, nothing special\",\n    \"Not bad but not great either\",\n    \"Quite disappointing\",\n    \"Terrible! Complete waste of time!\",\n    \"The movie had some interesting moments\",\n    \"Outstanding performances all around!\",\n]\n\ny_true = [1, 1, 0, 0, 0, 0, 1, 1]  # True labels: 1=positive, 0=negative\n\nlabels = [\"A negative movie review\", \"A positive movie review\"]\n\nconfidence_threshold = 0.15  # TODO: Try values: 0.05, 0.15, 0.30, 0.50\n\nlabel_embeddings = model.encode(labels)\nreview_embeddings = model.encode(test_reviews)\nsim_matrix = cosine_similarity(review_embeddings, label_embeddings)\n\ndef calculate_margin(similarities):\n    \"\"\"Margin = difference between top two predictions\"\"\"\n    sorted_sims = np.sort(similarities)[::-1]\n    margin = sorted_sims[0] - sorted_sims[1]\n    return margin\n\n# TODO: Try this alternative uncertainty measure\n# def calculate_margin(similarities):\n#     \"\"\"Alternative: Use absolute confidence in top prediction\"\"\"\n#     max_confidence = np.max(similarities)\n#     margin = max_confidence - 0.5\n#     return margin",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Calculate metrics\nmade_predictions = [r for r in results if r['pred'] is not None]\nuncertain_cases = [r for r in results if r['pred'] is None]\ncorrect_predictions = [r for r in made_predictions if r['pred'] == r['true']]\n\ntotal = len(results)\nn_predicted = len(made_predictions)\nn_uncertain = len(uncertain_cases)\nn_correct = len(correct_predictions)\n\ncoverage = n_predicted / total\naccuracy = n_correct / n_predicted if n_predicted > 0 else 0\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"PERFORMANCE ANALYSIS\")\nprint(\"=\"*80)\n\nprint(f\"\\nCoverage: {n_predicted}/{total} = {coverage:.1%}\")\nprint(f\"   Made predictions for {n_predicted} reviews\")\nprint(f\"   Refused to predict on {n_uncertain} reviews\")\n\nprint(f\"\\nAccuracy (on predictions made): {n_correct}/{n_predicted} = {accuracy:.1%}\")\nprint(f\"   Of the {n_predicted} predictions, {n_correct} were correct\")\n\nprint(f\"\\nTrade-off Analysis:\")\nprint(f\"  Threshold = {confidence_threshold}\")\nprint(f\"   Higher threshold = fewer predictions but higher accuracy\")\nprint(f\"   Lower threshold = more predictions but lower accuracy\")\n\nif n_uncertain > 0:\n    print(f\"\\n\" + \"-\"*80)\n    print(f\"UNCERTAIN CASES (margin < {confidence_threshold}):\")\n    print(\"-\"*80)\n    for r in uncertain_cases:\n        print(f\"  \u2022 '{r['review']}'\")\n        print(f\"    Margin: {r['margin']:.3f} (too close to call)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#### Medium Task 4: Systematic Classifier Failure Analysis\n\nUnderstanding where and why your classifier fails is crucial for improvement. Run the analysis to see overall error patterns, examine high-confidence errors (the most surprising mistakes), and test edge cases like sarcasm and mixed sentiment. Add your own challenging test cases, then propose improvements based on what you find.",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#### Medium Task 4: Systematic Classifier Failure Analysis\n\nLooking at the overall error patterns reveals where your classifier struggles. Run the analysis to examine high-confidence errors (the most surprising mistakes) and test edge cases like sarcasm and mixed sentiment. Add your own challenging test cases, then propose improvements based on what you find.",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Questions\n\n1. At what training size did embedding classifier match the task-specific model?\n\n2. Were there cases where one model was correct and the other wrong?\n\n3. Is 100 samples enough labeled data?\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Medium Task 3: Confidence-Based Classifier\n\nRun with threshold=0.15, then try 0.05 and 0.30 to see the trade-offs.\n"
  },
  {
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# Reviews with varying levels of clarity\n",
    "test_reviews = [\n",
    "    \"Absolutely fantastic! Best movie ever!\",           # Clear positive\n",
    "    \"Pretty good, I liked it\",                          # Weak positive\n",
    "    \"It was fine, nothing special\",                     # Ambiguous\n",
    "    \"Not bad but not great either\",                     # Very ambiguous\n",
    "    \"Quite disappointing\",                              # Weak negative\n",
    "    \"Terrible! Complete waste of time!\",                # Clear negative\n",
    "    \"The movie had some interesting moments\",           # Ambiguous positive\n",
    "    \"Outstanding performances all around!\",             # Clear positive\n",
    "]\n",
    "\n",
    "# True labels (for evaluation)\n",
    "y_true = [1, 1, 0, 0, 0, 0, 1, 1]  # 1=positive, 0=negative\n",
    "\n",
    "labels = [\"A negative movie review\", \"A positive movie review\"]\n",
    "\n",
    "# TODO: EXPERIMENT WITH THIS - Try: 0.05, 0.15, 0.30, 0.50\n",
    "confidence_threshold = 0.15\n",
    "\n",
    "label_embeddings = model.encode(labels)\n",
    "review_embeddings = model.encode(test_reviews)\n",
    "sim_matrix = cosine_similarity(review_embeddings, label_embeddings)\n",
    "\n",
    "def calculate_margin(similarities):\n",
    "    \"\"\"\n",
    "    Margin = difference between top two predictions\n",
    "    Small margin = uncertain (predictions are close)\n",
    "    \"\"\"\n",
    "    sorted_sims = np.sort(similarities)[::-1]\n",
    "    margin = sorted_sims[0] - sorted_sims[1]\n",
    "    return margin\n",
    "\n",
    "# TODO: After first run, uncomment this alternative uncertainty measure:\n",
    "# def calculate_margin(similarities):\n",
    "#     \"\"\"\n",
    "#     Alternative: Use absolute confidence in top prediction\n",
    "#     Low confidence = uncertain\n",
    "#     \"\"\"\n",
    "#     max_confidence = np.max(similarities)\n",
    "#     # Convert to margin-like score (higher = more certain)\n",
    "#     # If max is 0.6, margin = 0.6 - 0.5 = 0.1 (uncertain)\n",
    "#     # If max is 0.9, margin = 0.9 - 0.5 = 0.4 (certain)\n",
    "#     margin = max_confidence - 0.5\n",
    "#     return margin\n",
    "\n",
    "# Classify with confidence threshold\n",
    "results = []\n",
    "predictions = []\n",
    "confidences = []\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"CONFIDENCE-BASED CLASSIFICATION (threshold={confidence_threshold})\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, review in enumerate(test_reviews):\n",
    "    similarities = sim_matrix[i]\n",
    "    predicted_idx = np.argmax(similarities)\n",
    "    top_confidence = similarities[predicted_idx]\n",
    "    margin = calculate_margin(similarities)\n",
    "\n",
    "    # Decision: predict only if confident enough\n",
    "    if margin >= confidence_threshold:\n",
    "        prediction = predicted_idx\n",
    "        status = \"PREDICTED\"\n",
    "        predictions.append(prediction)\n",
    "    else:\n",
    "        prediction = None\n",
    "        status = \"UNCERTAIN\"\n",
    "        predictions.append(None)\n",
    "\n",
    "    true_label = \"Positive\" if y_true[i] == 1 else \"Negative\"\n",
    "    pred_label = labels[predicted_idx] if prediction is not None else \"UNCERTAIN\"\n",
    "\n",
    "    print(f\"\\n{i+1}. '{review}'\")\n",
    "    print(f\"   True label: {true_label}\")\n",
    "    print(f\"   Prediction: {pred_label}\")\n",
    "    print(f\"   Top confidence: {top_confidence:.3f}\")\n",
    "    print(f\"   Margin: {margin:.3f} {' Above threshold' if margin >= confidence_threshold else ' Below threshold'}\")\n",
    "    print(f\"   Status: {status}\", end=\"\")\n",
    "\n",
    "    if prediction is not None:\n",
    "        correct = prediction == y_true[i]\n",
    "        print(f\" - {' CORRECT' if correct else ' INCORRECT'}\")\n",
    "    else:\n",
    "        print()\n",
    "\n",
    "    results.append({\n",
    "        'review': review,\n",
    "        'true': y_true[i],\n",
    "        'pred': prediction,\n",
    "        'margin': margin,\n",
    "        'status': status\n",
    "    })\n",
    "\n",
    "# Calculate metrics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "made_predictions = [r for r in results if r['pred'] is not None]\n",
    "uncertain_cases = [r for r in results if r['pred'] is None]\n",
    "correct_predictions = [r for r in made_predictions if r['pred'] == r['true']]\n",
    "\n",
    "total = len(results)\n",
    "n_predicted = len(made_predictions)\n",
    "n_uncertain = len(uncertain_cases)\n",
    "n_correct = len(correct_predictions)\n",
    "\n",
    "coverage = n_predicted / total\n",
    "accuracy = n_correct / n_predicted if n_predicted > 0 else 0\n",
    "\n",
    "print(f\"\\nCoverage: {n_predicted}/{total} = {coverage:.1%}\")\n",
    "print(f\"   Made predictions for {n_predicted} reviews\")\n",
    "print(f\"   Refused to predict on {n_uncertain} reviews\")\n",
    "\n",
    "print(f\"\\nAccuracy (on predictions made): {n_correct}/{n_predicted} = {accuracy:.1%}\")\n",
    "print(f\"   Of the {n_predicted} predictions, {n_correct} were correct\")\n",
    "\n",
    "print(f\"\\nTrade-off Analysis:\")\n",
    "print(f\"  Threshold = {confidence_threshold}\")\n",
    "print(f\"   Higher threshold = fewer predictions but higher accuracy\")\n",
    "print(f\"   Lower threshold = more predictions but lower accuracy\")\n",
    "\n",
    "# Show which reviews were uncertain\n",
    "if n_uncertain > 0:\n",
    "    print(f\"\\n\" + \"-\"*80)\n",
    "    print(f\"UNCERTAIN CASES (margin < {confidence_threshold}):\")\n",
    "    print(\"-\"*80)\n",
    "    for r in uncertain_cases:\n",
    "        print(f\"  \u2022 '{r['review']}'\")\n",
    "        print(f\"    Margin: {r['margin']:.3f} (too close to call)\")\n",
    "\n",
    "# TODO: After experimenting with thresholds, analyze the trade-off\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPERIMENT LOG - Fill this in as you try different thresholds:\")\n",
    "print(\"=\"*80)\n",
    "print(\"| Threshold | Coverage | Accuracy | Notes                    |\")\n",
    "print(\"|-----------|----------|----------|--------------------------|\")\n",
    "print(\"| 0.05      | ??.?%    | ??.?%    | ?                        |\")\n",
    "print(\"| 0.15      | ??.?%    | ??.?%    | ?                        |\")\n",
    "print(\"| 0.30      | ??.?%    | ??.?%    | ?                        |\")\n",
    "print(\"| 0.50      | ??.?%    | ??.?%    | ?                        |\")\n",
    "print()\n",
    "print(f\"Current:    | {confidence_threshold:<9.2f} | {coverage*100:>5.1f}%    | {accuracy*100:>5.1f}%    |\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lpzBi9YBFZRi",
    "outputId": "28f21c01-c8d7-4c04-8256-6f0dd47f890c"
   },
   "execution_count": 32,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "================================================================================\n",
      "CONFIDENCE-BASED CLASSIFICATION (threshold=0.15)\n",
      "================================================================================\n",
      "\n",
      "1. 'Absolutely fantastic! Best movie ever!'\n",
      "   True label: Positive\n",
      "   Prediction: UNCERTAIN\n",
      "   Top confidence: 0.451\n",
      "   Margin: 0.092 \u2717 Below threshold\n",
      "   Status: UNCERTAIN\n",
      "\n",
      "2. 'Pretty good, I liked it'\n",
      "   True label: Positive\n",
      "   Prediction: UNCERTAIN\n",
      "   Top confidence: 0.410\n",
      "   Margin: 0.018 \u2717 Below threshold\n",
      "   Status: UNCERTAIN\n",
      "\n",
      "3. 'It was fine, nothing special'\n",
      "   True label: Negative\n",
      "   Prediction: UNCERTAIN\n",
      "   Top confidence: 0.418\n",
      "   Margin: 0.051 \u2717 Below threshold\n",
      "   Status: UNCERTAIN\n",
      "\n",
      "4. 'Not bad but not great either'\n",
      "   True label: Negative\n",
      "   Prediction: UNCERTAIN\n",
      "   Top confidence: 0.414\n",
      "   Margin: 0.053 \u2717 Below threshold\n",
      "   Status: UNCERTAIN\n",
      "\n",
      "5. 'Quite disappointing'\n",
      "   True label: Negative\n",
      "   Prediction: UNCERTAIN\n",
      "   Top confidence: 0.354\n",
      "   Margin: 0.080 \u2717 Below threshold\n",
      "   Status: UNCERTAIN\n",
      "\n",
      "6. 'Terrible! Complete waste of time!'\n",
      "   True label: Negative\n",
      "   Prediction: UNCERTAIN\n",
      "   Top confidence: 0.397\n",
      "   Margin: 0.121 \u2717 Below threshold\n",
      "   Status: UNCERTAIN\n",
      "\n",
      "7. 'The movie had some interesting moments'\n",
      "   True label: Positive\n",
      "   Prediction: UNCERTAIN\n",
      "   Top confidence: 0.506\n",
      "   Margin: 0.118 \u2717 Below threshold\n",
      "   Status: UNCERTAIN\n",
      "\n",
      "8. 'Outstanding performances all around!'\n",
      "   True label: Positive\n",
      "   Prediction: UNCERTAIN\n",
      "   Top confidence: 0.282\n",
      "   Margin: 0.098 \u2717 Below threshold\n",
      "   Status: UNCERTAIN\n",
      "\n",
      "================================================================================\n",
      "PERFORMANCE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Coverage: 0/8 = 0.0%\n",
      "  \u2192 Made predictions for 0 reviews\n",
      "  \u2192 Refused to predict on 8 reviews\n",
      "\n",
      "Accuracy (on predictions made): 0/0 = 0.0%\n",
      "  \u2192 Of the 0 predictions, 0 were correct\n",
      "\n",
      "Trade-off Analysis:\n",
      "  Threshold = 0.15\n",
      "  \u2192 Higher threshold = fewer predictions but higher accuracy\n",
      "  \u2192 Lower threshold = more predictions but lower accuracy\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "UNCERTAIN CASES (margin < 0.15):\n",
      "--------------------------------------------------------------------------------\n",
      "  \u2022 'Absolutely fantastic! Best movie ever!'\n",
      "    Margin: 0.092 (too close to call)\n",
      "  \u2022 'Pretty good, I liked it'\n",
      "    Margin: 0.018 (too close to call)\n",
      "  \u2022 'It was fine, nothing special'\n",
      "    Margin: 0.051 (too close to call)\n",
      "  \u2022 'Not bad but not great either'\n",
      "    Margin: 0.053 (too close to call)\n",
      "  \u2022 'Quite disappointing'\n",
      "    Margin: 0.080 (too close to call)\n",
      "  \u2022 'Terrible! Complete waste of time!'\n",
      "    Margin: 0.121 (too close to call)\n",
      "  \u2022 'The movie had some interesting moments'\n",
      "    Margin: 0.118 (too close to call)\n",
      "  \u2022 'Outstanding performances all around!'\n",
      "    Margin: 0.098 (too close to call)\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT LOG - Fill this in as you try different thresholds:\n",
      "================================================================================\n",
      "| Threshold | Coverage | Accuracy | Notes                    |\n",
      "|-----------|----------|----------|--------------------------|\n",
      "| 0.05      | ??.?%    | ??.?%    | ?                        |\n",
      "| 0.15      | ??.?%    | ??.?%    | ?                        |\n",
      "| 0.30      | ??.?%    | ??.?%    | ?                        |\n",
      "| 0.50      | ??.?%    | ??.?%    | ?                        |\n",
      "\n",
      "Current:    | 0.15      |   0.0%    |   0.0%    |\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Questions\n",
    "\n",
    "1. What do uncertain reviews have in common? Are they using hedging language like \"kind of\" or \"somewhat\"?\n",
    "\n",
    "2. Compare results at threshold=0.05 vs 0.30. Describe the coverage vs accuracy trade-off. When would you want high coverage vs high accuracy?\n",
    "\n",
    "3. How could you use confidence-based prediction in production? What should a system do when the model is uncertain?"
   ],
   "metadata": {
    "id": "H7-OE7H9F6vx"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Medium Task 4: Classifier Failure Analysis\n\nTrain the classifier and see what kinds of reviews it gets wrong. Then add your own test cases.\n"
  },
  {
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "data = load_dataset(\"rotten_tomatoes\")\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# Use subset for faster experimentation\n",
    "train_subset = data[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "test_subset = data[\"test\"].shuffle(seed=42).select(range(200))\n",
    "\n",
    "# Train classifier\n",
    "print(\"Training classifier on 1000 movie reviews...\")\n",
    "train_embeddings = model.encode(train_subset[\"text\"], show_progress_bar=False)\n",
    "test_embeddings = model.encode(test_subset[\"text\"], show_progress_bar=False)\n",
    "\n",
    "clf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "clf.fit(train_embeddings, train_subset[\"label\"])\n",
    "\n",
    "# Get predictions\n",
    "predictions = clf.predict(test_embeddings)\n",
    "probabilities = clf.predict_proba(test_embeddings)\n",
    "\n",
    "# Analyze errors\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ERROR ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "errors = []\n",
    "for i in range(len(test_subset)):\n",
    "    if predictions[i] != test_subset[\"label\"][i]:\n",
    "        confidence = probabilities[i][predictions[i]]\n",
    "        errors.append({\n",
    "            'index': i,\n",
    "            'text': test_subset[\"text\"][i],\n",
    "            'true_label': test_subset[\"label\"][i],\n",
    "            'predicted_label': predictions[i],\n",
    "            'confidence': confidence,\n",
    "            'length': len(test_subset[\"text\"][i].split())\n",
    "        })\n",
    "\n",
    "total_errors = len(errors)\n",
    "total_samples = len(test_subset)\n",
    "accuracy = (total_samples - total_errors) / total_samples\n",
    "\n",
    "print(f\"\\nOverall Performance:\")\n",
    "print(f\"  Correct: {total_samples - total_errors}/{total_samples} ({accuracy:.1%})\")\n",
    "print(f\"  Errors:  {total_errors}/{total_samples} ({total_errors/total_samples:.1%})\")\n",
    "\n",
    "# Categorize errors\n",
    "false_positives = [e for e in errors if e['predicted_label'] == 1]\n",
    "false_negatives = [e for e in errors if e['predicted_label'] == 0]\n",
    "\n",
    "print(f\"\\nError Types:\")\n",
    "print(f\"  False Positives: {len(false_positives)} (predicted positive, actually negative)\")\n",
    "print(f\"  False Negatives: {len(false_negatives)} (predicted negative, actually positive)\")\n",
    "\n",
    "# Show high-confidence errors (most surprising)\n",
    "high_conf_errors = [e for e in errors if e['confidence'] > 0.7]\n",
    "\n",
    "print(f\"\\n\" + \"-\"*80)\n",
    "print(f\"HIGH-CONFIDENCE ERRORS (confidence > 0.7)\")\n",
    "print(f\"These are the most surprising mistakes:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for i, error in enumerate(high_conf_errors[:5]):\n",
    "    true_sent = \"Positive\" if error['true_label'] == 1 else \"Negative\"\n",
    "    pred_sent = \"Positive\" if error['predicted_label'] == 1 else \"Negative\"\n",
    "\n",
    "    print(f\"\\n{i+1}. '{error['text']}'\")\n",
    "    print(f\"   True: {true_sent} | Predicted: {pred_sent} | Confidence: {error['confidence']:.3f}\")\n",
    "    print(f\"   Length: {error['length']} words\")\n",
    "\n",
    "# Analyze by text length\n",
    "print(f\"\\n\" + \"-\"*80)\n",
    "print(\"ERROR ANALYSIS BY TEXT LENGTH\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "error_lengths = [e['length'] for e in errors]\n",
    "correct_lengths = [len(test_subset[\"text\"][i].split())\n",
    "                   for i in range(len(test_subset))\n",
    "                   if predictions[i] == test_subset[\"label\"][i]]\n",
    "\n",
    "avg_error_length = np.mean(error_lengths) if error_lengths else 0\n",
    "avg_correct_length = np.mean(correct_lengths) if correct_lengths else 0\n",
    "\n",
    "print(f\"\\nAverage length of ERROR reviews: {avg_error_length:.1f} words\")\n",
    "print(f\"Average length of CORRECT reviews: {avg_correct_length:.1f} words\")\n",
    "\n",
    "if avg_error_length < avg_correct_length:\n",
    "    print(f\" Observation: Errors tend to be SHORTER\")\n",
    "elif avg_error_length > avg_correct_length:\n",
    "    print(f\" Observation: Errors tend to be LONGER\")\n",
    "else:\n",
    "    print(f\" Observation: No clear length pattern\")\n",
    "\n",
    "# Test edge cases\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING EDGE CASES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "edge_cases = [\n",
    "    (\"Sarcastic\", \"Oh great, another masterpiece. NOT!\", 0),\n",
    "    (\"Mixed\", \"The acting was great but the plot was terrible\", 0),\n",
    "    (\"Backhanded\", \"Not as bad as I expected\", 1),\n",
    "    (\"Double negative\", \"Not unwatchable\", 1),\n",
    "    (\"Very short\", \"Boring\", 0),\n",
    "    (\"Ambiguous\", \"It was a movie\", 0),\n",
    "]\n",
    "\n",
    "# TODO: After analyzing above errors, add your own test cases:\n",
    "# edge_cases.extend([\n",
    "#     (\"Your category\", \"Your test review here\", expected_label_0_or_1),\n",
    "#     (\"Another category\", \"Another test review\", expected_label),\n",
    "# ])\n",
    "\n",
    "print(\"\\nTesting challenging cases that often fail:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "edge_embeddings = model.encode([text for _, text, _ in edge_cases])\n",
    "edge_predictions = clf.predict(edge_embeddings)\n",
    "edge_probs = clf.predict_proba(edge_embeddings)\n",
    "\n",
    "correct_count = 0\n",
    "for i, (category, text, true_label) in enumerate(edge_cases):\n",
    "    pred = edge_predictions[i]\n",
    "    conf = edge_probs[i][pred]\n",
    "    correct = pred == true_label\n",
    "    if correct:\n",
    "        correct_count += 1\n",
    "\n",
    "    true_sent = \"Positive\" if true_label == 1 else \"Negative\"\n",
    "    pred_sent = \"Positive\" if pred == 1 else \"Negative\"\n",
    "\n",
    "    print(f\"\\n{category}: '{text}'\")\n",
    "    print(f\"  True: {true_sent} | Predicted: {pred_sent} | Confidence: {conf:.3f}\")\n",
    "    print(f\"  Result: {' CORRECT' if correct else ' WRONG'}\")\n",
    "\n",
    "edge_accuracy = correct_count / len(edge_cases)\n",
    "print(f\"\\n\" + \"-\"*80)\n",
    "print(f\"Edge Case Accuracy: {correct_count}/{len(edge_cases)} ({edge_accuracy:.1%})\")\n",
    "print(f\"Regular Test Accuracy: {accuracy:.1%}\")\n",
    "print(f\"Difference: {accuracy - edge_accuracy:+.1%}\")\n",
    "\n",
    "# Summary and insights\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY INSIGHTS FROM ERROR ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. Error Distribution:\")\n",
    "print(f\"   - False Positives (predicted too optimistic): {len(false_positives)}\")\n",
    "print(f\"   - False Negatives (predicted too pessimistic): {len(false_negatives)}\")\n",
    "if len(false_positives) > len(false_negatives):\n",
    "    print(f\"    Classifier has POSITIVE BIAS\")\n",
    "elif len(false_negatives) > len(false_positives):\n",
    "    print(f\"    Classifier has NEGATIVE BIAS\")\n",
    "\n",
    "print(\"\\n2. Challenging Cases:\")\n",
    "failing_categories = [cat for cat, text, true in edge_cases\n",
    "                     if clf.predict(model.encode([text]))[0] != true]\n",
    "if failing_categories:\n",
    "    print(f\"   The classifier struggles with: {', '.join(failing_categories)}\")\n",
    "\n",
    "print(\"\\n3. Confidence Analysis:\")\n",
    "if high_conf_errors:\n",
    "    print(f\"   Found {len(high_conf_errors)} high-confidence errors\")\n",
    "    print(f\"    The model is 'confidently wrong' on some cases\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TODO: Based on your error analysis, propose improvements:\")\n",
    "print(\"=\"*80)\n",
    "print(\"# Write your observations here:\")\n",
    "print(\"# 1. What patterns did you notice in the errors?\")\n",
    "print(\"# 2. Which edge cases failed most?\")\n",
    "print(\"# 3. How would you improve the classifier?\")\n",
    "print(\"#    - Better training data?\")\n",
    "print(\"#    - Different features?\")\n",
    "print(\"#    - Ensemble approach?\")\n",
    "print(\"#    - Confidence thresholds?\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sfz2qOmPGHWp",
    "outputId": "341250f6-c025-4e7a-ded8-de541c0ebc44"
   },
   "execution_count": 33,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training classifier on 1000 movie reviews...\n",
      "\n",
      "================================================================================\n",
      "ERROR ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Overall Performance:\n",
      "  Correct: 174/200 (87.0%)\n",
      "  Errors:  26/200 (13.0%)\n",
      "\n",
      "Error Types:\n",
      "  False Positives: 11 (predicted positive, actually negative)\n",
      "  False Negatives: 15 (predicted negative, actually positive)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "HIGH-CONFIDENCE ERRORS (confidence > 0.7)\n",
      "These are the most surprising mistakes:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. 'an uneasy mix of run-of-the-mill raunchy humor and seemingly sincere personal reflection .'\n",
      "   True: Negative | Predicted: Positive | Confidence: 0.701\n",
      "   Length: 13 words\n",
      "\n",
      "2. 'the stunt work is top-notch ; the dialogue and drama often food-spittingly funny .'\n",
      "   True: Negative | Predicted: Positive | Confidence: 0.867\n",
      "   Length: 14 words\n",
      "\n",
      "3. 'goldmember is funny enough to justify the embarrassment of bringing a barf bag to the moviehouse .'\n",
      "   True: Positive | Predicted: Negative | Confidence: 0.710\n",
      "   Length: 17 words\n",
      "\n",
      "4. 'steven soderbergh doesn't remake andrei tarkovsky's solaris so much as distill it .'\n",
      "   True: Positive | Predicted: Negative | Confidence: 0.730\n",
      "   Length: 13 words\n",
      "\n",
      "5. '\" what really happened ? \" is a question for philosophers , not filmmakers ; all the filmmakers need to do is engage an audience .'\n",
      "   True: Positive | Predicted: Negative | Confidence: 0.717\n",
      "   Length: 26 words\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "ERROR ANALYSIS BY TEXT LENGTH\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Average length of ERROR reviews: 19.8 words\n",
      "Average length of CORRECT reviews: 21.4 words\n",
      "\u2192 Observation: Errors tend to be SHORTER\n",
      "\n",
      "================================================================================\n",
      "TESTING EDGE CASES\n",
      "================================================================================\n",
      "\n",
      "Testing challenging cases that often fail:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sarcastic: 'Oh great, another masterpiece. NOT!'\n",
      "  True: Negative | Predicted: Positive | Confidence: 0.648\n",
      "  Result: \u2717 WRONG\n",
      "\n",
      "Mixed: 'The acting was great but the plot was terrible'\n",
      "  True: Negative | Predicted: Negative | Confidence: 0.920\n",
      "  Result: \u2713 CORRECT\n",
      "\n",
      "Backhanded: 'Not as bad as I expected'\n",
      "  True: Positive | Predicted: Negative | Confidence: 0.809\n",
      "  Result: \u2717 WRONG\n",
      "\n",
      "Double negative: 'Not unwatchable'\n",
      "  True: Positive | Predicted: Negative | Confidence: 0.902\n",
      "  Result: \u2717 WRONG\n",
      "\n",
      "Very short: 'Boring'\n",
      "  True: Negative | Predicted: Negative | Confidence: 0.949\n",
      "  Result: \u2713 CORRECT\n",
      "\n",
      "Ambiguous: 'It was a movie'\n",
      "  True: Negative | Predicted: Negative | Confidence: 0.709\n",
      "  Result: \u2713 CORRECT\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Edge Case Accuracy: 3/6 (50.0%)\n",
      "Regular Test Accuracy: 87.0%\n",
      "Difference: +37.0%\n",
      "\n",
      "================================================================================\n",
      "KEY INSIGHTS FROM ERROR ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "1. Error Distribution:\n",
      "   - False Positives (predicted too optimistic): 11\n",
      "   - False Negatives (predicted too pessimistic): 15\n",
      "   \u2192 Classifier has NEGATIVE BIAS\n",
      "\n",
      "2. Challenging Cases:\n",
      "   The classifier struggles with: Sarcastic, Backhanded, Double negative\n",
      "\n",
      "3. Confidence Analysis:\n",
      "   Found 11 high-confidence errors\n",
      "   \u2192 The model is 'confidently wrong' on some cases\n",
      "\n",
      "================================================================================\n",
      "TODO: Based on your error analysis, propose improvements:\n",
      "================================================================================\n",
      "# Write your observations here:\n",
      "# 1. What patterns did you notice in the errors?\n",
      "# 2. Which edge cases failed most?\n",
      "# 3. How would you improve the classifier?\n",
      "#    - Better training data?\n",
      "#    - Different features?\n",
      "#    - Ensemble approach?\n",
      "#    - Confidence thresholds?\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Questions\n\n1. What do high-confidence errors have in common?\n\n2. Do errors tend to be shorter or longer than correct predictions?\n\n3. Which edge cases failed most - sarcasm, mixed sentiment, or double negatives?\n"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}