{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Text Classification - Medium Tasks\n",
    "\n",
    "This notebook focuses on building practical text classifiers. You'll create custom multi-class sentiment classifiers, evaluate performance with limited training data, implement confidence-based classification with uncertainty handling, and perform systematic failure analysis. These skills are crucial for real-world NLP applications where data and perfect accuracy are often limited.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "Run all cells in this section to set up the environment and load necessary data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LGW2SD-c864"
   },
   "source": [
    "### [OPTIONAL] - Installing Packages on <img src=\"https://colab.google/static/images/icons/colab.png\" width=100>\n",
    "\n",
    "\n",
    "If you are viewing this notebook on Google Colab (or any other cloud vendor), you need to **uncomment and run** the following codeblock to install the dependencies for this chapter:\n",
    "\n",
    "---\n",
    "\n",
    " **NOTE**: We will want to use a GPU to run the examples in this notebook. In Google Colab, go to\n",
    "**Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4**.\n",
    "\n",
    "---\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "N-PxmOIhc865"
   },
   "outputs": [],
   "source": [
    " %%capture\n",
    "!pip install transformers sentence-transformers openai\n",
    "!pip install -U datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 784,
     "referenced_widgets": [
      "169816892c8646e3888f213295349f00",
      "4a590f6ceb104873b97b9620d6107017",
      "2d14e5528bbd446e8a34c235201f88ac",
      "f4c9e362c7ff40559a1a5632a8b6e907",
      "3c4c633f0af84d099ffc74bac8901b07",
      "35226b53943b4db8a5d46aae09720818",
      "1a6faf9a5ed748f0809834cc52435b3d",
      "4fe179f7fab44513aeaa33cedf44f1bb",
      "5befa9362a09459695f4489c7173c34d",
      "458ad67bb385497889eab5bbdd977de5",
      "027a7c3f8a9e4114ac6d4e64fb62d440",
      "0cce924a44f94cbfaa69a78735ece1d5",
      "28ac2aeffcb6424f97fb47bfaee8e66f",
      "ab02cefe39774db3844d914b65a48790",
      "6bf6e7d710eb4924a3144e6e5d0679ca",
      "b7b774839b134ee687d9d89a0e15d166",
      "c583fda17dd44217875e86209fbad80a",
      "cd4eeedee4fc4f28acf650785bbaa5f3",
      "e5138fd5028f48f9ad8eea2b41b1592a",
      "82f4409e3f854415a871e3da31e9393e",
      "aeb337caa109443a9fc4f93e9f92753d",
      "e7e6399dec234052b7f30e3135a21b00",
      "44a764c1267042259b2c9597a69f6f76",
      "05e8f57e23ff47f0be24d3f9c18554d4",
      "616321e6e87a4d8589d9ed939e19c7b2",
      "8d7284d945004b98b28b6c631a2c4726",
      "97d864e9c07149aeb22a844ced3909d5",
      "dbadb6dfe23141ec8f38ffb747ea882e",
      "82d0c0a18606421f92ed77d8d4c61e10",
      "0c64a3bc4d4d461cbdc3c3c0acae2f94",
      "cf707e36593b4071b8253fd33d18b665",
      "079412e5345a4a09875ed0c86dde93e3",
      "1e2ffd31336946cf952abb5f62651c16",
      "8466edc3103c45d8a5d3fe8166508648",
      "f8d0d8f24551457e862d2e041f3699ab",
      "426852cccf704c58bdc7decbb1c583e6",
      "6b2663befc574a8e8154631c3ac95ff3",
      "091bd22cb6d94ae5b7c962d18e528ca2",
      "5ba5883cc2144929b9d7ddd980e4508d",
      "971d17ca015446b4bbf8a908835295cd",
      "c7086b545eae4b09af3b2d77b78af4b9",
      "5f47aeddde3449a5b5ed280d0ec9bc18",
      "43791444ad3e49f993d17887e6c15dbf",
      "c2c3403eb38b420cba7390d385f325be",
      "9097dd47971f445ea0fd55e7c42c15a0",
      "18cbf6cbaa4c436d9074224f83472863",
      "831180b810da4b549a8ca75536607e31",
      "d9779ccf530e49b1ac52172252ec5ef4",
      "78d69219fad44722a9bad3a12a3ccb61",
      "f08fa19e07644f4d9fff30d02040e315",
      "e1a2a1e765f042a7b9b081688688dab5",
      "76d299a97ab54dbe96f4525095afedcb",
      "73ac94bdcaf8427bbbcdd808f1428dff",
      "6f9a5d0dfe2f4aa49c57e942b927521b",
      "2d7810d299f649a2ac84541e7215d29a",
      "40dc8a185cf34e99a427f7c81bc76540",
      "bcac3e31740e4a2894b4e8bc9656a0c5",
      "87f29bce5b5d4df08b5337d62ffa9568",
      "3129b232f1e3411ead0d913654a11eaa",
      "4e7c3e87552640018f89c6cf9e070642",
      "01ccc633ad5d4350ad04651e360cc478",
      "735737e7c1eb477fae9d06d7324e81e8",
      "882b69fb84034d2dace625ef267f51cb",
      "8c77be8e5b74452bb028b97fd9edba36",
      "55a6296e28394f41b2a6d1f3d76e5944",
      "8f6c7a732bb6498db1de4d66f4b3c623",
      "22541d211626493e87c168143671e5ce",
      "111b9c6182994e2e85a323d07980b1ab",
      "3c53665ac233434fa899399da13650f0",
      "c4867626e6eb41bfaade350790de9f40",
      "a11866f208c4411dbc3d627ef6dbd74f",
      "3485c0ebc7f24965a0fadd3a6570d51e",
      "d111b1f753554eeda57cdbe420335fda",
      "8fb77e0530e946d38c007c9012c37a79",
      "8d669f0719d04980a6f04ffc2f65cf7c",
      "d79a649275a14fdeb2cb01e5ab75021f",
      "14186c7dd75e43d99183d5c8b308c0de"
     ]
    },
    "id": "5phRS_z2U_3T",
    "outputId": "e51c9c23-a48a-4c61-e7ee-cd0ef03915a4"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "169816892c8646e3888f213295349f00"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "train.parquet:   0%|          | 0.00/699k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0cce924a44f94cbfaa69a78735ece1d5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "validation.parquet:   0%|          | 0.00/90.0k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "44a764c1267042259b2c9597a69f6f76"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "test.parquet:   0%|          | 0.00/92.2k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8466edc3103c45d8a5d3fe8166508648"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9097dd47971f445ea0fd55e7c42c15a0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating validation split:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "40dc8a185cf34e99a427f7c81bc76540"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating test split:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "22541d211626493e87c168143671e5ce"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 8530\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": "from datasets import load_dataset\n# Load our data\ndata = load_dataset(\"rotten_tomatoes\")\ndata"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "X0KyKHtqyjn3"
   },
   "outputs": [],
   "source": "from sklearn.metrics import classification_report\ndef evaluate_performance(y_true, y_pred):\n    \"\"\"Create and print the classification report\"\"\"\n    performance = classification_report(\n        y_true, y_pred,\n        target_names=[\"Negative Review\", \"Positive Review\"]\n    )\n    print(performance)"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Your Turn - Text Classification Experiments\n",
    "\n",
    "Run each task first to see the baseline results. Follow the instructions to modify and experiment."
   ],
   "metadata": {
    "id": "NKYNfoaVC4hU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This section is divided into EASY, MEDIUM, & HARD."
   ],
   "metadata": {
    "id": "hHVONn85DElL"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Medium Tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Medium Tasks - Building Real Classifiers\n",
    "\n",
    "These tasks require more modification and experimentation. You'll build complete classification systems."
   ],
   "metadata": {
    "id": "2ipz0pJYEne6"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Run the code to see how 5-level classification works. Then try adding a 6th category.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#### Medium Task 1: Multi-Class Sentiment Classification\nIn this task, you'll build a sentiment classifier with 5 different categories (from extremely negative to extremely positive) instead of just binary positive/negative.\n**What to do:**\n1. Run the cells below to see baseline 5-level classification\n2. Observe which reviews are uncertain (low margin between top predictions)\n3. Try uncommenting the 6-level version to add more granularity\n4. Compare how predictions change with more categories"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Set up the 5 sentiment categories and compute embeddings:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import pipeline\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score, classification_report\nfrom datasets import load_dataset\nimport numpy as np\ndata = load_dataset(\"rotten_tomatoes\")\ntrain_size = 1000  # TODO: Try different values: 100, 500, 1000, 2000, 5000\ntest_size = 300\ntrain_subset = data[\"train\"].shuffle(seed=42).select(range(min(train_size, len(data[\"train\"]))))\ntest_subset = data[\"test\"].shuffle(seed=42).select(range(test_size))\nprint(f\"EXPERIMENT: Training Size = {train_size}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Classify each review and show confidence scores:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print()\nprint(\"CATEGORY CONFUSION ANALYSIS\")\nlabel_similarity = cosine_similarity(label_embeddings)\nprint(f\"{'Category Pair':<60s} {'Similarity':<12s}\")\nconfusions = []\nfor i in range(len(sentiment_labels)):\n    for j in range(i+1, len(sentiment_labels)):\n        sim = label_similarity[i][j]\n        confusions.append((i, j, sim))\nfor i, j, sim in sorted(confusions, key=lambda x: x[2], reverse=True)[:10]:\n    pair_name = f\"{sentiment_labels[i]} <-> {sentiment_labels[j]}\"\n    marker = \" \" if sim > 0.7 else \"\"\n    print(f\"{marker}{pair_name:<60s} {sim:.3f}\")"
  },
  {
   "cell_type": "markdown",
   "source": "As you can see, the classifier assigns each review to one of the 5 sentiment categories. The **margin** (difference between top 2 predictions) indicates confidence - large margins (>0.15) mean the model is confident, while small margins (<0.05) indicate uncertainty. Reviews with extreme language (\"best ever\", \"terrible\") have higher confidence, while moderate reviews (\"pretty good\", \"quite bad\") show more uncertainty.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Analyze which categories are most similar to each other:"
  },
  {
   "cell_type": "markdown",
   "source": "Notice that adjacent categories (like \"somewhat negative\" and \"neutral\") tend to have higher similarity scores, which explains why the model sometimes confuses them. Categories with similarity > 0.7 are particularly prone to confusion.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Medium Task 2: Classifier Performance with Limited Training Data\n\nTry different training sizes (100, 500, 1000, 2000) and fill in the results table.\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from transformers import pipeline\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score, classification_report\nfrom datasets import load_dataset\nimport numpy as np\ndata = load_dataset(\"rotten_tomatoes\")\ntrain_size = 1000  # TODO: Try different values: 100, 500, 1000, 2000, 5000\ntest_size = 300\ntrain_subset = data[\"train\"].shuffle(seed=42).select(range(min(train_size, len(data[\"train\"]))))\ntest_subset = data[\"test\"].shuffle(seed=42).select(range(test_size))\nprint(f\"EXPERIMENT: Training Size = {train_size}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import pipeline\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score, classification_report\nfrom datasets import load_dataset\nimport numpy as np\ndata = load_dataset(\"rotten_tomatoes\")\n# TODO: Try different values: 100, 500, 1000, 2000, 5000\ntrain_size = 1000\ntest_size = 300\ntrain_subset = data[\"train\"].shuffle(seed=42).select(range(min(train_size, len(data[\"train\"]))))\ntest_subset = data[\"test\"].shuffle(seed=42).select(range(test_size))\nprint(f\"EXPERIMENT: Training Size = {train_size}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n[1/2] Testing Task-Specific Model...\")\ntask_model = pipeline(\n    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n    tokenizer=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n    return_all_scores=True,\n    device=-1\n)\ny_pred_task = []\nfor text in test_subset[\"text\"]:\n    output = task_model(text)[0]\n    neg_score = output[0][\"score\"]\n    pos_score = output[2][\"score\"]\n    y_pred_task.append(1 if pos_score > neg_score else 0)\ntask_f1 = f1_score(test_subset[\"label\"], y_pred_task, average='weighted')\nprint(f\" Task-Specific Model F1: {task_f1:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Train the embedding-based classifier on your labeled data:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(f\"\\n[2/2] Training Embedding Classifier on {train_size} samples...\")\nembedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\ntrain_embeddings = embedding_model.encode(train_subset[\"text\"], show_progress_bar=False)\ntest_embeddings = embedding_model.encode(test_subset[\"text\"], show_progress_bar=False)\nclf = LogisticRegression(random_state=42, max_iter=1000)\nclf.fit(train_embeddings, train_subset[\"label\"])\ny_pred_embed = clf.predict(test_embeddings)\nembed_f1 = f1_score(test_subset[\"label\"], y_pred_embed, average='weighted')\nprint(f\" Embedding Classifier F1: {embed_f1:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Compare the two approaches and show example predictions:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print()\nprint(\"RESULTS SUMMARY\")\nprint(f\"Training samples used: {train_size}\")\nprint(f\"\\nTask-Specific (pre-trained):  F1 = {task_f1:.4f}\")\nprint(f\"Embedding + Classifier:       F1 = {embed_f1:.4f}\")\nprint(f\"Difference:                       {embed_f1 - task_f1:+.4f}\")\nif embed_f1 > task_f1:\n    print(f\"\\n Embedding approach WINS with {train_size} samples!\")\nelif embed_f1 > task_f1 - 0.01:\n    print(f\"\\n Essentially TIED\")\nelse:\n    print(f\"\\n Task-specific model wins\")\n# Show example predictions\nprint()\nprint(\"EXAMPLE PREDICTIONS (first 5)\")\nfor i in range(5):\n    true_label = \"Positive\" if test_subset[\"label\"][i] == 1 else \"Negative\"\n    task_pred = \"Positive\" if y_pred_task[i] == 1 else \"Negative\"\n    embed_pred = \"Positive\" if y_pred_embed[i] == 1 else \"Negative\"\n    task_correct = \"\" if y_pred_task[i] == test_subset[\"label\"][i] else \"\"\n    embed_correct = \"\" if y_pred_embed[i] == test_subset[\"label\"][i] else \"\"\n    print(f\"\\n{i+1}. '{test_subset['text'][i][:60]}...'\")\n    print(f\"   True: {true_label}\")\n    print(f\"   Task-Specific: {task_pred} {task_correct}\")\n    print(f\"   Embedding:     {embed_pred} {embed_correct}\")\nprint()\nprint(\"TODO: Record your results\")\nprint(f\"Current: | {train_size:<10} | {task_f1:.4f}  | {embed_f1:.4f}       | {'Embed' if embed_f1 > task_f1 else 'Task':<11} |\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Questions\n\n1. At what training size did embedding classifier match the task-specific model?\n\n2. Were there cases where one model was correct and the other wrong?\n\n3. Is 100 samples enough labeled data?\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Medium Task 3: Confidence-Based Classifier with Uncertainty Handling\n\nIn production, refusing a prediction beats making a wrong one. Here's the key insight: when your model is uncertain, it should say \"I don't know\" rather than guessing. This creates a trade-off between coverage (how many predictions you make) and accuracy (how often you're right).\n\nTry this:\n- Run with threshold of 0.15 first\n- Test 0.05, 0.30, and 0.50 to see how the trade-off shifts\n- Check the uncertain cases (typically have hedging language)\n- Experiment with the alternative uncertainty measure"
  },
  {
   "cell_type": "code",
   "source": "from sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "from sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# TODO: After experimenting with thresholds, analyze the trade-off\nprint()\nprint(\"=\"*80)\nprint(\"EXPERIMENT LOG - Fill this in as you try different thresholds:\")\nprint(\"=\"*80)\nprint(\"| Threshold | Coverage | Accuracy | Notes                    |\")\nprint(\"|-----------|----------|----------|--------------------------|\")\nprint(\"| 0.05      | ??.?%    | ??.?%    | ?                        |\")\nprint(\"| 0.15      | ??.?%    | ??.?%    | ?                        |\")\nprint(\"| 0.30      | ??.?%    | ??.?%    | ?                        |\")\nprint(\"| 0.50      | ??.?%    | ??.?%    | ?                        |\")\nprint()\nprint(f\"Current:    | {confidence_threshold:<9.2f} | {coverage*100:>5.1f}%    | {accuracy*100:>5.1f}%    |\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Experiment log for tracking results:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Show which reviews were uncertain\nif n_uncertain > 0:\n    print(f\"\\n\" + \"-\"*80)\n    print(f\"UNCERTAIN CASES (margin < {confidence_threshold}):\")\n    print(\"-\"*80)\n    for r in uncertain_cases:\n        print(f\"  • '{r['review']}'\")\n        print(f\"    Margin: {r['margin']:.3f} (too close to call)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "from sentence_transformers import SentenceTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom datasets import load_dataset\nimport numpy as np",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print()\nprint(\"=\"*80)\nprint(\"TODO: Based on your error analysis, propose improvements:\")\nprint(\"=\"*80)\nprint(\"# Write your observations here:\")\nprint(\"# 1. What patterns did you notice in the errors?\")\nprint(\"# 2. Which edge cases failed most?\")\nprint(\"# 3. How would you improve the classifier?\")\nprint(\"#    - Better training data?\")\nprint(\"#    - Different features?\")\nprint(\"#    - Ensemble approach?\")\nprint(\"#    - Confidence thresholds?\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Reflection questions for improvement:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Summary and insights\nprint()\nprint(\"=\"*80)\nprint(\"KEY INSIGHTS FROM ERROR ANALYSIS\")\nprint(\"=\"*80)\n\nprint(\"\\n1. Error Distribution:\")\nprint(f\"   - False Positives (predicted too optimistic): {len(false_positives)}\")\nprint(f\"   - False Negatives (predicted too pessimistic): {len(false_negatives)}\")\n\nif len(false_positives) > len(false_negatives):\n    print(f\"   → Classifier has POSITIVE BIAS\")\nelif len(false_negatives) > len(false_positives):\n    print(f\"   → Classifier has NEGATIVE BIAS\")\n\nprint(\"\\n2. Challenging Cases:\")\nfailing_categories = [cat for cat, text, true in edge_cases\n                     if clf.predict(model.encode([text]))[0] != true]\nif failing_categories:\n    print(f\"   The classifier struggles with: {', '.join(failing_categories)}\")\n\nprint(\"\\n3. Confidence Analysis:\")\nif high_conf_errors:\n    print(f\"   Found {len(high_conf_errors)} high-confidence errors\")\n    print(f\"   → The model is 'confidently wrong' on some cases\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Summary of key insights:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "edge_accuracy = correct_count / len(edge_cases)\n\nprint(f\"\\n\" + \"-\"*80)\nprint(f\"Edge Case Accuracy: {correct_count}/{len(edge_cases)} ({edge_accuracy:.1%})\")\nprint(f\"Regular Test Accuracy: {accuracy:.1%}\")\nprint(f\"Difference: {accuracy - edge_accuracy:+.1%}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Compare edge case performance to regular test set:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print(\"\\nTesting challenging cases that often fail:\")\nprint(\"-\"*80)\n\nedge_embeddings = model.encode([text for _, text, _ in edge_cases])\nedge_predictions = clf.predict(edge_embeddings)\nedge_probs = clf.predict_proba(edge_embeddings)\n\ncorrect_count = 0\nfor i, (category, text, true_label) in enumerate(edge_cases):\n    pred = edge_predictions[i]\n    conf = edge_probs[i][pred]\n    correct = pred == true_label\n    if correct:\n        correct_count += 1\n    \n    true_sent = \"Positive\" if true_label == 1 else \"Negative\"\n    pred_sent = \"Positive\" if pred == 1 else \"Negative\"\n    \n    print(f\"\\n{category}: '{text}'\")\n    print(f\"  True: {true_sent} | Predicted: {pred_sent} | Confidence: {conf:.3f}\")\n    print(f\"  Result: {'✓ CORRECT' if correct else '✗ WRONG'}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Run predictions on edge cases:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Test edge cases\nprint()\nprint(\"=\"*80)\nprint(\"TESTING EDGE CASES\")\nprint(\"=\"*80)\n\nedge_cases = [\n    (\"Sarcastic\", \"Oh great, another masterpiece. NOT!\", 0),\n    (\"Mixed\", \"The acting was great but the plot was terrible\", 0),\n    (\"Backhanded\", \"Not as bad as I expected\", 1),\n    (\"Double negative\", \"Not unwatchable\", 1),\n    (\"Very short\", \"Boring\", 0),\n    (\"Ambiguous\", \"It was a movie\", 0),\n]\n\n# TODO: After analyzing above errors, add your own test cases:\n# edge_cases.extend([\n#     (\"Your category\", \"Your test review here\", expected_label_0_or_1),\n#     (\"Another category\", \"Another test review\", expected_label),\n# ])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Test challenging edge cases:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Analyze by text length\nprint(f\"\\n\" + \"-\"*80)\nprint(\"ERROR ANALYSIS BY TEXT LENGTH\")\nprint(\"-\"*80)\n\nerror_lengths = [e['length'] for e in errors]\ncorrect_lengths = [len(test_subset[\"text\"][i].split())\n                   for i in range(len(test_subset))\n                   if predictions[i] == test_subset[\"label\"][i]]\n\navg_error_length = np.mean(error_lengths) if error_lengths else 0\navg_correct_length = np.mean(correct_lengths) if correct_lengths else 0\n\nprint(f\"\\nAverage length of ERROR reviews: {avg_error_length:.1f} words\")\nprint(f\"Average length of CORRECT reviews: {avg_correct_length:.1f} words\")\n\nif avg_error_length < avg_correct_length:\n    print(f\"→ Observation: Errors tend to be SHORTER\")\nelif avg_error_length > avg_correct_length:\n    print(f\"→ Observation: Errors tend to be LONGER\")\nelse:\n    print(f\"→ Observation: No clear length pattern\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Analyze error patterns by text length:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Show high-confidence errors (most surprising)\nhigh_conf_errors = [e for e in errors if e['confidence'] > 0.7]\n\nprint(f\"\\n\" + \"-\"*80)\nprint(f\"HIGH-CONFIDENCE ERRORS (confidence > 0.7)\")\nprint(f\"These are the most surprising mistakes:\")\nprint(\"-\"*80)\n\nfor i, error in enumerate(high_conf_errors[:5]):\n    true_sent = \"Positive\" if error['true_label'] == 1 else \"Negative\"\n    pred_sent = \"Positive\" if error['predicted_label'] == 1 else \"Negative\"\n    print(f\"\\n{i+1}. '{error['text']}'\")\n    print(f\"   True: {true_sent} | Predicted: {pred_sent} | Confidence: {error['confidence']:.3f}\")\n    print(f\"   Length: {error['length']} words\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Show high-confidence errors (most surprising mistakes):",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Analyze errors\nprint()\nprint(\"=\"*80)\nprint(\"ERROR ANALYSIS\")\nprint(\"=\"*80)\n\nerrors = []\nfor i in range(len(test_subset)):\n    if predictions[i] != test_subset[\"label\"][i]:\n        confidence = probabilities[i][predictions[i]]\n        errors.append({\n            'index': i,\n            'text': test_subset[\"text\"][i],\n            'true_label': test_subset[\"label\"][i],\n            'predicted_label': predictions[i],\n            'confidence': confidence,\n            'length': len(test_subset[\"text\"][i].split())\n        })\n\ntotal_errors = len(errors)\ntotal_samples = len(test_subset)\naccuracy = (total_samples - total_errors) / total_samples\n\nprint(f\"\\nOverall Performance:\")\nprint(f\"  Correct: {total_samples - total_errors}/{total_samples} ({accuracy:.1%})\")\nprint(f\"  Errors:  {total_errors}/{total_samples} ({total_errors/total_samples:.1%})\")\n\n# Categorize errors\nfalse_positives = [e for e in errors if e['predicted_label'] == 1]\nfalse_negatives = [e for e in errors if e['predicted_label'] == 0]\n\nprint(f\"\\nError Types:\")\nprint(f\"  False Positives: {len(false_positives)} (predicted positive, actually negative)\")\nprint(f\"  False Negatives: {len(false_negatives)} (predicted negative, actually positive)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Analyze classification errors:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print(\"Training classifier on 1000 movie reviews...\")\ntrain_embeddings = model.encode(train_subset[\"text\"], show_progress_bar=False)\ntest_embeddings = model.encode(test_subset[\"text\"], show_progress_bar=False)\n\nclf = LogisticRegression(random_state=42, max_iter=1000)\nclf.fit(train_embeddings, train_subset[\"label\"])\n\n# Get predictions\npredictions = clf.predict(test_embeddings)\nprobabilities = clf.predict_proba(test_embeddings)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Train the classifier:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load data\ndata = load_dataset(\"rotten_tomatoes\")\nmodel = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n\n# Use subset for faster experimentation\ntrain_subset = data[\"train\"].shuffle(seed=42).select(range(1000))\ntest_subset = data[\"test\"].shuffle(seed=42).select(range(200))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Load and prepare the dataset:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Calculate metrics\nmade_predictions = [r for r in results if r['pred'] is not None]\nuncertain_cases = [r for r in results if r['pred'] is None]\ncorrect_predictions = [r for r in made_predictions if r['pred'] == r['true']]\n\ntotal = len(results)\nn_predicted = len(made_predictions)\nn_uncertain = len(uncertain_cases)\nn_correct = len(correct_predictions)\n\ncoverage = n_predicted / total\naccuracy = n_correct / n_predicted if n_predicted > 0 else 0\n\nprint()\nprint(\"=\"*80)\nprint(\"PERFORMANCE ANALYSIS\")\nprint(\"=\"*80)\n\nprint(f\"\\nCoverage: {n_predicted}/{total} = {coverage:.1%}\")\nprint(f\"  → Made predictions for {n_predicted} reviews\")\nprint(f\"  → Refused to predict on {n_uncertain} reviews\")\n\nprint(f\"\\nAccuracy (on predictions made): {n_correct}/{n_predicted} = {accuracy:.1%}\")\nprint(f\"  → Of the {n_predicted} predictions, {n_correct} were correct\")\n\nprint(f\"\\nTrade-off Analysis:\")\nprint(f\"  Threshold = {confidence_threshold}\")\nprint(f\"  → Higher threshold = fewer predictions but higher accuracy\")\nprint(f\"  → Lower threshold = more predictions but lower accuracy\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Calculate performance metrics:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Classify with confidence threshold\nresults = []\npredictions = []\nconfidences = []\n\nprint(f\"CONFIDENCE-BASED CLASSIFICATION (threshold={confidence_threshold})\")\nprint(\"=\"*80)\n\nfor i, review in enumerate(test_reviews):\n    similarities = sim_matrix[i]\n    predicted_idx = np.argmax(similarities)\n    top_confidence = similarities[predicted_idx]\n    margin = calculate_margin(similarities)\n    \n    # Decision: predict only if confident enough\n    if margin >= confidence_threshold:\n        prediction = predicted_idx\n        status = \"PREDICTED\"\n        predictions.append(prediction)\n    else:\n        prediction = None\n        status = \"UNCERTAIN\"\n        predictions.append(None)\n    \n    true_label = \"Positive\" if y_true[i] == 1 else \"Negative\"\n    pred_label = labels[predicted_idx] if prediction is not None else \"UNCERTAIN\"\n    \n    print(f\"\\n{i+1}. '{review}'\")\n    print(f\"   True label: {true_label}\")\n    print(f\"   Prediction: {pred_label}\")\n    print(f\"   Top confidence: {top_confidence:.3f}\")\n    print(f\"   Margin: {margin:.3f} {'✓ Above threshold' if margin >= confidence_threshold else '✗ Below threshold'}\")\n    print(f\"   Status: {status}\", end=\"\")\n    \n    if prediction is not None:\n        correct = prediction == y_true[i]\n        print(f\" - {'✓ CORRECT' if correct else '✗ INCORRECT'}\")\n    else:\n        print()\n    \n    results.append({\n        'review': review,\n        'true': y_true[i],\n        'pred': prediction,\n        'margin': margin,\n        'status': status\n    })",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Run classification with confidence threshold:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def calculate_margin(similarities):\n    \"\"\"\n    Margin = difference between top two predictions\n    Small margin = uncertain (predictions are close)\n    \"\"\"\n    sorted_sims = np.sort(similarities)[::-1]\n    margin = sorted_sims[0] - sorted_sims[1]\n    return margin\n\n# TODO: After first run, uncomment this alternative uncertainty measure:\n# def calculate_margin(similarities):\n#     \"\"\"\n#     Alternative: Use absolute confidence in top prediction\n#     Low confidence = uncertain\n#     \"\"\"\n#     max_confidence = np.max(similarities)\n#     # Convert to margin-like score (higher = more certain)\n#     # If max is 0.6, margin = 0.6 - 0.5 = 0.1 (uncertain)\n#     # If max is 0.9, margin = 0.9 - 0.5 = 0.4 (certain)\n#     margin = max_confidence - 0.5\n#     return margin",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Define the margin calculation function:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "label_embeddings = model.encode(labels)\nreview_embeddings = model.encode(test_reviews)\nsim_matrix = cosine_similarity(review_embeddings, label_embeddings)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Compute embeddings and similarity scores:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "labels = [\"A negative movie review\", \"A positive movie review\"]\n\n# TODO: EXPERIMENT WITH THIS - Try: 0.05, 0.15, 0.30, 0.50\nconfidence_threshold = 0.15",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Configure labels and confidence threshold:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n\n# Reviews with varying levels of clarity\ntest_reviews = [\n    \"Absolutely fantastic! Best movie ever!\",           # Clear positive\n    \"Pretty good, I liked it\",                          # Weak positive\n    \"It was fine, nothing special\",                     # Ambiguous\n    \"Not bad but not great either\",                     # Very ambiguous\n    \"Quite disappointing\",                              # Weak negative\n    \"Terrible! Complete waste of time!\",                # Clear negative\n    \"The movie had some interesting moments\",           # Ambiguous positive\n    \"Outstanding performances all around!\",             # Clear positive\n]\n\n# True labels (for evaluation)\ny_true = [1, 1, 0, 0, 0, 0, 1, 1]  # 1=positive, 0=negative",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Load the model and set up test data:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "Define the uncertainty measure (margin calculation):",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "label_embeddings = model.encode(labels)\nreview_embeddings = model.encode(test_reviews)\nsim_matrix = cosine_similarity(review_embeddings, label_embeddings)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Compute embeddings and similarity matrix:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "labels = [\"A negative movie review\", \"A positive movie review\"]\nconfidence_threshold = 0.15  # TODO: Try values: 0.05, 0.15, 0.30, 0.50",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Set up the classification labels and confidence threshold:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n\n# Reviews with varying clarity levels\ntest_reviews = [\n    \"Absolutely fantastic! Best movie ever!\",\n    \"Pretty good, I liked it\",\n    \"It was fine, nothing special\",\n    \"Not bad but not great either\",\n    \"Quite disappointing\",\n    \"Terrible! Complete waste of time!\",\n    \"The movie had some interesting moments\",\n    \"Outstanding performances all around!\",\n]\n\ny_true = [1, 1, 0, 0, 0, 0, 1, 1]  # True labels: 1=positive, 0=negative",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Load the model and prepare test reviews:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\nmodel = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n# Reviews with varying levels of clarity\ntest_reviews = [\n    \"Absolutely fantastic! Best movie ever!\",           # Clear positive\n    \"Pretty good, I liked it\",                          # Weak positive\n    \"It was fine, nothing special\",                     # Ambiguous\n    \"Not bad but not great either\",                     # Very ambiguous\n    \"Quite disappointing\",                              # Weak negative\n    \"Terrible! Complete waste of time!\",                # Clear negative\n    \"The movie had some interesting moments\",           # Ambiguous positive\n    \"Outstanding performances all around!\",             # Clear positive\n]\n# True labels (for evaluation)\ny_true = [1, 1, 0, 0, 0, 0, 1, 1]  # 1=positive, 0=negative\nlabels = [\"A negative movie review\", \"A positive movie review\"]\n# TODO: EXPERIMENT WITH THIS - Try: 0.05, 0.15, 0.30, 0.50\nconfidence_threshold = 0.15\nlabel_embeddings = model.encode(labels)\nreview_embeddings = model.encode(test_reviews)\nsim_matrix = cosine_similarity(review_embeddings, label_embeddings)\ndef calculate_margin(similarities):\n    \"\"\"\n    Margin = difference between top two predictions\n    Small margin = uncertain (predictions are close)\n    \"\"\"\n    sorted_sims = np.sort(similarities)[::-1]\n    margin = sorted_sims[0] - sorted_sims[1]\n    return margin\n# TODO: After first run, uncomment this alternative uncertainty measure:\n# def calculate_margin(similarities):\n#     \"\"\"\n#     Alternative: Use absolute confidence in top prediction\n#     Low confidence = uncertain\n#     \"\"\"\n#     max_confidence = np.max(similarities)\n#     # Convert to margin-like score (higher = more certain)\n#     # If max is 0.6, margin = 0.6 - 0.5 = 0.1 (uncertain)\n#     # If max is 0.9, margin = 0.9 - 0.5 = 0.4 (certain)\n#     margin = max_confidence - 0.5\n#     return margin\n# Classify with confidence threshold\nresults = []\npredictions = []\nconfidences = []\nprint(f\"CONFIDENCE-BASED CLASSIFICATION (threshold={confidence_threshold})\")\nfor i, review in enumerate(test_reviews):\n    similarities = sim_matrix[i]\n    predicted_idx = np.argmax(similarities)\n    top_confidence = similarities[predicted_idx]\n    margin = calculate_margin(similarities)\n    # Decision: predict only if confident enough\n    if margin >= confidence_threshold:\n        prediction = predicted_idx\n        status = \"PREDICTED\"\n        predictions.append(prediction)\n    else:\n        prediction = None\n        status = \"UNCERTAIN\"\n        predictions.append(None)\n    true_label = \"Positive\" if y_true[i] == 1 else \"Negative\"\n    pred_label = labels[predicted_idx] if prediction is not None else \"UNCERTAIN\"\n    print(f\"\\n{i+1}. '{review}'\")\n    print(f\"   True label: {true_label}\")\n    print(f\"   Prediction: {pred_label}\")\n    print(f\"   Top confidence: {top_confidence:.3f}\")\n    print(f\"   Margin: {margin:.3f} {' Above threshold' if margin >= confidence_threshold else ' Below threshold'}\")\n    print(f\"   Status: {status}\", end=\"\")\n    if prediction is not None:\n        correct = prediction == y_true[i]\n        print(f\" - {' CORRECT' if correct else ' INCORRECT'}\")\n    else:\n        print()\n    results.append({\n        'review': review,\n        'true': y_true[i],\n        'pred': prediction,\n        'margin': margin,\n        'status': status\n    })\n# Calculate metrics\nprint()\nprint(\"PERFORMANCE ANALYSIS\")\nmade_predictions = [r for r in results if r['pred'] is not None]\nuncertain_cases = [r for r in results if r['pred'] is None]\ncorrect_predictions = [r for r in made_predictions if r['pred'] == r['true']]\ntotal = len(results)\nn_predicted = len(made_predictions)\nn_uncertain = len(uncertain_cases)\nn_correct = len(correct_predictions)\ncoverage = n_predicted / total\naccuracy = n_correct / n_predicted if n_predicted > 0 else 0\nprint(f\"\\nCoverage: {n_predicted}/{total} = {coverage:.1%}\")\nprint(f\"   Made predictions for {n_predicted} reviews\")\nprint(f\"   Refused to predict on {n_uncertain} reviews\")\nprint(f\"\\nAccuracy (on predictions made): {n_correct}/{n_predicted} = {accuracy:.1%}\")\nprint(f\"   Of the {n_predicted} predictions, {n_correct} were correct\")\nprint(f\"\\nTrade-off Analysis:\")\nprint(f\"  Threshold = {confidence_threshold}\")\nprint(f\"   Higher threshold = fewer predictions but higher accuracy\")\nprint(f\"   Lower threshold = more predictions but lower accuracy\")\n# Show which reviews were uncertain\nif n_uncertain > 0:\n    print(f\"\\n\" + \"-\"*80)\n    print(f\"UNCERTAIN CASES (margin < {confidence_threshold}):\")\n    for r in uncertain_cases:\n        print(f\"  • '{r['review']}'\")\n        print(f\"    Margin: {r['margin']:.3f} (too close to call)\")\n# TODO: After experimenting with thresholds, analyze the trade-off\nprint()\nprint(\"EXPERIMENT LOG - Fill this in as you try different thresholds:\")\nprint(\"| Threshold | Coverage | Accuracy | Notes                    |\")\nprint(\"|-----------|----------|----------|--------------------------|\")\nprint(\"| 0.05      | ??.?%    | ??.?%    | ?                        |\")\nprint(\"| 0.15      | ??.?%    | ??.?%    | ?                        |\")\nprint(\"| 0.30      | ??.?%    | ??.?%    | ?                        |\")\nprint(\"| 0.50      | ??.?%    | ??.?%    | ?                        |\")\nprint()\nprint(f\"Current:    | {confidence_threshold:<9.2f} | {coverage*100:>5.1f}%    | {accuracy*100:>5.1f}%    |\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lpzBi9YBFZRi",
    "outputId": "28f21c01-c8d7-4c04-8256-6f0dd47f890c"
   },
   "execution_count": 32,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "================================================================================\n",
      "CONFIDENCE-BASED CLASSIFICATION (threshold=0.15)\n",
      "================================================================================\n",
      "\n",
      "1. 'Absolutely fantastic! Best movie ever!'\n",
      "   True label: Positive\n",
      "   Prediction: UNCERTAIN\n",
      "   Top confidence: 0.451\n",
      "   Margin: 0.092 ✗ Below threshold\n",
      "   Status: UNCERTAIN\n",
      "\n",
      "2. 'Pretty good, I liked it'\n",
      "   True label: Positive\n",
      "   Prediction: UNCERTAIN\n",
      "   Top confidence: 0.410\n",
      "   Margin: 0.018 ✗ Below threshold\n",
      "   Status: UNCERTAIN\n",
      "\n",
      "3. 'It was fine, nothing special'\n",
      "   True label: Negative\n",
      "   Prediction: UNCERTAIN\n",
      "   Top confidence: 0.418\n",
      "   Margin: 0.051 ✗ Below threshold\n",
      "   Status: UNCERTAIN\n",
      "\n",
      "4. 'Not bad but not great either'\n",
      "   True label: Negative\n",
      "   Prediction: UNCERTAIN\n",
      "   Top confidence: 0.414\n",
      "   Margin: 0.053 ✗ Below threshold\n",
      "   Status: UNCERTAIN\n",
      "\n",
      "5. 'Quite disappointing'\n",
      "   True label: Negative\n",
      "   Prediction: UNCERTAIN\n",
      "   Top confidence: 0.354\n",
      "   Margin: 0.080 ✗ Below threshold\n",
      "   Status: UNCERTAIN\n",
      "\n",
      "6. 'Terrible! Complete waste of time!'\n",
      "   True label: Negative\n",
      "   Prediction: UNCERTAIN\n",
      "   Top confidence: 0.397\n",
      "   Margin: 0.121 ✗ Below threshold\n",
      "   Status: UNCERTAIN\n",
      "\n",
      "7. 'The movie had some interesting moments'\n",
      "   True label: Positive\n",
      "   Prediction: UNCERTAIN\n",
      "   Top confidence: 0.506\n",
      "   Margin: 0.118 ✗ Below threshold\n",
      "   Status: UNCERTAIN\n",
      "\n",
      "8. 'Outstanding performances all around!'\n",
      "   True label: Positive\n",
      "   Prediction: UNCERTAIN\n",
      "   Top confidence: 0.282\n",
      "   Margin: 0.098 ✗ Below threshold\n",
      "   Status: UNCERTAIN\n",
      "\n",
      "================================================================================\n",
      "PERFORMANCE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Coverage: 0/8 = 0.0%\n",
      "  → Made predictions for 0 reviews\n",
      "  → Refused to predict on 8 reviews\n",
      "\n",
      "Accuracy (on predictions made): 0/0 = 0.0%\n",
      "  → Of the 0 predictions, 0 were correct\n",
      "\n",
      "Trade-off Analysis:\n",
      "  Threshold = 0.15\n",
      "  → Higher threshold = fewer predictions but higher accuracy\n",
      "  → Lower threshold = more predictions but lower accuracy\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "UNCERTAIN CASES (margin < 0.15):\n",
      "--------------------------------------------------------------------------------\n",
      "  • 'Absolutely fantastic! Best movie ever!'\n",
      "    Margin: 0.092 (too close to call)\n",
      "  • 'Pretty good, I liked it'\n",
      "    Margin: 0.018 (too close to call)\n",
      "  • 'It was fine, nothing special'\n",
      "    Margin: 0.051 (too close to call)\n",
      "  • 'Not bad but not great either'\n",
      "    Margin: 0.053 (too close to call)\n",
      "  • 'Quite disappointing'\n",
      "    Margin: 0.080 (too close to call)\n",
      "  • 'Terrible! Complete waste of time!'\n",
      "    Margin: 0.121 (too close to call)\n",
      "  • 'The movie had some interesting moments'\n",
      "    Margin: 0.118 (too close to call)\n",
      "  • 'Outstanding performances all around!'\n",
      "    Margin: 0.098 (too close to call)\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT LOG - Fill this in as you try different thresholds:\n",
      "================================================================================\n",
      "| Threshold | Coverage | Accuracy | Notes                    |\n",
      "|-----------|----------|----------|--------------------------|\n",
      "| 0.05      | ??.?%    | ??.?%    | ?                        |\n",
      "| 0.15      | ??.?%    | ??.?%    | ?                        |\n",
      "| 0.30      | ??.?%    | ??.?%    | ?                        |\n",
      "| 0.50      | ??.?%    | ??.?%    | ?                        |\n",
      "\n",
      "Current:    | 0.15      |   0.0%    |   0.0%    |\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "# Calculate metrics\nmade_predictions = [r for r in results if r['pred'] is not None]\nuncertain_cases = [r for r in results if r['pred'] is None]\ncorrect_predictions = [r for r in made_predictions if r['pred'] == r['true']]\ntotal = len(results)\nn_predicted = len(made_predictions)\nn_uncertain = len(uncertain_cases)\nn_correct = len(correct_predictions)\ncoverage = n_predicted / total\naccuracy = n_correct / n_predicted if n_predicted > 0 else 0\nprint()\nprint(\"PERFORMANCE ANALYSIS\")\nprint(f\"\\nCoverage: {n_predicted}/{total} = {coverage:.1%}\")\nprint(f\"   Made predictions for {n_predicted} reviews\")\nprint(f\"   Refused to predict on {n_uncertain} reviews\")\nprint(f\"\\nAccuracy (on predictions made): {n_correct}/{n_predicted} = {accuracy:.1%}\")\nprint(f\"   Of the {n_predicted} predictions, {n_correct} were correct\")\nprint(f\"\\nTrade-off Analysis:\")\nprint(f\"  Threshold = {confidence_threshold}\")\nprint(f\"   Higher threshold = fewer predictions but higher accuracy\")\nprint(f\"   Lower threshold = more predictions but lower accuracy\")\nif n_uncertain > 0:\n    print(f\"\\n\" + \"-\"*80)\n    print(f\"UNCERTAIN CASES (margin < {confidence_threshold}):\")\n    for r in uncertain_cases:\n        print(f\"  • '{r['review']}'\")\n        print(f\"    Margin: {r['margin']:.3f} (too close to call)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Questions\n",
    "\n",
    "1. What do uncertain reviews have in common? Are they using hedging language like \"kind of\" or \"somewhat\"?\n",
    "\n",
    "2. Compare results at threshold=0.05 vs 0.30. Describe the coverage vs accuracy trade-off. When would you want high coverage vs high accuracy?\n",
    "\n",
    "3. How could you use confidence-based prediction in production? What should a system do when the model is uncertain?"
   ],
   "metadata": {
    "id": "H7-OE7H9F6vx"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Medium Task 4: Classifier Failure Analysis\n\nTrain the classifier and see what kinds of reviews it gets wrong. Then add your own test cases.\n"
  },
  {
   "cell_type": "code",
   "source": "from sentence_transformers import SentenceTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom datasets import load_dataset\nimport numpy as np\n# Load data\ndata = load_dataset(\"rotten_tomatoes\")\nmodel = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n# Use subset for faster experimentation\ntrain_subset = data[\"train\"].shuffle(seed=42).select(range(1000))\ntest_subset = data[\"test\"].shuffle(seed=42).select(range(200))\n# Train classifier\nprint(\"Training classifier on 1000 movie reviews...\")\ntrain_embeddings = model.encode(train_subset[\"text\"], show_progress_bar=False)\ntest_embeddings = model.encode(test_subset[\"text\"], show_progress_bar=False)\nclf = LogisticRegression(random_state=42, max_iter=1000)\nclf.fit(train_embeddings, train_subset[\"label\"])\n# Get predictions\npredictions = clf.predict(test_embeddings)\nprobabilities = clf.predict_proba(test_embeddings)\n# Analyze errors\nprint()\nprint(\"ERROR ANALYSIS\")\nerrors = []\nfor i in range(len(test_subset)):\n    if predictions[i] != test_subset[\"label\"][i]:\n        confidence = probabilities[i][predictions[i]]\n        errors.append({\n            'index': i,\n            'text': test_subset[\"text\"][i],\n            'true_label': test_subset[\"label\"][i],\n            'predicted_label': predictions[i],\n            'confidence': confidence,\n            'length': len(test_subset[\"text\"][i].split())\n        })\ntotal_errors = len(errors)\ntotal_samples = len(test_subset)\naccuracy = (total_samples - total_errors) / total_samples\nprint(f\"\\nOverall Performance:\")\nprint(f\"  Correct: {total_samples - total_errors}/{total_samples} ({accuracy:.1%})\")\nprint(f\"  Errors:  {total_errors}/{total_samples} ({total_errors/total_samples:.1%})\")\n# Categorize errors\nfalse_positives = [e for e in errors if e['predicted_label'] == 1]\nfalse_negatives = [e for e in errors if e['predicted_label'] == 0]\nprint(f\"\\nError Types:\")\nprint(f\"  False Positives: {len(false_positives)} (predicted positive, actually negative)\")\nprint(f\"  False Negatives: {len(false_negatives)} (predicted negative, actually positive)\")\n# Show high-confidence errors (most surprising)\nhigh_conf_errors = [e for e in errors if e['confidence'] > 0.7]\nprint(f\"\\n\" + \"-\"*80)\nprint(f\"HIGH-CONFIDENCE ERRORS (confidence > 0.7)\")\nprint(f\"These are the most surprising mistakes:\")\nfor i, error in enumerate(high_conf_errors[:5]):\n    true_sent = \"Positive\" if error['true_label'] == 1 else \"Negative\"\n    pred_sent = \"Positive\" if error['predicted_label'] == 1 else \"Negative\"\n    print(f\"\\n{i+1}. '{error['text']}'\")\n    print(f\"   True: {true_sent} | Predicted: {pred_sent} | Confidence: {error['confidence']:.3f}\")\n    print(f\"   Length: {error['length']} words\")\n# Analyze by text length\nprint(f\"\\n\" + \"-\"*80)\nprint(\"ERROR ANALYSIS BY TEXT LENGTH\")\nerror_lengths = [e['length'] for e in errors]\ncorrect_lengths = [len(test_subset[\"text\"][i].split())\n                   for i in range(len(test_subset))\n                   if predictions[i] == test_subset[\"label\"][i]]\navg_error_length = np.mean(error_lengths) if error_lengths else 0\navg_correct_length = np.mean(correct_lengths) if correct_lengths else 0\nprint(f\"\\nAverage length of ERROR reviews: {avg_error_length:.1f} words\")\nprint(f\"Average length of CORRECT reviews: {avg_correct_length:.1f} words\")\nif avg_error_length < avg_correct_length:\n    print(f\" Observation: Errors tend to be SHORTER\")\nelif avg_error_length > avg_correct_length:\n    print(f\" Observation: Errors tend to be LONGER\")\nelse:\n    print(f\" Observation: No clear length pattern\")\n# Test edge cases\nprint()\nprint(\"TESTING EDGE CASES\")\nedge_cases = [\n    (\"Sarcastic\", \"Oh great, another masterpiece. NOT!\", 0),\n    (\"Mixed\", \"The acting was great but the plot was terrible\", 0),\n    (\"Backhanded\", \"Not as bad as I expected\", 1),\n    (\"Double negative\", \"Not unwatchable\", 1),\n    (\"Very short\", \"Boring\", 0),\n    (\"Ambiguous\", \"It was a movie\", 0),\n]\n# TODO: After analyzing above errors, add your own test cases:\n# edge_cases.extend([\n#     (\"Your category\", \"Your test review here\", expected_label_0_or_1),\n#     (\"Another category\", \"Another test review\", expected_label),\n# ])\nprint(\"\\nTesting challenging cases that often fail:\")\nedge_embeddings = model.encode([text for _, text, _ in edge_cases])\nedge_predictions = clf.predict(edge_embeddings)\nedge_probs = clf.predict_proba(edge_embeddings)\ncorrect_count = 0\nfor i, (category, text, true_label) in enumerate(edge_cases):\n    pred = edge_predictions[i]\n    conf = edge_probs[i][pred]\n    correct = pred == true_label\n    if correct:\n        correct_count += 1\n    true_sent = \"Positive\" if true_label == 1 else \"Negative\"\n    pred_sent = \"Positive\" if pred == 1 else \"Negative\"\n    print(f\"\\n{category}: '{text}'\")\n    print(f\"  True: {true_sent} | Predicted: {pred_sent} | Confidence: {conf:.3f}\")\n    print(f\"  Result: {' CORRECT' if correct else ' WRONG'}\")\nedge_accuracy = correct_count / len(edge_cases)\nprint(f\"\\n\" + \"-\"*80)\nprint(f\"Edge Case Accuracy: {correct_count}/{len(edge_cases)} ({edge_accuracy:.1%})\")\nprint(f\"Regular Test Accuracy: {accuracy:.1%}\")\nprint(f\"Difference: {accuracy - edge_accuracy:+.1%}\")\n# Summary and insights\nprint()\nprint(\"KEY INSIGHTS FROM ERROR ANALYSIS\")\nprint(\"\\n1. Error Distribution:\")\nprint(f\"   - False Positives (predicted too optimistic): {len(false_positives)}\")\nprint(f\"   - False Negatives (predicted too pessimistic): {len(false_negatives)}\")\nif len(false_positives) > len(false_negatives):\n    print(f\"    Classifier has POSITIVE BIAS\")\nelif len(false_negatives) > len(false_positives):\n    print(f\"    Classifier has NEGATIVE BIAS\")\nprint(\"\\n2. Challenging Cases:\")\nfailing_categories = [cat for cat, text, true in edge_cases\n                     if clf.predict(model.encode([text]))[0] != true]\nif failing_categories:\n    print(f\"   The classifier struggles with: {', '.join(failing_categories)}\")\nprint(\"\\n3. Confidence Analysis:\")\nif high_conf_errors:\n    print(f\"   Found {len(high_conf_errors)} high-confidence errors\")\n    print(f\"    The model is 'confidently wrong' on some cases\")\nprint()\nprint(\"TODO: Based on your error analysis, propose improvements:\")\nprint(\"# Write your observations here:\")\nprint(\"# 1. What patterns did you notice in the errors?\")\nprint(\"# 2. Which edge cases failed most?\")\nprint(\"# 3. How would you improve the classifier?\")\nprint(\"#    - Better training data?\")\nprint(\"#    - Different features?\")\nprint(\"#    - Ensemble approach?\")\nprint(\"#    - Confidence thresholds?\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sfz2qOmPGHWp",
    "outputId": "341250f6-c025-4e7a-ded8-de541c0ebc44"
   },
   "execution_count": 33,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training classifier on 1000 movie reviews...\n",
      "\n",
      "================================================================================\n",
      "ERROR ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Overall Performance:\n",
      "  Correct: 174/200 (87.0%)\n",
      "  Errors:  26/200 (13.0%)\n",
      "\n",
      "Error Types:\n",
      "  False Positives: 11 (predicted positive, actually negative)\n",
      "  False Negatives: 15 (predicted negative, actually positive)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "HIGH-CONFIDENCE ERRORS (confidence > 0.7)\n",
      "These are the most surprising mistakes:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. 'an uneasy mix of run-of-the-mill raunchy humor and seemingly sincere personal reflection .'\n",
      "   True: Negative | Predicted: Positive | Confidence: 0.701\n",
      "   Length: 13 words\n",
      "\n",
      "2. 'the stunt work is top-notch ; the dialogue and drama often food-spittingly funny .'\n",
      "   True: Negative | Predicted: Positive | Confidence: 0.867\n",
      "   Length: 14 words\n",
      "\n",
      "3. 'goldmember is funny enough to justify the embarrassment of bringing a barf bag to the moviehouse .'\n",
      "   True: Positive | Predicted: Negative | Confidence: 0.710\n",
      "   Length: 17 words\n",
      "\n",
      "4. 'steven soderbergh doesn't remake andrei tarkovsky's solaris so much as distill it .'\n",
      "   True: Positive | Predicted: Negative | Confidence: 0.730\n",
      "   Length: 13 words\n",
      "\n",
      "5. '\" what really happened ? \" is a question for philosophers , not filmmakers ; all the filmmakers need to do is engage an audience .'\n",
      "   True: Positive | Predicted: Negative | Confidence: 0.717\n",
      "   Length: 26 words\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "ERROR ANALYSIS BY TEXT LENGTH\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Average length of ERROR reviews: 19.8 words\n",
      "Average length of CORRECT reviews: 21.4 words\n",
      "→ Observation: Errors tend to be SHORTER\n",
      "\n",
      "================================================================================\n",
      "TESTING EDGE CASES\n",
      "================================================================================\n",
      "\n",
      "Testing challenging cases that often fail:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sarcastic: 'Oh great, another masterpiece. NOT!'\n",
      "  True: Negative | Predicted: Positive | Confidence: 0.648\n",
      "  Result: ✗ WRONG\n",
      "\n",
      "Mixed: 'The acting was great but the plot was terrible'\n",
      "  True: Negative | Predicted: Negative | Confidence: 0.920\n",
      "  Result: ✓ CORRECT\n",
      "\n",
      "Backhanded: 'Not as bad as I expected'\n",
      "  True: Positive | Predicted: Negative | Confidence: 0.809\n",
      "  Result: ✗ WRONG\n",
      "\n",
      "Double negative: 'Not unwatchable'\n",
      "  True: Positive | Predicted: Negative | Confidence: 0.902\n",
      "  Result: ✗ WRONG\n",
      "\n",
      "Very short: 'Boring'\n",
      "  True: Negative | Predicted: Negative | Confidence: 0.949\n",
      "  Result: ✓ CORRECT\n",
      "\n",
      "Ambiguous: 'It was a movie'\n",
      "  True: Negative | Predicted: Negative | Confidence: 0.709\n",
      "  Result: ✓ CORRECT\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Edge Case Accuracy: 3/6 (50.0%)\n",
      "Regular Test Accuracy: 87.0%\n",
      "Difference: +37.0%\n",
      "\n",
      "================================================================================\n",
      "KEY INSIGHTS FROM ERROR ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "1. Error Distribution:\n",
      "   - False Positives (predicted too optimistic): 11\n",
      "   - False Negatives (predicted too pessimistic): 15\n",
      "   → Classifier has NEGATIVE BIAS\n",
      "\n",
      "2. Challenging Cases:\n",
      "   The classifier struggles with: Sarcastic, Backhanded, Double negative\n",
      "\n",
      "3. Confidence Analysis:\n",
      "   Found 11 high-confidence errors\n",
      "   → The model is 'confidently wrong' on some cases\n",
      "\n",
      "================================================================================\n",
      "TODO: Based on your error analysis, propose improvements:\n",
      "================================================================================\n",
      "# Write your observations here:\n",
      "# 1. What patterns did you notice in the errors?\n",
      "# 2. Which edge cases failed most?\n",
      "# 3. How would you improve the classifier?\n",
      "#    - Better training data?\n",
      "#    - Different features?\n",
      "#    - Ensemble approach?\n",
      "#    - Confidence thresholds?\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Questions\n\n1. What do high-confidence errors have in common?\n\n2. Do errors tend to be shorter or longer than correct predictions?\n\n3. Which edge cases failed most - sarcasm, mixed sentiment, or double negatives?\n"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}