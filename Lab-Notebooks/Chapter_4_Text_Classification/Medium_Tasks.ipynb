{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IyffKxPbyhq"
   },
   "source": [
    "# Chapter 4: Text Classification - Medium Tasks\n",
    "\n",
    "This notebook focuses on building practical text classifiers. You'll create custom multi-class sentiment classifiers, evaluate performance with limited training data, implement confidence-based classification with uncertainty handling, and perform systematic failure analysis. These skills are crucial for real-world NLP applications where data and perfect accuracy are often limited.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOxEdnfDbyhu"
   },
   "source": [
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "Run all cells in this section to set up the environment and load necessary data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LGW2SD-c864"
   },
   "source": [
    "### [OPTIONAL] - Installing Packages on <img src=\"https://colab.google/static/images/icons/colab.png\" width=100>\n",
    "\n",
    "\n",
    "If you are viewing this notebook on Google Colab (or any other cloud vendor), you need to **uncomment and run** the following codeblock to install the dependencies for this chapter:\n",
    "\n",
    "---\n",
    "\n",
    " **NOTE**: We will want to use a GPU to run the examples in this notebook. In Google Colab, go to\n",
    "**Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "N-PxmOIhc865"
   },
   "outputs": [],
   "source": [
    " %%capture\n",
    "!pip install transformers sentence-transformers openai\n",
    "!pip install -U datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APb1rVC1byhy"
   },
   "source": [
    "### Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5phRS_z2U_3T",
    "outputId": "f830d12c-08ab-4537-a6c2-c86d1c1271ff"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 8530\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# Load our data\n",
    "data = load_dataset(\"rotten_tomatoes\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30nUV21qbyh0"
   },
   "source": [
    "### Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "X0KyKHtqyjn3"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def evaluate_performance(y_true, y_pred):\n",
    "    \"\"\"Create and print the classification report\"\"\"\n",
    "    performance = classification_report(\n",
    "        y_true, y_pred,\n",
    "        target_names=[\"Negative Review\", \"Positive Review\"]\n",
    "    )\n",
    "    print(performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Your Turn - Text Classification Experiments\n",
    "\n",
    "Run each task first to see the baseline results. Follow the instructions to modify and experiment."
   ],
   "metadata": {
    "id": "NKYNfoaVC4hU"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VlVd1GVpbyh5"
   },
   "source": [
    "---\n",
    "\n",
    "## Medium Tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Medium Tasks - Building Real Classifiers\n",
    "\n",
    "These tasks require more modification and experimentation. You'll build complete classification systems."
   ],
   "metadata": {
    "id": "2ipz0pJYEne6"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_uLZBrWMbyh8"
   },
   "source": [
    "Run the code to see how 5-level classification works. Then try adding a 6th category.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Medium Task 1: Multi-Class Sentiment Classification\n",
    "In this task, you'll build a sentiment classifier with 5 different categories (from extremely negative to extremely positive) instead of just binary positive/negative.\n",
    "**What to do:**\n",
    "1. Run the cells below to see baseline 5-level classification\n",
    "2. Observe which reviews are uncertain (low margin between top predictions)\n",
    "3. Try uncommenting the 6-level version to add more granularity\n",
    "4. Compare how predictions change with more categories"
   ],
   "metadata": {
    "id": "3CZEijHVcMGI"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9PXpGfijbyh-"
   },
   "source": [
    "Set up the 5 sentiment categories and compute embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "7ceixsfybyh-"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3B5rf4Dbyh_"
   },
   "source": [
    "Classify each review and show confidence scores:"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Define 5 sentiment categories\n",
    "sentiment_labels = [\n",
    "    \"extremely negative review\",\n",
    "    \"somewhat negative review\",\n",
    "    \"neutral review\",\n",
    "    \"somewhat positive review\",\n",
    "    \"extremely positive review\"\n",
    "]\n",
    "\n",
    "# Create embeddings for each category\n",
    "label_embeddings = model.encode(sentiment_labels)\n",
    "\n",
    "print(\"Sentiment categories:\")\n",
    "for i, label in enumerate(sentiment_labels):\n",
    "    print(f\"  {i}: {label}\")"
   ],
   "metadata": {
    "id": "x7dk3IHDbyh_",
    "outputId": "440fde98-efc5-4128-ea8a-f151164f528d",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 32,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sentiment categories:\n",
      "  0: extremely negative review\n",
      "  1: somewhat negative review\n",
      "  2: neutral review\n",
      "  3: somewhat positive review\n",
      "  4: extremely positive review\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Test on some sample reviews\n",
    "test_reviews = [\n",
    "    \"This is the best movie I have ever seen! Absolute masterpiece!\",\n",
    "    \"Pretty good film, I enjoyed it\",\n",
    "    \"It was okay, nothing special\",\n",
    "    \"Not very good, quite boring\",\n",
    "    \"Terrible movie, waste of time\"\n",
    "]\n",
    "\n",
    "review_embeddings = model.encode(test_reviews)\n",
    "similarities = cosine_similarity(review_embeddings, label_embeddings)\n",
    "\n",
    "print(\"Classification results:\")\n",
    "for i, review in enumerate(test_reviews):\n",
    "    predicted_category = sentiment_labels[similarities[i].argmax()]\n",
    "    confidence = similarities[i].max()\n",
    "    top_two_scores = sorted(similarities[i], reverse=True)[:2]\n",
    "    margin = top_two_scores[0] - top_two_scores[1]\n",
    "\n",
    "    print(f\"\\n'{review}'\")\n",
    "    print(f\"  \u2192 {predicted_category}\")\n",
    "    print(f\"  Confidence: {confidence:.3f} | Margin: {margin:.3f}\")"
   ],
   "metadata": {
    "id": "SlF4O-VVbyiA",
    "outputId": "727ce0fb-4e07-4602-eb98-ec91388598d3",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 33,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classification results:\n",
      "\n",
      "'This is the best movie I have ever seen! Absolute masterpiece!'\n",
      "  \u2192 extremely positive review\n",
      "  Confidence: 0.256 | Margin: 0.099\n",
      "\n",
      "'Pretty good film, I enjoyed it'\n",
      "  \u2192 somewhat positive review\n",
      "  Confidence: 0.428 | Margin: 0.073\n",
      "\n",
      "'It was okay, nothing special'\n",
      "  \u2192 somewhat negative review\n",
      "  Confidence: 0.439 | Margin: 0.004\n",
      "\n",
      "'Not very good, quite boring'\n",
      "  \u2192 somewhat positive review\n",
      "  Confidence: 0.378 | Margin: 0.000\n",
      "\n",
      "'Terrible movie, waste of time'\n",
      "  \u2192 extremely negative review\n",
      "  Confidence: 0.426 | Margin: 0.009\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "ZPl8Fu08byiB",
    "outputId": "05dcad22-5e72-49cb-d028-0a83165884d3",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "CATEGORY CONFUSION ANALYSIS\n",
      "Category Pair                                                Similarity  \n",
      " extremely negative review <-> somewhat negative review       0.952\n",
      " somewhat negative review <-> somewhat positive review        0.863\n",
      " somewhat positive review <-> extremely positive review       0.818\n",
      " somewhat negative review <-> neutral review                  0.794\n",
      " extremely negative review <-> somewhat positive review       0.766\n",
      " extremely negative review <-> neutral review                 0.748\n",
      " neutral review <-> somewhat positive review                  0.748\n",
      "somewhat negative review <-> extremely positive review       0.674\n",
      "extremely negative review <-> extremely positive review      0.664\n",
      "neutral review <-> extremely positive review                 0.616\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(\"CATEGORY CONFUSION ANALYSIS\")\n",
    "label_similarity = cosine_similarity(label_embeddings)\n",
    "print(f\"{'Category Pair':<60s} {'Similarity':<12s}\")\n",
    "confusions = []\n",
    "for i in range(len(sentiment_labels)):\n",
    "    for j in range(i+1, len(sentiment_labels)):\n",
    "        sim = label_similarity[i][j]\n",
    "        confusions.append((i, j, sim))\n",
    "for i, j, sim in sorted(confusions, key=lambda x: x[2], reverse=True)[:10]:\n",
    "    pair_name = f\"{sentiment_labels[i]} <-> {sentiment_labels[j]}\"\n",
    "    marker = \" \" if sim > 0.7 else \"\"\n",
    "    print(f\"{marker}{pair_name:<60s} {sim:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, the classifier assigns each review to one of the 5 sentiment categories. The **margin** (difference between top 2 predictions) indicates confidence - large margins (>0.15) mean the model is confident, while small margins (<0.05) indicate uncertainty. Reviews with extreme language (\"best ever\", \"terrible\") have higher confidence, while moderate reviews (\"pretty good\", \"quite bad\") show more uncertainty."
   ],
   "metadata": {
    "id": "fECdQPWjbyiC"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFy4lcn9byiD"
   },
   "source": [
    "Analyze which categories are most similar to each other:"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notice that adjacent categories (like \"somewhat negative\" and \"neutral\") tend to have higher similarity scores, which explains why the model sometimes confuses them. Categories with similarity > 0.7 are particularly prone to confusion."
   ],
   "metadata": {
    "id": "EV2pv7pbbyiD"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zL_xe6AabyiE"
   },
   "source": [
    "#### Medium Task 2: Classifier Performance with Limited Training Data\n",
    "\n",
    "Try different training sizes (100, 500, 1000, 2000) and fill in the results table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "JeFbrbUobyiE",
    "outputId": "d491aaff-4930-4d6f-ee69-525642d2e5e8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "EXPERIMENT: Training Size = 1000\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "data = load_dataset(\"rotten_tomatoes\")\n",
    "# TODO: Try different values: 100, 500, 1000, 2000, 5000\n",
    "train_size = 1000\n",
    "test_size = 300\n",
    "train_subset = data[\"train\"].shuffle(seed=42).select(range(min(train_size, len(data[\"train\"]))))\n",
    "test_subset = data[\"test\"].shuffle(seed=42).select(range(test_size))\n",
    "print(f\"EXPERIMENT: Training Size = {train_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "9_26CEFobyiF",
    "outputId": "4298dfc1-3e57-4231-8c83-833c1cfdc887",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389,
     "referenced_widgets": [
      "a41b343bbc3c4f478e8cd0e99287add4",
      "924fe2f9ac3a41c2b81b1bea355a6c4a",
      "ed701d331e784f4c8c11a07903360998",
      "dca8cdb22b4c4d25b6da390cfc3a082d",
      "12d2ec8204944b7b8af6f4f3286f9a67",
      "19282f7d41c440a5b42a0112a52bb5f7",
      "fd83136c1f6c4593a4b5d0dbe4b8623d",
      "e024590a83204e04b0c65117844f541e",
      "eba24658b8024b3e9e19a458e5c7272f",
      "81e0c5397320443cbbf8cf9ffe023dda",
      "948c201f837d48f492d33393f68e167f",
      "75d2636d4288459cbff8431249e741f6",
      "f0f4afa6d3914c33b08d20e25c37d21f",
      "8ea00df86d4842ba93f21a44c89d3778",
      "da9b3b71b6cd4b68aa28be3a8febdbc0",
      "480d9a4f19a0492390908be23595e32a",
      "be3f059b28ef48f48c027a296f2bad84",
      "970126d6b60c4bea9b6f7574135fd53e",
      "0490b3b0629a408f93f13766e6290208",
      "e9d18559b4474378b5ba8c9c0fe931a6",
      "ed1cabea5903457ba93c3a0fcc0750b3",
      "a47f09c0913c432f932716d55130a6af",
      "c429b59b3a4f4ab58b970601a062b303",
      "4f5ad27e35ce4e95b4119a55c8332f30",
      "74f668168fd247fb85dd0d87eacfebd5",
      "4467d56553e1427695abb950858de8dc",
      "8a4c52bf6e224b53a7b8d9a706cfc5ad",
      "4fce251b76174c6da0cbf340db156447",
      "0b84fd7f88d4441ba234a85ff8ba8262",
      "a5a761c1a77d400db5fb84cdc17a6860",
      "9f55be6e126443ae8720e707820b517e",
      "53aa90e254d34884b8410e40d2660637",
      "95b72680cc0b42329560dfa64f1a0317",
      "9f10ef7962ea48ac9552cf31fedfbf83",
      "1a176c1be9e64261b34b5cd0ea6215c8",
      "807a9f29466743b890b33d19279fe713",
      "2737920ba9c644abbef58a1ade9b70b2",
      "078f5327965f4b5fb5fc59b775c0faa1",
      "347e7e0a52ca4d948b8e0e8248cd1592",
      "48bc178ccdb446329a075d50bef1bead",
      "eb13856074c1431f9367ed111c1f202d",
      "1969c33c743f42aa8477e739328b9578",
      "99607d6a12e645428a8f1dc863e2d6b4",
      "f10402210c3244e9bae797a13a3af380",
      "69c4a8e8ab6b49f382c830c7e46d2b9a",
      "764a30da7a3a41988e8341ed44cda5ff",
      "d6703a2b4df9452faa034bd6d17d5bcc",
      "93c258267a0b477a816fc63d694a40df",
      "59943dc1bfc74b10a5dda50c608ac439",
      "4e0e7a9305b04eaab2291ad98657dff7",
      "e99b157f33094e5a94f79dfb8fb83cc5",
      "7984edbac1a3424aa9a2dc08c49cffcf",
      "fe8fa50aacb14138b8bde246435a6183",
      "f3f9523c2d37443d9fdaa6f29296efb9",
      "958feedf13414922be7049a282079d92",
      "7f38b1e57a8d423597e7979da556f204",
      "577fb6f9b1914533ab4fa7fc325f22ba",
      "a62db26823bb40a0a674898adbe892dd",
      "11364dbb978446e3b3e254f365a9b3bd",
      "43a922431f6b4c3d8123bf81a4d1fd9b",
      "eb94b3ffa2b546f284e96bfd6579eba3",
      "6f3c0f9fbca448119a1bf59b6b76bee4",
      "b12e68aad27e47c59c55f7d5a8969369",
      "c24e736c54844bfb82b3fd2a9b881023",
      "4de63ceeea4241ffacc8f33dd7f3220b",
      "ffb21321c2944c75885c136bd12d2f9d"
     ]
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "[1/2] Testing Task-Specific Model...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a41b343bbc3c4f478e8cd0e99287add4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "75d2636d4288459cbff8431249e741f6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c429b59b3a4f4ab58b970601a062b303"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9f10ef7962ea48ac9552cf31fedfbf83"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "69c4a8e8ab6b49f382c830c7e46d2b9a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7f38b1e57a8d423597e7979da556f204"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Device set to use cpu\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " Task-Specific Model F1: 0.7709\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[1/2] Testing Task-Specific Model...\")\n",
    "task_model = pipeline(\n",
    "    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    tokenizer=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    return_all_scores=True,\n",
    "    device=-1\n",
    ")\n",
    "y_pred_task = []\n",
    "for text in test_subset[\"text\"]:\n",
    "    output = task_model(text)[0]\n",
    "    neg_score = output[0][\"score\"]\n",
    "    pos_score = output[2][\"score\"]\n",
    "    y_pred_task.append(1 if pos_score > neg_score else 0)\n",
    "task_f1 = f1_score(test_subset[\"label\"], y_pred_task, average='weighted')\n",
    "print(f\" Task-Specific Model F1: {task_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYLELJ-hbyiF"
   },
   "source": [
    "Train the embedding-based classifier on your labeled data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "q7Jgae8WbyiG",
    "outputId": "7117c5a5-a654-4ba5-dab9-ca9f1e7a81a4",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422,
     "referenced_widgets": [
      "c2a5948a699e4ab6b81421b5e88a115f",
      "6d3060b3fdaf47a0b6edacf68e2968f8",
      "dec6e065eb3f4c2eab6fcd3aa2fddec0",
      "dfebefb8f6c34bbd8c605d6834edf72e",
      "fb06c29e61b54c7dbb691e32c172fac7",
      "43e78e1a1e604a81aa0c01f746fed935",
      "8b0802a3ebb243ba8e45dab25713922f",
      "63e09f2868c84acda03a584b9e47770b",
      "68954509bb7c4402b768f9c1e9ce66af",
      "55fd73c644eb4d01a729253fd01712fa",
      "714d66886a4146af954b53268660c845",
      "20ff85af9fa943ec89866c6f24dcb3b6",
      "07a536fa9d814be9a185aee2ba0711a0",
      "81c9b866bcca4519823100180c022942",
      "fd76507c7398424facf84c3850420b6b",
      "66d897087315472e89e936351891511f",
      "188cad60710c4b6ca0a5d2ea110b8151",
      "390717daf3154079bdc53f314a8bd40e",
      "26ef4c031dac434b8939d34505f02f8c",
      "2817d5d94cc64265b5e83f17d37f3a02",
      "7cfb1226149847ba80fbc88cd4542d5e",
      "49d602a4dff5421f9cde72fb7b866200",
      "d489bdaf593e405da4420a14f7dc0c3a",
      "6f7f853d18e440f98d6953a6f6bb8026",
      "52115d4ec7b24182a90285234558a776",
      "7b08c1b9c0f84fe2942e21df53e2c6a2",
      "3831e38dc6574fb496d7af6ec371218d",
      "f0adafdf33224cc49e09233e8b0f9455",
      "ddfb2a19aa154de79434ea4c4623c96e",
      "4dfa89d2271d4994bb76beb1aa2c52a3",
      "18a7fb61f6f04c56b09de4b6ff236895",
      "bf1ac9f95f664b2f8e33cd82066a2840",
      "8e4d2f540706448aa64f94256b342572",
      "3d2da677346c41e58fbd2eefd044340a",
      "8650f6a787af4b1e85e971d53c7a7578",
      "1a1cda5874e64bc3b977e62cdd26312e",
      "382d1d647f044925b89cb453b242e4de",
      "0877ea4f44674d5295dcb70c0644ee96",
      "77a55c21e36640559fe55fb139513612",
      "c32900ef77dc416a83bff53472ccd218",
      "6a7e09a0fec142339e68c084900d96e8",
      "324ec59f66cc486ca8efbedd2cae1946",
      "2f9452a04de7481abfee6d45e1e2beab",
      "000c23ba4b984086b978d34b78f9a08f",
      "50da0844af184d45833eed96f35c0a0c",
      "3facf8131b8e4668b41bd09fd7a87d0a",
      "87e9fa322ead4a9a82574567d894f741",
      "903183889e144b0d8db06c4989c9e01b",
      "963d4cbebb64407481114bf87ea62441",
      "7e5db19848f541bab8d2e98a1f5a228e",
      "fef7bae9a29b488b833683255ba1fed3",
      "72ff530bd6f64e18980f1ec3ceaa68c8",
      "68b1ef791909457084f99d06ec16455a",
      "fc8d27cd5307447897d70ab74d19cd59",
      "ec3f8ede6cc74881b30bbd38953fb5cc",
      "2c77bf34d4d54f70842a4a0a8b15a682",
      "f35896bf2a034d2d8cb82c3d9b3af071",
      "91d48267bc454fb2a2063cd5cb88fe6a",
      "f0683c42483643cda543cec4387ce43d",
      "2fdb37a34c0f4808854d512c5840cc75",
      "7b86cf58d81a40fcaaa700835180dd31",
      "732730e4ae4448438e1c251ca359b465",
      "af3c71b9dc4b49169e3063c9af44c4a3",
      "4738e39518504438a504b16bf61c3394",
      "849781942d4f44f1977259ca147b80b2",
      "8eeb77c430d64178a73d13e796615195",
      "7aeccf54a233416caa9c5b3bc32cdc40",
      "444e0b7d412847f7a2c3deb907253955",
      "488ab94adbad4dc1886320f40e691395",
      "2ebda5fdd2164ab78d18e8cf23014789",
      "89ef87c01f8a413286e4b4a051128ab0",
      "059e9a15a6614fe8938cb16057cdd014",
      "01ed0f31b0c14f488aeb76b37066ce60",
      "a89e70cbea36417daf21bf6002bef012",
      "724121ab614747c49b275367fa7b4016",
      "e27080a35a944b899b9df038d7639316",
      "3dd2138f0fdf4de98eb73c7e845b01cf",
      "f3045dbbf48846b7a9ec76d9157aa9ad",
      "578c7ffdc5e0408ca880ec8620ecfb44",
      "06a26d00971c4a96ad1899cb587776f0",
      "96257647cfe147d2a762d03dacc3c007",
      "2affd3e4908f449a9b18cb24497b828e",
      "10811ca00d8d47b4a6399cb3de8b635d",
      "d3939e644ba2434da4918b652f4c4ad8",
      "e3e8626e4f1e4d278f04f1098a726611",
      "3ae1be8fb6c54afc8f26d8b516391427",
      "4c83b3ea9f11426ab545d6fec0430187",
      "e2efc1779aa94c42b75020b377fd95c4",
      "e0946076a58440d3ae296a0b86106a76",
      "160ab4c6ce294398a1656372367ff4b2",
      "efb4598bfb43447eaf2c84492cf88eb1",
      "9809097e851041158337c76865538138",
      "e712be60227b41f4a590392bfcf21da2",
      "afe144823be14b739696f951cddbc2f7",
      "edf61db28d6a46bd89134d6f77fc061d",
      "6b13ffb1b1b4415a84e9772e4a702940",
      "a59ed180e6f9430a927ca0e5bcc43dc1",
      "2a42a799cc774f57bf6e2aaae90d680b",
      "e368fa3b8a8048ea8df0701124ee22af",
      "4cf220bd97804000b2a0439114668064",
      "30b9a9df17294acdb97594d8c2c6c0c2",
      "5aafc4bf70304a3f9d15907157e8e04d",
      "ff78ae125e494b9ab6cb025c90f6af2c",
      "b9b2a2ddf1b54279af058ced85c521b5",
      "7c0b9b636d7041e9b8cd23c311332545",
      "789444dcee9c449f8190e7cdc97735c3",
      "bee0b024d72540d3a4fd01f9f550fd14",
      "614523dc07bc4a0c91566c8b53818d9d",
      "f1529b5c1a2c4a51b629ec1d91e7d32b",
      "92de33c3bebd498a8fe69f835c0a042a",
      "39760c3920c046e18ff97c7756b01e01",
      "17d62a3e5a7d41d4b7c0a264e01141ec",
      "90ae8d94b61945418c4f614bcfa1dc2b",
      "a1360b3fe82a45b58181d3cab2de1fae",
      "fe8cd072d0744b80af94fa2a39ed3b5f",
      "17e79394e6e34bc8a847c7c16aec094c",
      "99a44220ced647e2b533cbdaad5d51f8",
      "01b540c441d44e2eb7c83b8163deeb0a",
      "a6e7e637e6584971a5b5863b4582d19f",
      "d0cd59e06a2d4209b00e84f452dc47bd",
      "28cc9fefdb9c4e9280e2972523520c74"
     ]
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "[2/2] Training Embedding Classifier on 1000 samples...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c2a5948a699e4ab6b81421b5e88a115f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "20ff85af9fa943ec89866c6f24dcb3b6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d489bdaf593e405da4420a14f7dc0c3a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3d2da677346c41e58fbd2eefd044340a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "50da0844af184d45833eed96f35c0a0c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2c77bf34d4d54f70842a4a0a8b15a682"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7aeccf54a233416caa9c5b3bc32cdc40"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f3045dbbf48846b7a9ec76d9157aa9ad"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e0946076a58440d3ae296a0b86106a76"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4cf220bd97804000b2a0439114668064"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "39760c3920c046e18ff97c7756b01e01"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " Embedding Classifier F1: 0.8699\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n[2/2] Training Embedding Classifier on {train_size} samples...\")\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "train_embeddings = embedding_model.encode(train_subset[\"text\"], show_progress_bar=False)\n",
    "test_embeddings = embedding_model.encode(test_subset[\"text\"], show_progress_bar=False)\n",
    "clf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "clf.fit(train_embeddings, train_subset[\"label\"])\n",
    "y_pred_embed = clf.predict(test_embeddings)\n",
    "embed_f1 = f1_score(test_subset[\"label\"], y_pred_embed, average='weighted')\n",
    "print(f\" Embedding Classifier F1: {embed_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acpiODYAbyiH"
   },
   "source": [
    "Compare the two approaches and show example predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "KKZ7v0nvbyiH",
    "outputId": "96cc0e0e-f7a1-4cf6-8249-48b14cd996c8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "RESULTS SUMMARY\n",
      "Training samples used: 1000\n",
      "\n",
      "Task-Specific (pre-trained):  F1 = 0.7709\n",
      "Embedding + Classifier:       F1 = 0.8699\n",
      "Difference:                       +0.0990\n",
      "\n",
      " Embedding approach WINS with 1000 samples!\n",
      "\n",
      "EXAMPLE PREDICTIONS (first 5)\n",
      "\n",
      "1. 'unpretentious , charming , quirky , original...'\n",
      "   True: Positive\n",
      "   Task-Specific: Positive \n",
      "   Embedding:     Positive \n",
      "\n",
      "2. 'a film really has to be exceptional to justify a three hour ...'\n",
      "   True: Negative\n",
      "   Task-Specific: Negative \n",
      "   Embedding:     Negative \n",
      "\n",
      "3. 'working from a surprisingly sensitive script co-written by g...'\n",
      "   True: Positive\n",
      "   Task-Specific: Positive \n",
      "   Embedding:     Positive \n",
      "\n",
      "4. 'it may not be particularly innovative , but the film's crisp...'\n",
      "   True: Positive\n",
      "   Task-Specific: Positive \n",
      "   Embedding:     Positive \n",
      "\n",
      "5. 'such a premise is ripe for all manner of lunacy , but kaufma...'\n",
      "   True: Negative\n",
      "   Task-Specific: Negative \n",
      "   Embedding:     Negative \n",
      "\n",
      "TODO: Record your results\n",
      "Current: | 1000       | 0.7709  | 0.8699       | Embed       |\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(f\"Training samples used: {train_size}\")\n",
    "print(f\"\\nTask-Specific (pre-trained):  F1 = {task_f1:.4f}\")\n",
    "print(f\"Embedding + Classifier:       F1 = {embed_f1:.4f}\")\n",
    "print(f\"Difference:                       {embed_f1 - task_f1:+.4f}\")\n",
    "if embed_f1 > task_f1:\n",
    "    print(f\"\\n Embedding approach WINS with {train_size} samples!\")\n",
    "elif embed_f1 > task_f1 - 0.01:\n",
    "    print(f\"\\n Essentially TIED\")\n",
    "else:\n",
    "    print(f\"\\n Task-specific model wins\")\n",
    "# Show example predictions\n",
    "print()\n",
    "print(\"EXAMPLE PREDICTIONS (first 5)\")\n",
    "for i in range(5):\n",
    "    true_label = \"Positive\" if test_subset[\"label\"][i] == 1 else \"Negative\"\n",
    "    task_pred = \"Positive\" if y_pred_task[i] == 1 else \"Negative\"\n",
    "    embed_pred = \"Positive\" if y_pred_embed[i] == 1 else \"Negative\"\n",
    "    task_correct = \"\" if y_pred_task[i] == test_subset[\"label\"][i] else \"\"\n",
    "    embed_correct = \"\" if y_pred_embed[i] == test_subset[\"label\"][i] else \"\"\n",
    "    print(f\"\\n{i+1}. '{test_subset['text'][i][:60]}...'\")\n",
    "    print(f\"   True: {true_label}\")\n",
    "    print(f\"   Task-Specific: {task_pred} {task_correct}\")\n",
    "    print(f\"   Embedding:     {embed_pred} {embed_correct}\")\n",
    "print()\n",
    "print(\"TODO: Record your results\")\n",
    "print(f\"Current: | {train_size:<10} | {task_f1:.4f}  | {embed_f1:.4f}       | {'Embed' if embed_f1 > task_f1 else 'Task':<11} |\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCWBW3qObyiH"
   },
   "source": [
    "### Questions\n",
    "\n",
    "1. At what training size did embedding classifier match the task-specific model?\n",
    "\n",
    "2. Were there cases where one model was correct and the other wrong?\n",
    "\n",
    "3. Is 100 samples enough labeled data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eKfJlPDtbyiI"
   },
   "source": [
    "#### Medium Task 3: Confidence-Based Classifier with Uncertainty Handling\n",
    "\n",
    "In production, refusing a prediction beats making a wrong one. Here's the key insight: when your model is uncertain, it should say \"I don't know\" rather than guessing. This creates a trade-off between coverage (how many predictions you make) and accuracy (how often you're right).\n",
    "\n",
    "Try this:\n",
    "- Run with threshold of 0.15 first\n",
    "- Test 0.05, 0.30, and 0.50 to see how the trade-off shifts\n",
    "- Check the uncertain cases (typically have hedging language)\n",
    "- Experiment with the alternative uncertainty measure"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ],
   "metadata": {
    "id": "UrlfyjrabyiJ"
   },
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# Reviews with varying levels of clarity\n",
    "test_reviews = [\n",
    "    \"Absolutely fantastic! Best movie ever!\",           # Clear positive\n",
    "    \"Pretty good, I liked it\",                          # Weak positive\n",
    "    \"It was fine, nothing special\",                     # Ambiguous\n",
    "    \"Not bad but not great either\",                     # Very ambiguous\n",
    "    \"Quite disappointing\",                              # Weak negative\n",
    "    \"Terrible! Complete waste of time!\",                # Clear negative\n",
    "    \"The movie had some interesting moments\",           # Ambiguous positive\n",
    "    \"Outstanding performances all around!\",             # Clear positive\n",
    "]\n",
    "\n",
    "# True labels (for evaluation)\n",
    "y_true = [1, 1, 0, 0, 0, 0, 1, 1]  # 1=positive, 0=negative"
   ],
   "metadata": {
    "id": "9sf1K6sibyiJ"
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "labels = [\"A negative movie review\", \"A positive movie review\"]\n",
    "\n",
    "# TODO: EXPERIMENT WITH THIS - Try: 0.05, 0.15, 0.30, 0.50\n",
    "confidence_threshold = 0.15"
   ],
   "metadata": {
    "id": "jLZjsRNVbyiK"
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def calculate_margin(similarities):\n",
    "    \"\"\"\n",
    "    Margin = difference between top two predictions\n",
    "    Small margin = uncertain (predictions are close)\n",
    "    \"\"\"\n",
    "    sorted_sims = np.sort(similarities)[::-1]\n",
    "    margin = sorted_sims[0] - sorted_sims[1]\n",
    "    return margin\n",
    "\n",
    "# TODO: After first run, uncomment this alternative uncertainty measure:\n",
    "# def calculate_margin(similarities):\n",
    "#     \"\"\"\n",
    "#     Alternative: Use absolute confidence in top prediction\n",
    "#     Low confidence = uncertain\n",
    "#     \"\"\"\n",
    "#     max_confidence = np.max(similarities)\n",
    "#     # Convert to margin-like score (higher = more certain)\n",
    "#     # If max is 0.6, margin = 0.6 - 0.5 = 0.1 (uncertain)\n",
    "#     # If max is 0.9, margin = 0.9 - 0.5 = 0.4 (certain)\n",
    "#     margin = max_confidence - 0.5\n",
    "#     return margin"
   ],
   "metadata": {
    "id": "Ss-J6mVsbyiL"
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "label_embeddings = model.encode(labels)\n",
    "review_embeddings = model.encode(test_reviews)\n",
    "sim_matrix = cosine_similarity(review_embeddings, label_embeddings)"
   ],
   "metadata": {
    "id": "htw33fKTbyiL"
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Classify with confidence threshold\n",
    "results = []\n",
    "predictions = []\n",
    "confidences = []\n",
    "\n",
    "print(f\"CONFIDENCE-BASED CLASSIFICATION (threshold={confidence_threshold})\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, review in enumerate(test_reviews):\n",
    "    similarities = sim_matrix[i]\n",
    "    predicted_idx = np.argmax(similarities)\n",
    "    top_confidence = similarities[predicted_idx]\n",
    "    margin = calculate_margin(similarities)\n",
    "\n",
    "    # Decision: predict only if confident enough\n",
    "    if margin >= confidence_threshold:\n",
    "        prediction = predicted_idx\n",
    "        status = \"PREDICTED\"\n",
    "        predictions.append(prediction)\n",
    "    else:\n",
    "        prediction = None\n",
    "        status = \"UNCERTAIN\"\n",
    "        predictions.append(None)\n",
    "\n",
    "    true_label = \"Positive\" if y_true[i] == 1 else \"Negative\"\n",
    "    pred_label = labels[predicted_idx] if prediction is not None else \"UNCERTAIN\"\n",
    "\n",
    "    print(f\"\\n{i+1}. '{review}'\")\n",
    "    print(f\"   True label: {true_label}\")\n",
    "    print(f\"   Prediction: {pred_label}\")\n",
    "    print(f\"   Top confidence: {top_confidence:.3f}\")\n",
    "    print(f\"   Margin: {margin:.3f} {'\u2713 Above threshold' if margin >= confidence_threshold else '\u2717 Below threshold'}\")\n",
    "    print(f\"   Status: {status}\", end=\"\")\n",
    "\n",
    "    if prediction is not None:\n",
    "        correct = prediction == y_true[i]\n",
    "        print(f\" - {'\u2713 CORRECT' if correct else '\u2717 INCORRECT'}\")\n",
    "    else:\n",
    "        print()\n",
    "\n",
    "    results.append({\n",
    "        'review': review,\n",
    "        'true': y_true[i],\n",
    "        'pred': prediction,\n",
    "        'margin': margin,\n",
    "        'status': status\n",
    "    })"
   ],
   "metadata": {
    "id": "oDGa7ACXbyiL",
    "outputId": "e446c124-e294-4a6d-f299-72cf0582d5bf",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 24,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CONFIDENCE-BASED CLASSIFICATION (threshold=0.15)\n",
      "================================================================================\n",
      "\n",
      "1. 'Absolutely fantastic! Best movie ever!'\n",
      "   True label: Positive\n",
      "   Prediction: UNCERTAIN\n",
      "   Top confidence: 0.451\n",
      "   Margin: 0.092 \u2717 Below threshold\n",
      "   Status: UNCERTAIN\n",
      "\n",
      "2. 'Pretty good, I liked it'\n",
      "   True label: Positive\n",
      "   Prediction: UNCERTAIN\n",
      "   Top confidence: 0.410\n",
      "   Margin: 0.018 \u2717 Below threshold\n",
      "   Status: UNCERTAIN\n",
      "\n",
      "3. 'It was fine, nothing special'\n",
      "   True label: Negative\n",
      "   Prediction: UNCERTAIN\n",
      "   Top confidence: 0.418\n",
      "   Margin: 0.051 \u2717 Below threshold\n",
      "   Status: UNCERTAIN\n",
      "\n",
      "4. 'Not bad but not great either'\n",
      "   True label: Negative\n",
      "   Prediction: UNCERTAIN\n",
      "   Top confidence: 0.414\n",
      "   Margin: 0.053 \u2717 Below threshold\n",
      "   Status: UNCERTAIN\n",
      "\n",
      "5. 'Quite disappointing'\n",
      "   True label: Negative\n",
      "   Prediction: UNCERTAIN\n",
      "   Top confidence: 0.354\n",
      "   Margin: 0.080 \u2717 Below threshold\n",
      "   Status: UNCERTAIN\n",
      "\n",
      "6. 'Terrible! Complete waste of time!'\n",
      "   True label: Negative\n",
      "   Prediction: UNCERTAIN\n",
      "   Top confidence: 0.397\n",
      "   Margin: 0.121 \u2717 Below threshold\n",
      "   Status: UNCERTAIN\n",
      "\n",
      "7. 'The movie had some interesting moments'\n",
      "   True label: Positive\n",
      "   Prediction: UNCERTAIN\n",
      "   Top confidence: 0.506\n",
      "   Margin: 0.118 \u2717 Below threshold\n",
      "   Status: UNCERTAIN\n",
      "\n",
      "8. 'Outstanding performances all around!'\n",
      "   True label: Positive\n",
      "   Prediction: UNCERTAIN\n",
      "   Top confidence: 0.282\n",
      "   Margin: 0.098 \u2717 Below threshold\n",
      "   Status: UNCERTAIN\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Calculate metrics\n",
    "made_predictions = [r for r in results if r['pred'] is not None]\n",
    "uncertain_cases = [r for r in results if r['pred'] is None]\n",
    "correct_predictions = [r for r in made_predictions if r['pred'] == r['true']]\n",
    "\n",
    "total = len(results)\n",
    "n_predicted = len(made_predictions)\n",
    "n_uncertain = len(uncertain_cases)\n",
    "n_correct = len(correct_predictions)\n",
    "\n",
    "coverage = n_predicted / total\n",
    "accuracy = n_correct / n_predicted if n_predicted > 0 else 0\n",
    "\n",
    "print()\n",
    "print(\"=\"*80)\n",
    "print(\"PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nCoverage: {n_predicted}/{total} = {coverage:.1%}\")\n",
    "print(f\"  \u2192 Made predictions for {n_predicted} reviews\")\n",
    "print(f\"  \u2192 Refused to predict on {n_uncertain} reviews\")\n",
    "\n",
    "print(f\"\\nAccuracy (on predictions made): {n_correct}/{n_predicted} = {accuracy:.1%}\")\n",
    "print(f\"  \u2192 Of the {n_predicted} predictions, {n_correct} were correct\")\n",
    "\n",
    "print(f\"\\nTrade-off Analysis:\")\n",
    "print(f\"  Threshold = {confidence_threshold}\")\n",
    "print(f\"  \u2192 Higher threshold = fewer predictions but higher accuracy\")\n",
    "print(f\"  \u2192 Lower threshold = more predictions but lower accuracy\")"
   ],
   "metadata": {
    "id": "Oc38LloRbyiM",
    "outputId": "d378ffd3-3b2e-477c-d415-31edb19d0343",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 25,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "================================================================================\n",
      "PERFORMANCE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Coverage: 0/8 = 0.0%\n",
      "  \u2192 Made predictions for 0 reviews\n",
      "  \u2192 Refused to predict on 8 reviews\n",
      "\n",
      "Accuracy (on predictions made): 0/0 = 0.0%\n",
      "  \u2192 Of the 0 predictions, 0 were correct\n",
      "\n",
      "Trade-off Analysis:\n",
      "  Threshold = 0.15\n",
      "  \u2192 Higher threshold = fewer predictions but higher accuracy\n",
      "  \u2192 Lower threshold = more predictions but lower accuracy\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Questions\n",
    "\n",
    "1. What do uncertain reviews have in common? Are they using hedging language like \"kind of\" or \"somewhat\"?\n",
    "\n",
    "2. Compare results at threshold=0.05 vs 0.30. Describe the coverage vs accuracy trade-off. When would you want high coverage vs high accuracy?\n",
    "\n",
    "3. How could you use confidence-based prediction in production? What should a system do when the model is uncertain?"
   ],
   "metadata": {
    "id": "H7-OE7H9F6vx"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDEBU0cLbyiN"
   },
   "source": [
    "#### Medium Task 4: Classifier Failure Analysis\n",
    "\n",
    "Train the classifier and see what kinds of reviews it gets wrong. Then add your own test cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2UDR7v_0byiN"
   },
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datasets import load_dataset\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgPRrIxLbyiO"
   },
   "source": [
    "Load data and train a classifier."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mkW9_JNLbyiO",
    "outputId": "78ccf26c-e42c-4408-9e5f-52bc9ce54c34",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "data = load_dataset(\"rotten_tomatoes\")\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "train_subset = data[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "test_subset = data[\"test\"].shuffle(seed=42).select(range(200))\n",
    "\n",
    "print(\"Training classifier...\")\n",
    "train_embeddings = model.encode(train_subset[\"text\"], show_progress_bar=False)\n",
    "test_embeddings = model.encode(test_subset[\"text\"], show_progress_bar=False)\n",
    "\n",
    "clf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "clf.fit(train_embeddings, train_subset[\"label\"])\n",
    "\n",
    "predictions = clf.predict(test_embeddings)\n",
    "probabilities = clf.predict_proba(test_embeddings)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training classifier...\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wm3p9MfHbyiO"
   },
   "source": [
    "Analyze the errors."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PpSZkDVobyiP",
    "outputId": "92f4c8a6-13d5-46f5-d1e9-db3695151b04",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "errors = []\n",
    "for i in range(len(test_subset)):\n",
    "    if predictions[i] != test_subset[\"label\"][i]:\n",
    "        confidence = probabilities[i][predictions[i]]\n",
    "        errors.append({\n",
    "            'index': i,\n",
    "            'text': test_subset[\"text\"][i],\n",
    "            'true_label': test_subset[\"label\"][i],\n",
    "            'predicted_label': predictions[i],\n",
    "            'confidence': confidence,\n",
    "            'length': len(test_subset[\"text\"][i].split())\n",
    "        })\n",
    "\n",
    "total_errors = len(errors)\n",
    "total_samples = len(test_subset)\n",
    "accuracy = (total_samples - total_errors) / total_samples\n",
    "\n",
    "print(f\"Overall: {total_samples - total_errors}/{total_samples} correct ({accuracy:.1%})\")\n",
    "print(f\"Errors: {total_errors}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overall: 174/200 correct (87.0%)\n",
      "Errors: 26\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LOX4A0LtbyiP"
   },
   "source": [
    "Look at high-confidence errors (the most surprising mistakes)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kY0cgAP-byiP",
    "outputId": "c835ffab-c91f-4808-96dd-af2f0fdd16b3",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "high_conf_errors = [e for e in errors if e['confidence'] > 0.7]\n",
    "\n",
    "print(\"HIGH-CONFIDENCE ERRORS:\")\n",
    "for i, error in enumerate(high_conf_errors[:5]):\n",
    "    true_sent = \"Positive\" if error['true_label'] == 1 else \"Negative\"\n",
    "    pred_sent = \"Positive\" if error['predicted_label'] == 1 else \"Negative\"\n",
    "\n",
    "    print(f\"\\n{i+1}. '{error['text']}'\")\n",
    "    print(f\"   True: {true_sent} | Predicted: {pred_sent}\")\n",
    "    print(f\"   Confidence: {error['confidence']:.3f}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "HIGH-CONFIDENCE ERRORS:\n",
      "\n",
      "1. 'an uneasy mix of run-of-the-mill raunchy humor and seemingly sincere personal reflection .'\n",
      "   True: Negative | Predicted: Positive\n",
      "   Confidence: 0.701\n",
      "\n",
      "2. 'the stunt work is top-notch ; the dialogue and drama often food-spittingly funny .'\n",
      "   True: Negative | Predicted: Positive\n",
      "   Confidence: 0.867\n",
      "\n",
      "3. 'goldmember is funny enough to justify the embarrassment of bringing a barf bag to the moviehouse .'\n",
      "   True: Positive | Predicted: Negative\n",
      "   Confidence: 0.710\n",
      "\n",
      "4. 'steven soderbergh doesn't remake andrei tarkovsky's solaris so much as distill it .'\n",
      "   True: Positive | Predicted: Negative\n",
      "   Confidence: 0.730\n",
      "\n",
      "5. '\" what really happened ? \" is a question for philosophers , not filmmakers ; all the filmmakers need to do is engage an audience .'\n",
      "   True: Positive | Predicted: Negative\n",
      "   Confidence: 0.717\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQmT2SO8byiQ"
   },
   "source": [
    "Test on edge cases like sarcasm and mixed sentiment."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HbH3zcFMbyiQ",
    "outputId": "4816b41f-fb3c-43db-ad82-7682d1bb2f06",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "edge_cases = [\n",
    "    (\"Sarcastic\", \"Oh great, another masterpiece. NOT!\", 0),\n",
    "    (\"Mixed\", \"The acting was great but the plot was terrible\", 0),\n",
    "    (\"Backhanded\", \"Not as bad as I expected\", 1),\n",
    "    (\"Double negative\", \"Not unwatchable\", 1),\n",
    "    (\"Very short\", \"Boring\", 0),\n",
    "    (\"Ambiguous\", \"It was a movie\", 0),\n",
    "]\n",
    "\n",
    "# TODO: Add your own test cases\n",
    "\n",
    "edge_embeddings = model.encode([text for _, text, _ in edge_cases])\n",
    "edge_predictions = clf.predict(edge_embeddings)\n",
    "edge_probs = clf.predict_proba(edge_embeddings)\n",
    "\n",
    "print(\"\\nEDGE CASES:\")\n",
    "correct_count = 0\n",
    "for i, (category, text, true_label) in enumerate(edge_cases):\n",
    "    pred = edge_predictions[i]\n",
    "    conf = edge_probs[i][pred]\n",
    "    correct = pred == true_label\n",
    "    if correct:\n",
    "        correct_count += 1\n",
    "\n",
    "    result = \"CORRECT\" if correct else \"WRONG\"\n",
    "    print(f\"\\n{category}: '{text}'\")\n",
    "    print(f\"  Predicted: {pred} | Actual: {true_label} | {result}\")\n",
    "\n",
    "print(f\"\\nEdge case accuracy: {correct_count}/{len(edge_cases)}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "EDGE CASES:\n",
      "\n",
      "Sarcastic: 'Oh great, another masterpiece. NOT!'\n",
      "  Predicted: 1 | Actual: 0 | WRONG\n",
      "\n",
      "Mixed: 'The acting was great but the plot was terrible'\n",
      "  Predicted: 0 | Actual: 0 | CORRECT\n",
      "\n",
      "Backhanded: 'Not as bad as I expected'\n",
      "  Predicted: 0 | Actual: 1 | WRONG\n",
      "\n",
      "Double negative: 'Not unwatchable'\n",
      "  Predicted: 0 | Actual: 1 | WRONG\n",
      "\n",
      "Very short: 'Boring'\n",
      "  Predicted: 0 | Actual: 0 | CORRECT\n",
      "\n",
      "Ambiguous: 'It was a movie'\n",
      "  Predicted: 0 | Actual: 0 | CORRECT\n",
      "\n",
      "Edge case accuracy: 3/6\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WgJCrepxbyiR"
   },
   "source": [
    "### Questions\n",
    "\n",
    "1. What do high-confidence errors have in common?\n",
    "\n",
    "2. Do errors tend to be shorter or longer than correct predictions?\n",
    "\n",
    "3. Which edge cases failed most - sarcasm, mixed sentiment, or double negatives?\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "0yNuuGK5cFj7"
   },
   "execution_count": 30,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}