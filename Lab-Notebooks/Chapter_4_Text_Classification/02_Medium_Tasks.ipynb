{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IyffKxPbyhq"
      },
      "source": [
        "# Chapter 4: Text Classification - Medium Tasks\n",
        "\n",
        "This notebook focuses on building practical text classifiers. You'll create custom multi-class sentiment classifiers, evaluate performance with limited training data, implement confidence-based classification with uncertainty handling, and perform systematic failure analysis. These skills are crucial for real-world NLP applications where data and perfect accuracy are often limited.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOxEdnfDbyhu"
      },
      "source": [
        "---\n",
        "\n",
        "## Setup\n",
        "\n",
        "Run all cells in this section to set up the environment and load necessary data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LGW2SD-c864"
      },
      "source": [
        "### [OPTIONAL] - Installing Packages on <img src=\"https://colab.google/static/images/icons/colab.png\" width=100>\n",
        "\n",
        "\n",
        "If you are viewing this notebook on Google Colab (or any other cloud vendor), you need to **uncomment and run** the following codeblock to install the dependencies for this chapter:\n",
        "\n",
        "---\n",
        "\n",
        " **NOTE**: We will want to use a GPU to run the examples in this notebook. In Google Colab, go to\n",
        "**Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4**.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "N-PxmOIhc865"
      },
      "outputs": [],
      "source": [
        " %%capture\n",
        "!pip install transformers sentence-transformers openai\n",
        "!pip install -U datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APb1rVC1byhy"
      },
      "source": [
        "### Data Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5phRS_z2U_3T",
        "outputId": "1a0a97f1-625f-4b1d-dbd7-d9c2f7375225"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 8530\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 1066\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 1066\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "# Load our data\n",
        "data = load_dataset(\"rotten_tomatoes\")\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30nUV21qbyh0"
      },
      "source": [
        "### Helper Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "X0KyKHtqyjn3"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "def evaluate_performance(y_true, y_pred):\n",
        "    \"\"\"Create and print the classification report\"\"\"\n",
        "    performance = classification_report(\n",
        "        y_true, y_pred,\n",
        "        target_names=[\"Negative Review\", \"Positive Review\"]\n",
        "    )\n",
        "    print(performance)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Your Turn - Text Classification Experiments\n",
        "\n",
        "Run each task first to see the baseline results. Follow the instructions to modify and experiment."
      ],
      "metadata": {
        "id": "NKYNfoaVC4hU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlVd1GVpbyh5"
      },
      "source": [
        "---\n",
        "\n",
        "## Medium Tasks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Medium Tasks - Building Real Classifiers\n",
        "\n",
        "These tasks require more modification and experimentation. You'll build complete classification systems."
      ],
      "metadata": {
        "id": "2ipz0pJYEne6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uLZBrWMbyh8"
      },
      "source": [
        "Run the code to see how 5-level classification works. Then try adding a 6th category.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Medium Task 1: Multi-Class Sentiment Classification\n",
        "In this task, you'll build a sentiment classifier with 5 different categories (from extremely negative to extremely positive) instead of just binary positive/negative.\n",
        "**What to do:**\n",
        "1. Run the cells below to see baseline 5-level classification\n",
        "2. Observe which reviews are uncertain (low margin between top predictions)\n",
        "3. Try uncommenting the 6-level version to add more granularity\n",
        "4. Compare how predictions change with more categories"
      ],
      "metadata": {
        "id": "3CZEijHVcMGI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PXpGfijbyh-"
      },
      "source": [
        "Set up the 5 sentiment categories and compute embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7ceixsfybyh-"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3B5rf4Dbyh_"
      },
      "source": [
        "Classify each review and show confidence scores:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define 5 sentiment categories\n",
        "sentiment_labels = [\n",
        "    \"extremely negative review\",\n",
        "    \"somewhat negative review\",\n",
        "    \"neutral review\",\n",
        "    \"somewhat positive review\",\n",
        "    \"extremely positive review\"\n",
        "]\n",
        "\n",
        "# Create embeddings for each category\n",
        "label_embeddings = model.encode(sentiment_labels)\n",
        "\n",
        "print(\"Sentiment categories:\")\n",
        "for i, label in enumerate(sentiment_labels):\n",
        "    print(f\"  {i}: {label}\")"
      ],
      "metadata": {
        "id": "x7dk3IHDbyh_",
        "outputId": "39b1f9f2-73d4-439e-fa6c-55d9471ac31a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment categories:\n",
            "  0: extremely negative review\n",
            "  1: somewhat negative review\n",
            "  2: neutral review\n",
            "  3: somewhat positive review\n",
            "  4: extremely positive review\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test on some sample reviews\n",
        "test_reviews = [\n",
        "    \"This is the best movie I have ever seen! Absolute masterpiece!\",\n",
        "    \"Pretty good film, I enjoyed it\",\n",
        "    \"It was okay, nothing special\",\n",
        "    \"Not very good, quite boring\",\n",
        "    \"Terrible movie, waste of time\"\n",
        "]\n",
        "\n",
        "review_embeddings = model.encode(test_reviews)\n",
        "similarities = cosine_similarity(review_embeddings, label_embeddings)\n",
        "\n",
        "print(\"Classification results:\")\n",
        "for i, review in enumerate(test_reviews):\n",
        "    predicted_category = sentiment_labels[similarities[i].argmax()]\n",
        "    confidence = similarities[i].max()\n",
        "    top_two_scores = sorted(similarities[i], reverse=True)[:2]\n",
        "    margin = top_two_scores[0] - top_two_scores[1]\n",
        "\n",
        "    print(f\"\\n'{review}'\")\n",
        "    print(f\"  → {predicted_category}\")\n",
        "    print(f\"  Confidence: {confidence:.3f} | Margin: {margin:.3f}\")"
      ],
      "metadata": {
        "id": "SlF4O-VVbyiA",
        "outputId": "55bcff11-fff2-48ee-8dda-5d8f560466e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification results:\n",
            "\n",
            "'This is the best movie I have ever seen! Absolute masterpiece!'\n",
            "  → extremely positive review\n",
            "  Confidence: 0.256 | Margin: 0.099\n",
            "\n",
            "'Pretty good film, I enjoyed it'\n",
            "  → somewhat positive review\n",
            "  Confidence: 0.428 | Margin: 0.073\n",
            "\n",
            "'It was okay, nothing special'\n",
            "  → somewhat negative review\n",
            "  Confidence: 0.439 | Margin: 0.004\n",
            "\n",
            "'Not very good, quite boring'\n",
            "  → somewhat positive review\n",
            "  Confidence: 0.378 | Margin: 0.000\n",
            "\n",
            "'Terrible movie, waste of time'\n",
            "  → extremely negative review\n",
            "  Confidence: 0.426 | Margin: 0.009\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZPl8Fu08byiB",
        "outputId": "010508bf-0d43-4622-9435-0236662f931b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CATEGORY CONFUSION ANALYSIS\n",
            "Category Pair                                                Similarity  \n",
            " extremely negative review <-> somewhat negative review       0.952\n",
            " somewhat negative review <-> somewhat positive review        0.863\n",
            " somewhat positive review <-> extremely positive review       0.818\n",
            " somewhat negative review <-> neutral review                  0.794\n",
            " extremely negative review <-> somewhat positive review       0.766\n",
            " extremely negative review <-> neutral review                 0.748\n",
            " neutral review <-> somewhat positive review                  0.748\n",
            "somewhat negative review <-> extremely positive review       0.674\n",
            "extremely negative review <-> extremely positive review      0.664\n",
            "neutral review <-> extremely positive review                 0.616\n"
          ]
        }
      ],
      "source": [
        "print()\n",
        "print(\"CATEGORY CONFUSION ANALYSIS\")\n",
        "label_similarity = cosine_similarity(label_embeddings)\n",
        "print(f\"{'Category Pair':<60s} {'Similarity':<12s}\")\n",
        "confusions = []\n",
        "for i in range(len(sentiment_labels)):\n",
        "    for j in range(i+1, len(sentiment_labels)):\n",
        "        sim = label_similarity[i][j]\n",
        "        confusions.append((i, j, sim))\n",
        "for i, j, sim in sorted(confusions, key=lambda x: x[2], reverse=True)[:10]:\n",
        "    pair_name = f\"{sentiment_labels[i]} <-> {sentiment_labels[j]}\"\n",
        "    marker = \" \" if sim > 0.7 else \"\"\n",
        "    print(f\"{marker}{pair_name:<60s} {sim:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the classifier assigns each review to one of the 5 sentiment categories. The **margin** (difference between top 2 predictions) indicates confidence - large margins (>0.15) mean the model is confident, while small margins (<0.05) indicate uncertainty. Reviews with extreme language (\"best ever\", \"terrible\") have higher confidence, while moderate reviews (\"pretty good\", \"quite bad\") show more uncertainty."
      ],
      "metadata": {
        "id": "fECdQPWjbyiC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFy4lcn9byiD"
      },
      "source": [
        "Analyze which categories are most similar to each other:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that adjacent categories (like \"somewhat negative\" and \"neutral\") tend to have higher similarity scores, which explains why the model sometimes confuses them. Categories with similarity > 0.7 are particularly prone to confusion."
      ],
      "metadata": {
        "id": "EV2pv7pbbyiD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zL_xe6AabyiE"
      },
      "source": [
        "#### Medium Task 2: Classifier Performance with Limited Training Data\n",
        "\n",
        "Try different training sizes (100, 500, 1000, 2000) and fill in the results table.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JeFbrbUobyiE",
        "outputId": "54740af4-1f3a-4922-d26e-300e53705cc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EXPERIMENT: Training Size = 1000\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "data = load_dataset(\"rotten_tomatoes\")\n",
        "# TODO: Try different values: 100, 500, 1000, 2000, 5000\n",
        "train_size = 1000\n",
        "test_size = 300\n",
        "train_subset = data[\"train\"].shuffle(seed=42).select(range(min(train_size, len(data[\"train\"]))))\n",
        "test_subset = data[\"test\"].shuffle(seed=42).select(range(test_size))\n",
        "print(f\"EXPERIMENT: Training Size = {train_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9_26CEFobyiF",
        "outputId": "9b387913-a338-42bc-c254-dc49d65a5e04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[1/2] Testing Task-Specific Model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Task-Specific Model F1: 0.7709\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n[1/2] Testing Task-Specific Model...\")\n",
        "task_model = pipeline(\n",
        "    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
        "    tokenizer=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
        "    return_all_scores=True,\n",
        "    device=-1\n",
        ")\n",
        "y_pred_task = []\n",
        "for text in test_subset[\"text\"]:\n",
        "    output = task_model(text)[0]\n",
        "    neg_score = output[0][\"score\"]\n",
        "    pos_score = output[2][\"score\"]\n",
        "    y_pred_task.append(1 if pos_score > neg_score else 0)\n",
        "task_f1 = f1_score(test_subset[\"label\"], y_pred_task, average='weighted')\n",
        "print(f\" Task-Specific Model F1: {task_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYLELJ-hbyiF"
      },
      "source": [
        "Train the embedding-based classifier on your labeled data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "q7Jgae8WbyiG",
        "outputId": "357918b4-8dfd-47a9-ed4c-0c48c64f97b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[2/2] Training Embedding Classifier on 1000 samples...\n",
            " Embedding Classifier F1: 0.8699\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\n[2/2] Training Embedding Classifier on {train_size} samples...\")\n",
        "embedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "train_embeddings = embedding_model.encode(train_subset[\"text\"], show_progress_bar=False)\n",
        "test_embeddings = embedding_model.encode(test_subset[\"text\"], show_progress_bar=False)\n",
        "clf = LogisticRegression(random_state=42, max_iter=1000)\n",
        "clf.fit(train_embeddings, train_subset[\"label\"])\n",
        "y_pred_embed = clf.predict(test_embeddings)\n",
        "embed_f1 = f1_score(test_subset[\"label\"], y_pred_embed, average='weighted')\n",
        "print(f\" Embedding Classifier F1: {embed_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acpiODYAbyiH"
      },
      "source": [
        "Compare the two approaches and show example predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KKZ7v0nvbyiH",
        "outputId": "ea37226c-2749-45ef-c812-b21d530dee54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "RESULTS SUMMARY\n",
            "Training samples used: 1000\n",
            "\n",
            "Task-Specific (pre-trained):  F1 = 0.7709\n",
            "Embedding + Classifier:       F1 = 0.8699\n",
            "Difference:                       +0.0990\n",
            "\n",
            " Embedding approach WINS with 1000 samples!\n",
            "\n",
            "EXAMPLE PREDICTIONS (first 5)\n",
            "\n",
            "1. 'unpretentious , charming , quirky , original...'\n",
            "   True: Positive\n",
            "   Task-Specific: Positive \n",
            "   Embedding:     Positive \n",
            "\n",
            "2. 'a film really has to be exceptional to justify a three hour ...'\n",
            "   True: Negative\n",
            "   Task-Specific: Negative \n",
            "   Embedding:     Negative \n",
            "\n",
            "3. 'working from a surprisingly sensitive script co-written by g...'\n",
            "   True: Positive\n",
            "   Task-Specific: Positive \n",
            "   Embedding:     Positive \n",
            "\n",
            "4. 'it may not be particularly innovative , but the film's crisp...'\n",
            "   True: Positive\n",
            "   Task-Specific: Positive \n",
            "   Embedding:     Positive \n",
            "\n",
            "5. 'such a premise is ripe for all manner of lunacy , but kaufma...'\n",
            "   True: Negative\n",
            "   Task-Specific: Negative \n",
            "   Embedding:     Negative \n",
            "\n",
            "TODO: Record your results\n",
            "Current: | 1000       | 0.7709  | 0.8699       | Embed       |\n"
          ]
        }
      ],
      "source": [
        "print()\n",
        "print(\"RESULTS SUMMARY\")\n",
        "print(f\"Training samples used: {train_size}\")\n",
        "print(f\"\\nTask-Specific (pre-trained):  F1 = {task_f1:.4f}\")\n",
        "print(f\"Embedding + Classifier:       F1 = {embed_f1:.4f}\")\n",
        "print(f\"Difference:                       {embed_f1 - task_f1:+.4f}\")\n",
        "if embed_f1 > task_f1:\n",
        "    print(f\"\\n Embedding approach WINS with {train_size} samples!\")\n",
        "elif embed_f1 > task_f1 - 0.01:\n",
        "    print(f\"\\n Essentially TIED\")\n",
        "else:\n",
        "    print(f\"\\n Task-specific model wins\")\n",
        "# Show example predictions\n",
        "print()\n",
        "print(\"EXAMPLE PREDICTIONS (first 5)\")\n",
        "for i in range(5):\n",
        "    true_label = \"Positive\" if test_subset[\"label\"][i] == 1 else \"Negative\"\n",
        "    task_pred = \"Positive\" if y_pred_task[i] == 1 else \"Negative\"\n",
        "    embed_pred = \"Positive\" if y_pred_embed[i] == 1 else \"Negative\"\n",
        "    task_correct = \"\" if y_pred_task[i] == test_subset[\"label\"][i] else \"\"\n",
        "    embed_correct = \"\" if y_pred_embed[i] == test_subset[\"label\"][i] else \"\"\n",
        "    print(f\"\\n{i+1}. '{test_subset['text'][i][:60]}...'\")\n",
        "    print(f\"   True: {true_label}\")\n",
        "    print(f\"   Task-Specific: {task_pred} {task_correct}\")\n",
        "    print(f\"   Embedding:     {embed_pred} {embed_correct}\")\n",
        "print()\n",
        "print(\"TODO: Record your results\")\n",
        "print(f\"Current: | {train_size:<10} | {task_f1:.4f}  | {embed_f1:.4f}       | {'Embed' if embed_f1 > task_f1 else 'Task':<11} |\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCWBW3qObyiH"
      },
      "source": [
        "### Questions\n",
        "\n",
        "1. At what training size did embedding classifier match the task-specific model?\n",
        "\n",
        "2. Were there cases where one model was correct and the other wrong?\n",
        "\n",
        "3. Is 100 samples enough labeled data?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKfJlPDtbyiI"
      },
      "source": [
        "#### Medium Task 3: Confidence-Based Classifier with Uncertainty Handling\n",
        "\n",
        "In production, refusing a prediction beats making a wrong one. Here's the key insight: when your model is uncertain, it should say \"I don't know\" rather than guessing. This creates a trade-off between coverage (how many predictions you make) and accuracy (how often you're right).\n",
        "\n",
        "Try this:\n",
        "- Run with threshold of 0.15 first\n",
        "- Test 0.05, 0.30, and 0.50 to see how the trade-off shifts\n",
        "- Check the uncertain cases (typically have hedging language)\n",
        "- Experiment with the alternative uncertainty measure"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "UrlfyjrabyiJ"
      },
      "outputs": [],
      "execution_count": 12
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "\n",
        "# Reviews with varying levels of clarity\n",
        "test_reviews = [\n",
        "    \"Absolutely fantastic! Best movie ever!\",           # Clear positive\n",
        "    \"Pretty good, I liked it\",                          # Weak positive\n",
        "    \"It was fine, nothing special\",                     # Ambiguous\n",
        "    \"Not bad but not great either\",                     # Very ambiguous\n",
        "    \"Quite disappointing\",                              # Weak negative\n",
        "    \"Terrible! Complete waste of time!\",                # Clear negative\n",
        "    \"The movie had some interesting moments\",           # Ambiguous positive\n",
        "    \"Outstanding performances all around!\",             # Clear positive\n",
        "]\n",
        "\n",
        "# True labels (for evaluation)\n",
        "y_true = [1, 1, 0, 0, 0, 0, 1, 1]  # 1=positive, 0=negative"
      ],
      "metadata": {
        "id": "9sf1K6sibyiJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [\"negative\", \"positive\"]\n",
        "\n",
        "# TODO: EXPERIMENT WITH THIS - Try: 0.05, 0.15, 0.30, 0.50\n",
        "confidence_threshold = 0.15"
      ],
      "metadata": {
        "id": "jLZjsRNVbyiK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_margin(similarities):\n",
        "    \"\"\"\n",
        "    Margin = difference between top two predictions\n",
        "    Small margin = uncertain (predictions are close)\n",
        "    \"\"\"\n",
        "    sorted_sims = np.sort(similarities)[::-1]\n",
        "    margin = sorted_sims[0] - sorted_sims[1]\n",
        "    return margin\n",
        "\n",
        "# TODO: After first run, uncomment this alternative uncertainty measure:\n",
        "# def calculate_margin(similarities):\n",
        "#     \"\"\"\n",
        "#     Alternative: Use absolute confidence in top prediction\n",
        "#     Low confidence = uncertain\n",
        "#     \"\"\"\n",
        "#     max_confidence = np.max(similarities)\n",
        "#     # Convert to margin-like score (higher = more certain)\n",
        "#     # If max is 0.6, margin = 0.6 - 0.5 = 0.1 (uncertain)\n",
        "#     # If max is 0.9, margin = 0.9 - 0.5 = 0.4 (certain)\n",
        "#     margin = max_confidence - 0.5\n",
        "#     return margin"
      ],
      "metadata": {
        "id": "Ss-J6mVsbyiL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_embeddings = model.encode(labels)\n",
        "review_embeddings = model.encode(test_reviews)\n",
        "sim_matrix = cosine_similarity(review_embeddings, label_embeddings)"
      ],
      "metadata": {
        "id": "htw33fKTbyiL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classify with confidence threshold\n",
        "results = []\n",
        "predictions = []\n",
        "confidences = []\n",
        "\n",
        "print(f\"CONFIDENCE-BASED CLASSIFICATION (threshold={confidence_threshold})\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for i, review in enumerate(test_reviews):\n",
        "    similarities = sim_matrix[i]\n",
        "    predicted_idx = np.argmax(similarities)\n",
        "    top_confidence = similarities[predicted_idx]\n",
        "    margin = calculate_margin(similarities)\n",
        "\n",
        "    # Decision: predict only if confident enough\n",
        "    if margin >= confidence_threshold:\n",
        "        prediction = predicted_idx\n",
        "        status = \"PREDICTED\"\n",
        "        predictions.append(prediction)\n",
        "    else:\n",
        "        prediction = None\n",
        "        status = \"UNCERTAIN\"\n",
        "        predictions.append(None)\n",
        "\n",
        "    true_label = \"Positive\" if y_true[i] == 1 else \"Negative\"\n",
        "    pred_label = labels[predicted_idx] if prediction is not None else \"UNCERTAIN\"\n",
        "\n",
        "    print(f\"\\n{i+1}. '{review}'\")\n",
        "    print(f\"   True label: {true_label}\")\n",
        "    print(f\"   Prediction: {pred_label}\")\n",
        "    print(f\"   Top confidence: {top_confidence:.3f}\")\n",
        "    print(f\"   Margin: {margin:.3f} {'✓ Above threshold' if margin >= confidence_threshold else '✗ Below threshold'}\")\n",
        "    print(f\"   Status: {status}\", end=\"\")\n",
        "\n",
        "    if prediction is not None:\n",
        "        correct = prediction == y_true[i]\n",
        "        print(f\" - {'✓ CORRECT' if correct else '✗ INCORRECT'}\")\n",
        "    else:\n",
        "        print()\n",
        "\n",
        "    results.append({\n",
        "        'review': review,\n",
        "        'true': y_true[i],\n",
        "        'pred': prediction,\n",
        "        'margin': margin,\n",
        "        'status': status\n",
        "    })"
      ],
      "metadata": {
        "id": "oDGa7ACXbyiL",
        "outputId": "1efd9f11-2ffb-4e0b-e9d4-facdca0d0599",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CONFIDENCE-BASED CLASSIFICATION (threshold=0.15)\n",
            "================================================================================\n",
            "\n",
            "1. 'Absolutely fantastic! Best movie ever!'\n",
            "   True label: Positive\n",
            "   Prediction: UNCERTAIN\n",
            "   Top confidence: 0.173\n",
            "   Margin: 0.047 ✗ Below threshold\n",
            "   Status: UNCERTAIN\n",
            "\n",
            "2. 'Pretty good, I liked it'\n",
            "   True label: Positive\n",
            "   Prediction: UNCERTAIN\n",
            "   Top confidence: 0.217\n",
            "   Margin: 0.033 ✗ Below threshold\n",
            "   Status: UNCERTAIN\n",
            "\n",
            "3. 'It was fine, nothing special'\n",
            "   True label: Negative\n",
            "   Prediction: UNCERTAIN\n",
            "   Top confidence: 0.254\n",
            "   Margin: 0.055 ✗ Below threshold\n",
            "   Status: UNCERTAIN\n",
            "\n",
            "4. 'Not bad but not great either'\n",
            "   True label: Negative\n",
            "   Prediction: UNCERTAIN\n",
            "   Top confidence: 0.209\n",
            "   Margin: 0.003 ✗ Below threshold\n",
            "   Status: UNCERTAIN\n",
            "\n",
            "5. 'Quite disappointing'\n",
            "   True label: Negative\n",
            "   Prediction: UNCERTAIN\n",
            "   Top confidence: 0.276\n",
            "   Margin: 0.061 ✗ Below threshold\n",
            "   Status: UNCERTAIN\n",
            "\n",
            "6. 'Terrible! Complete waste of time!'\n",
            "   True label: Negative\n",
            "   Prediction: UNCERTAIN\n",
            "   Top confidence: 0.324\n",
            "   Margin: 0.062 ✗ Below threshold\n",
            "   Status: UNCERTAIN\n",
            "\n",
            "7. 'The movie had some interesting moments'\n",
            "   True label: Positive\n",
            "   Prediction: UNCERTAIN\n",
            "   Top confidence: 0.169\n",
            "   Margin: 0.054 ✗ Below threshold\n",
            "   Status: UNCERTAIN\n",
            "\n",
            "8. 'Outstanding performances all around!'\n",
            "   True label: Positive\n",
            "   Prediction: UNCERTAIN\n",
            "   Top confidence: 0.141\n",
            "   Margin: 0.068 ✗ Below threshold\n",
            "   Status: UNCERTAIN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate metrics\n",
        "made_predictions = [r for r in results if r['pred'] is not None]\n",
        "uncertain_cases = [r for r in results if r['pred'] is None]\n",
        "correct_predictions = [r for r in made_predictions if r['pred'] == r['true']]\n",
        "\n",
        "total = len(results)\n",
        "n_predicted = len(made_predictions)\n",
        "n_uncertain = len(uncertain_cases)\n",
        "n_correct = len(correct_predictions)\n",
        "\n",
        "coverage = n_predicted / total\n",
        "accuracy = n_correct / n_predicted if n_predicted > 0 else 0\n",
        "\n",
        "print()\n",
        "print(\"=\"*80)\n",
        "print(\"PERFORMANCE ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nCoverage: {n_predicted}/{total} = {coverage:.1%}\")\n",
        "print(f\"  → Made predictions for {n_predicted} reviews\")\n",
        "print(f\"  → Refused to predict on {n_uncertain} reviews\")\n",
        "\n",
        "print(f\"\\nAccuracy (on predictions made): {n_correct}/{n_predicted} = {accuracy:.1%}\")\n",
        "print(f\"  → Of the {n_predicted} predictions, {n_correct} were correct\")\n",
        "\n",
        "print(f\"\\nTrade-off Analysis:\")\n",
        "print(f\"  Threshold = {confidence_threshold}\")\n",
        "print(f\"  → Higher threshold = fewer predictions but higher accuracy\")\n",
        "print(f\"  → Lower threshold = more predictions but lower accuracy\")"
      ],
      "metadata": {
        "id": "Oc38LloRbyiM",
        "outputId": "4e586ec6-6114-4736-85d9-7e44561c9331",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "PERFORMANCE ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "Coverage: 0/8 = 0.0%\n",
            "  → Made predictions for 0 reviews\n",
            "  → Refused to predict on 8 reviews\n",
            "\n",
            "Accuracy (on predictions made): 0/0 = 0.0%\n",
            "  → Of the 0 predictions, 0 were correct\n",
            "\n",
            "Trade-off Analysis:\n",
            "  Threshold = 0.15\n",
            "  → Higher threshold = fewer predictions but higher accuracy\n",
            "  → Lower threshold = more predictions but lower accuracy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Questions\n",
        "\n",
        "1. What do uncertain reviews have in common? Are they using hedging language like \"kind of\" or \"somewhat\"?\n",
        "\n",
        "2. Compare results at threshold=0.05 vs 0.30. Describe the coverage vs accuracy trade-off. When would you want high coverage vs high accuracy?\n",
        "\n",
        "3. How could you use confidence-based prediction in production? What should a system do when the model is uncertain?"
      ],
      "metadata": {
        "id": "H7-OE7H9F6vx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDEBU0cLbyiN"
      },
      "source": [
        "#### Medium Task 4: Classifier Failure Analysis\n",
        "\n",
        "Train the classifier and see what kinds of reviews it gets wrong. Then add your own test cases.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UDR7v_0byiN"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from datasets import load_dataset\n",
        "import numpy as np"
      ],
      "outputs": [],
      "execution_count": 19
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgPRrIxLbyiO"
      },
      "source": [
        "Load data and train a classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkW9_JNLbyiO",
        "outputId": "d55ce6e5-4b4a-473c-acf5-32189814b740",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data = load_dataset(\"rotten_tomatoes\")\n",
        "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "\n",
        "train_subset = data[\"train\"].shuffle(seed=42).select(range(1000))\n",
        "test_subset = data[\"test\"].shuffle(seed=42).select(range(200))\n",
        "\n",
        "print(\"Training classifier...\")\n",
        "train_embeddings = model.encode(train_subset[\"text\"], show_progress_bar=False)\n",
        "test_embeddings = model.encode(test_subset[\"text\"], show_progress_bar=False)\n",
        "\n",
        "clf = LogisticRegression(random_state=42, max_iter=1000)\n",
        "clf.fit(train_embeddings, train_subset[\"label\"])\n",
        "\n",
        "predictions = clf.predict(test_embeddings)\n",
        "probabilities = clf.predict_proba(test_embeddings)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training classifier...\n"
          ]
        }
      ],
      "execution_count": 20
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wm3p9MfHbyiO"
      },
      "source": [
        "Analyze the errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpSZkDVobyiP",
        "outputId": "13daf6ba-a1bf-4c0b-a55d-17c092799c25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "errors = []\n",
        "for i in range(len(test_subset)):\n",
        "    if predictions[i] != test_subset[\"label\"][i]:\n",
        "        confidence = probabilities[i][predictions[i]]\n",
        "        errors.append({\n",
        "            'index': i,\n",
        "            'text': test_subset[\"text\"][i],\n",
        "            'true_label': test_subset[\"label\"][i],\n",
        "            'predicted_label': predictions[i],\n",
        "            'confidence': confidence,\n",
        "            'length': len(test_subset[\"text\"][i].split())\n",
        "        })\n",
        "\n",
        "total_errors = len(errors)\n",
        "total_samples = len(test_subset)\n",
        "accuracy = (total_samples - total_errors) / total_samples\n",
        "\n",
        "print(f\"Overall: {total_samples - total_errors}/{total_samples} correct ({accuracy:.1%})\")\n",
        "print(f\"Errors: {total_errors}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall: 174/200 correct (87.0%)\n",
            "Errors: 26\n"
          ]
        }
      ],
      "execution_count": 21
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOX4A0LtbyiP"
      },
      "source": [
        "Look at high-confidence errors (the most surprising mistakes)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kY0cgAP-byiP",
        "outputId": "0e5c361d-05c0-4e1d-ecfc-2f5909d9523b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "high_conf_errors = [e for e in errors if e['confidence'] > 0.7]\n",
        "\n",
        "print(\"HIGH-CONFIDENCE ERRORS:\")\n",
        "for i, error in enumerate(high_conf_errors[:5]):\n",
        "    true_sent = \"Positive\" if error['true_label'] == 1 else \"Negative\"\n",
        "    pred_sent = \"Positive\" if error['predicted_label'] == 1 else \"Negative\"\n",
        "\n",
        "    print(f\"\\n{i+1}. '{error['text']}'\")\n",
        "    print(f\"   True: {true_sent} | Predicted: {pred_sent}\")\n",
        "    print(f\"   Confidence: {error['confidence']:.3f}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HIGH-CONFIDENCE ERRORS:\n",
            "\n",
            "1. 'an uneasy mix of run-of-the-mill raunchy humor and seemingly sincere personal reflection .'\n",
            "   True: Negative | Predicted: Positive\n",
            "   Confidence: 0.701\n",
            "\n",
            "2. 'the stunt work is top-notch ; the dialogue and drama often food-spittingly funny .'\n",
            "   True: Negative | Predicted: Positive\n",
            "   Confidence: 0.867\n",
            "\n",
            "3. 'goldmember is funny enough to justify the embarrassment of bringing a barf bag to the moviehouse .'\n",
            "   True: Positive | Predicted: Negative\n",
            "   Confidence: 0.710\n",
            "\n",
            "4. 'steven soderbergh doesn't remake andrei tarkovsky's solaris so much as distill it .'\n",
            "   True: Positive | Predicted: Negative\n",
            "   Confidence: 0.730\n",
            "\n",
            "5. '\" what really happened ? \" is a question for philosophers , not filmmakers ; all the filmmakers need to do is engage an audience .'\n",
            "   True: Positive | Predicted: Negative\n",
            "   Confidence: 0.717\n"
          ]
        }
      ],
      "execution_count": 22
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQmT2SO8byiQ"
      },
      "source": [
        "Test on edge cases like sarcasm and mixed sentiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbH3zcFMbyiQ",
        "outputId": "d26bed3a-ac6a-413c-d7b5-18b6080e1992",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "edge_cases = [\n",
        "    (\"Sarcastic\", \"Oh great, another masterpiece. NOT!\", 0),\n",
        "    (\"Mixed\", \"The acting was great but the plot was terrible\", 0),\n",
        "    (\"Backhanded\", \"Not as bad as I expected\", 1),\n",
        "    (\"Double negative\", \"Not unwatchable\", 1),\n",
        "    (\"Very short\", \"Boring\", 0),\n",
        "    (\"Ambiguous\", \"It was a movie\", 0),\n",
        "]\n",
        "\n",
        "# TODO: Add your own test cases\n",
        "\n",
        "edge_embeddings = model.encode([text for _, text, _ in edge_cases])\n",
        "edge_predictions = clf.predict(edge_embeddings)\n",
        "edge_probs = clf.predict_proba(edge_embeddings)\n",
        "\n",
        "print(\"\\nEDGE CASES:\")\n",
        "correct_count = 0\n",
        "for i, (category, text, true_label) in enumerate(edge_cases):\n",
        "    pred = edge_predictions[i]\n",
        "    conf = edge_probs[i][pred]\n",
        "    correct = pred == true_label\n",
        "    if correct:\n",
        "        correct_count += 1\n",
        "\n",
        "    result = \"CORRECT\" if correct else \"WRONG\"\n",
        "    print(f\"\\n{category}: '{text}'\")\n",
        "    print(f\"  Predicted: {pred} | Actual: {true_label} | {result}\")\n",
        "\n",
        "print(f\"\\nEdge case accuracy: {correct_count}/{len(edge_cases)}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "EDGE CASES:\n",
            "\n",
            "Sarcastic: 'Oh great, another masterpiece. NOT!'\n",
            "  Predicted: 1 | Actual: 0 | WRONG\n",
            "\n",
            "Mixed: 'The acting was great but the plot was terrible'\n",
            "  Predicted: 0 | Actual: 0 | CORRECT\n",
            "\n",
            "Backhanded: 'Not as bad as I expected'\n",
            "  Predicted: 0 | Actual: 1 | WRONG\n",
            "\n",
            "Double negative: 'Not unwatchable'\n",
            "  Predicted: 0 | Actual: 1 | WRONG\n",
            "\n",
            "Very short: 'Boring'\n",
            "  Predicted: 0 | Actual: 0 | CORRECT\n",
            "\n",
            "Ambiguous: 'It was a movie'\n",
            "  Predicted: 0 | Actual: 0 | CORRECT\n",
            "\n",
            "Edge case accuracy: 3/6\n"
          ]
        }
      ],
      "execution_count": 23
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgJCrepxbyiR"
      },
      "source": [
        "### Questions\n",
        "\n",
        "1. What do high-confidence errors have in common?\n",
        "\n",
        "2. Do errors tend to be shorter or longer than correct predictions?\n",
        "\n",
        "3. Which edge cases failed most - sarcasm, mixed sentiment, or double negatives?\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0yNuuGK5cFj7"
      },
      "execution_count": 23,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}