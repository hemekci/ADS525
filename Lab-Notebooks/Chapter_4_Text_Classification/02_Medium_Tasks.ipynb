{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IyffKxPbyhq"
      },
      "source": "# Chapter 4: Text Classification - Medium Tasks\n\nThis notebook focuses on building practical text classifiers. You'll create custom multi-class sentiment classifiers, evaluate performance with limited training data, implement confidence-based classification with uncertainty handling, and perform systematic failure analysis. These skills are crucial for real-world NLP applications where data and perfect accuracy are often limited.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOxEdnfDbyhu"
      },
      "source": "---\n\n## Setup\n\nRun all cells in this section to set up the environment and load necessary data.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LGW2SD-c864"
      },
      "source": "### [Optional] - Installing Packages on <img src=\"https://colab.google/static/images/icons/colab.png\" width=100>\n\nIf you are viewing this notebook on Google Colab (or any other cloud vendor), you need to **uncomment and run** the following codeblock to install the dependencies for this chapter:\n\n---\n\n **Note**: We will want to use a GPU to run the examples in this notebook. In Google Colab, go to\n**Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4**.\n\n---\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-PxmOIhc865"
      },
      "outputs": [],
      "source": " %%capture\n!pip install transformers sentence-transformers openai\n!pip install -U datasets"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APb1rVC1byhy"
      },
      "source": "### Data Loading\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5phRS_z2U_3T",
        "outputId": "1a0a97f1-625f-4b1d-dbd7-d9c2f7375225"
      },
      "outputs": [],
      "source": "from datasets import load_dataset\n# Load our data\ndata = load_dataset(\"rotten_tomatoes\")\ndata"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30nUV21qbyh0"
      },
      "source": "### Helper Functions\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0KyKHtqyjn3"
      },
      "outputs": [],
      "source": "from sklearn.metrics import classification_report\ndef evaluate_performance(y_true, y_pred):\n    \"\"\"Create and print the classification report\"\"\"\n    performance = classification_report(\n        y_true, y_pred,\n        target_names=[\"Negative Review\", \"Positive Review\"]\n    )\n    print(performance)"
    },
    {
      "cell_type": "markdown",
      "source": "## Your Turn - Text Classification Experiments\n\nRun each task first to see the baseline results. Follow the instructions to modify and experiment.",
      "metadata": {
        "id": "NKYNfoaVC4hU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlVd1GVpbyh5"
      },
      "source": "---\n\n## Medium Tasks\n"
    },
    {
      "cell_type": "markdown",
      "source": "### Medium Tasks - Building Real Classifiers\n\nThese tasks require more modification and experimentation. You'll build complete classification systems.",
      "metadata": {
        "id": "2ipz0pJYEne6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uLZBrWMbyh8"
      },
      "source": "Run the code to see how 5-level classification works. Then try adding a 6th category.\n"
    },
    {
      "cell_type": "markdown",
      "source": "#### Medium Task 1: Multi-Class Sentiment Classification\nIn this task, you'll build a sentiment classifier with 5 different categories (from extremely negative to extremely positive) instead of just binary positive/negative.\n**What to do:**\n1. Run the cells below to see baseline 5-level classification\n2. Observe which reviews are uncertain (low margin between top predictions)\n3. Try uncommenting the 6-level version to add more granularity\n4. Compare how predictions change with more categories",
      "metadata": {
        "id": "3CZEijHVcMGI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PXpGfijbyh-"
      },
      "source": "Set up the 5 sentiment categories and compute embeddings:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ceixsfybyh-"
      },
      "outputs": [],
      "source": "from sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nmodel = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3B5rf4Dbyh_"
      },
      "source": "Classify each review and show confidence scores:"
    },
    {
      "cell_type": "code",
      "source": "# Define 5 sentiment categories\nsentiment_labels = [\n    \"extremely negative review\",\n    \"somewhat negative review\",\n    \"neutral review\",\n    \"somewhat positive review\",\n    \"extremely positive review\"\n]\n\n# Create embeddings for each category\nlabel_embeddings = model.encode(sentiment_labels)\n\nprint(\"Sentiment categories:\")\nfor i, label in enumerate(sentiment_labels):\n    print(f\"  {i}: {label}\")",
      "metadata": {
        "id": "x7dk3IHDbyh_",
        "outputId": "39b1f9f2-73d4-439e-fa6c-55d9471ac31a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Test reviews\ntest_reviews = [\n    \"This is the best movie I have ever seen! Absolute masterpiece!\",\n    \"Pretty good film, I enjoyed it\",\n    \"It was okay, nothing special\",\n    \"Not very good, quite boring\",\n    \"Terrible movie, waste of time\"\n]\n\nreview_embeddings = model.encode(test_reviews)\nsimilarities = cosine_similarity(review_embeddings, label_embeddings)"
    },
    {
      "cell_type": "code",
      "source": "# Complete this: Classify each review\nprint(\"Classification results:\")\n\nfor i, review in enumerate(test_reviews):\n    # Find which category has highest similarity\n    predicted_idx = None  # use argmax\n    confidence = None  # max similarity score\n    \n    # Calculate margin between top 2 predictions\n    top_two = None  # sort and get top 2\n    margin = None  # difference\n    \n    if predicted_idx is not None:\n        print(f\"\\n'{review}'\")\n        print(f\"  -> {sentiment_labels[predicted_idx]}\")\n        print(f\"  Confidence: {confidence:.3f} | Margin: {margin:.3f}\")",
      "metadata": {
        "id": "SlF4O-VVbyiA",
        "outputId": "55bcff11-fff2-48ee-8dda-5d8f560466e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPl8Fu08byiB",
        "outputId": "010508bf-0d43-4622-9435-0236662f931b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [],
      "source": "# Analyze category confusion\nprint(\"\\nCategory Confusion Analysis\")\nlabel_similarity = cosine_similarity(label_embeddings)\n\n# Find most similar category pairs\nconfusions = []\nfor i in range(len(sentiment_labels)):\n    for j in range(i+1, len(sentiment_labels)):\n        sim = label_similarity[i][j]\n        confusions.append((i, j, sim))\n\n# Show top 5 confusions\nfor i, j, sim in sorted(confusions, key=lambda x: x[2], reverse=True)[:5]:\n    print(f\"{sentiment_labels[i]} <-> {sentiment_labels[j]}: {sim:.3f}\")"
    },
    {
      "cell_type": "markdown",
      "source": "As you can see, the classifier assigns each review to one of the 5 sentiment categories. The **margin** (difference between top 2 predictions) indicates confidence - large margins (>0.15) mean the model is confident, while small margins (<0.05) indicate uncertainty. Reviews with extreme language (\"best ever\", \"terrible\") have higher confidence, while moderate reviews (\"pretty good\", \"quite bad\") show more uncertainty.",
      "metadata": {
        "id": "fECdQPWjbyiC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFy4lcn9byiD"
      },
      "source": "Analyze which categories are most similar to each other:"
    },
    {
      "cell_type": "markdown",
      "source": "Notice that adjacent categories (like \"somewhat negative\" and \"neutral\") tend to have higher similarity scores, which explains why the model sometimes confuses them. Categories with similarity > 0.7 are particularly prone to confusion.",
      "metadata": {
        "id": "EV2pv7pbbyiD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zL_xe6AabyiE"
      },
      "source": "#### Medium Task 2: Classifier Performance with Limited Training Data\n\nTry different training sizes (100, 500, 1000, 2000) and fill in the results table.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeFbrbUobyiE",
        "outputId": "54740af4-1f3a-4922-d26e-300e53705cc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [],
      "source": "from transformers import pipeline\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score, classification_report\nfrom datasets import load_dataset\nimport numpy as np\ndata = load_dataset(\"rotten_tomatoes\")\n# Try different values: 100, 500, 1000, 2000, 5000\ntrain_size = 1000\ntest_size = 300\ntrain_subset = data[\"train\"].shuffle(seed=42).select(range(min(train_size, len(data[\"train\"]))))\ntest_subset = data[\"test\"].shuffle(seed=42).select(range(test_size))\nprint(f\"Experiment: Training Size = {train_size}\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_26CEFobyiF",
        "outputId": "9b387913-a338-42bc-c254-dc49d65a5e04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [],
      "source": "print(\"\\n[1/2] Testing Task-Specific Model...\")\ntask_model = pipeline(\n    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n    tokenizer=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n    return_all_scores=True,\n    device=-1\n)\ny_pred_task = []\nfor text in test_subset[\"text\"]:\n    output = task_model(text)[0]\n    neg_score = output[0][\"score\"]\n    pos_score = output[2][\"score\"]\n    y_pred_task.append(1 if pos_score > neg_score else 0)\ntask_f1 = f1_score(test_subset[\"label\"], y_pred_task, average='weighted')\nprint(f\" Task-Specific Model F1: {task_f1:.4f}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYLELJ-hbyiF"
      },
      "source": "Train the embedding-based classifier on your labeled data:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7Jgae8WbyiG",
        "outputId": "357918b4-8dfd-47a9-ed4c-0c48c64f97b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [],
      "source": "print(f\"\\n[2/2] Training Embedding Classifier on {train_size} samples...\")\nembedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\ntrain_embeddings = embedding_model.encode(train_subset[\"text\"], show_progress_bar=False)\ntest_embeddings = embedding_model.encode(test_subset[\"text\"], show_progress_bar=False)\nclf = LogisticRegression(random_state=42, max_iter=1000)\nclf.fit(train_embeddings, train_subset[\"label\"])\ny_pred_embed = clf.predict(test_embeddings)\nembed_f1 = f1_score(test_subset[\"label\"], y_pred_embed, average='weighted')\nprint(f\" Embedding Classifier F1: {embed_f1:.4f}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acpiODYAbyiH"
      },
      "source": "Compare the two approaches and show example predictions:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKZ7v0nvbyiH",
        "outputId": "ea37226c-2749-45ef-c812-b21d530dee54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [],
      "source": "print(\"\\nResults Summary\")\nprint(f\"Training samples: {train_size}\")\nprint(f\"Task-Specific F1: {task_f1:.4f}\")\nprint(f\"Embedding F1: {embed_f1:.4f}\")\nprint(f\"Difference: {embed_f1 - task_f1:+.4f}\")\n\nprint(\"\\nExample predictions:\")\nfor i in range(3):\n    print(f\"\\n'{test_subset['text'][i][:50]}...'\")\n    print(f\"Task-Specific: {y_pred_task[i]} | Embedding: {y_pred_embed[i]}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCWBW3qObyiH"
      },
      "source": "### Questions\n\n1. At what training size did embedding classifier match the task-specific model?\n\n2. Were there cases where one model was correct and the other wrong?\n\n3. Is 100 samples enough labeled data?\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKfJlPDtbyiI"
      },
      "source": "#### Medium Task 3: Confidence-Based Classifier with Uncertainty Handling\n\nIn production, refusing a prediction beats making a wrong one. Here's the key insight: when your model is uncertain, it should say \"I don't know\" rather than guessing. This creates a trade-off between coverage (how many predictions you make) and accuracy (how often you're right).\n\nTry this:\n- Run with threshold of 0.15 first\n- Test 0.05, 0.30, and 0.50 to see how the trade-off shifts\n- Check the uncertain cases (typically have hedging language)\n- Experiment with the alternative uncertainty measure"
    },
    {
      "cell_type": "code",
      "source": "from sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np",
      "metadata": {
        "id": "UrlfyjrabyiJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n\n# Reviews with varying levels of clarity\ntest_reviews = [\n    \"Absolutely fantastic! Best movie ever!\",           # Clear positive\n    \"Pretty good, I liked it\",                          # Weak positive\n    \"It was fine, nothing special\",                     # Ambiguous\n    \"Not bad but not great either\",                     # Very ambiguous\n    \"Quite disappointing\",                              # Weak negative\n    \"Terrible! Complete waste of time!\",                # Clear negative\n    \"The movie had some interesting moments\",           # Ambiguous positive\n    \"Outstanding performances all around!\",             # Clear positive\n]\n\n# True labels (for evaluation)\ny_true = [1, 1, 0, 0, 0, 0, 1, 1]  # 1=positive, 0=negative",
      "metadata": {
        "id": "9sf1K6sibyiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "labels = [\"negative\", \"positive\"]\n\n# Experiment WITH THIS - Try: 0.05, 0.15, 0.30, 0.50\nconfidence_threshold = 0.15",
      "metadata": {
        "id": "jLZjsRNVbyiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def calculate_margin(similarities):\n    \"\"\"\n    Margin = difference between top two predictions\n    Small margin = uncertain (predictions are close)\n    \"\"\"\n    sorted_sims = np.sort(similarities)[::-1]\n    margin = sorted_sims[0] - sorted_sims[1]\n    return margin\n\n# After first run, uncomment this alternative uncertainty measure:\n# def calculate_margin(similarities):\n#     \"\"\"\n#     Alternative: Use absolute confidence in top prediction\n#     Low confidence = uncertain\n#     \"\"\"\n#     max_confidence = np.max(similarities)\n#     # Convert to margin-like score (higher = more certain)\n#     # If max is 0.6, margin = 0.6 - 0.5 = 0.1 (uncertain)\n#     # If max is 0.9, margin = 0.9 - 0.5 = 0.4 (certain)\n#     margin = max_confidence - 0.5\n#     return margin",
      "metadata": {
        "id": "Ss-J6mVsbyiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "label_embeddings = model.encode(labels)\nreview_embeddings = model.encode(test_reviews)\nsim_matrix = cosine_similarity(review_embeddings, label_embeddings)",
      "metadata": {
        "id": "htw33fKTbyiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Classify with confidence threshold\nresults = []\n\nprint(f\"Confidence-based classification (threshold={confidence_threshold})\")\n\nfor i, review in enumerate(test_reviews):\n    similarities = sim_matrix[i]\n    predicted_idx = np.argmax(similarities)\n    margin = calculate_margin(similarities)\n    \n    # Predict only if margin is above threshold\n    if margin >= confidence_threshold:\n        prediction = predicted_idx\n    else:\n        prediction = None\n    \n    pred_label = labels[predicted_idx] if prediction is not None else \"uncertain\"\n    print(f\"{i+1}. '{review[:40]}...' -> {pred_label} (margin: {margin:.3f})\")\n    \n    results.append({'pred': prediction, 'true': y_true[i], 'margin': margin})",
      "metadata": {
        "id": "oDGa7ACXbyiL",
        "outputId": "1efd9f11-2ffb-4e0b-e9d4-facdca0d0599",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Calculate metrics\nmade_predictions = [r for r in results if r['pred'] is not None]\ncorrect = [r for r in made_predictions if r['pred'] == r['true']]\n\ncoverage = len(made_predictions) / len(results)\naccuracy = len(correct) / len(made_predictions) if made_predictions else 0\n\nprint(f\"\\nPerformance:\")\nprint(f\"Coverage: {len(made_predictions)}/{len(results)} = {coverage:.1%}\")\nprint(f\"Accuracy: {len(correct)}/{len(made_predictions)} = {accuracy:.1%}\")",
      "metadata": {
        "id": "Oc38LloRbyiM",
        "outputId": "4e586ec6-6114-4736-85d9-7e44561c9331",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Questions\n\n1. What do uncertain reviews have in common? Are they using hedging language like \"kind of\" or \"somewhat\"?\n\n2. Compare results at threshold=0.05 vs 0.30. Describe the coverage vs accuracy trade-off. When would you want high coverage vs high accuracy?\n\n3. How could you use confidence-based prediction in production? What should a system do when the model is uncertain?",
      "metadata": {
        "id": "H7-OE7H9F6vx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDEBU0cLbyiN"
      },
      "source": "#### Medium Task 4: Classifier Failure Analysis\n\nTrain the classifier and see what kinds of reviews it gets wrong. Then add your own test cases.\n"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UDR7v_0byiN"
      },
      "source": "from sentence_transformers import SentenceTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom datasets import load_dataset\nimport numpy as np",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgPRrIxLbyiO"
      },
      "source": "Load data and train a classifier."
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkW9_JNLbyiO",
        "outputId": "d55ce6e5-4b4a-473c-acf5-32189814b740",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": "data = load_dataset(\"rotten_tomatoes\")\nmodel = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n\ntrain_subset = data[\"train\"].shuffle(seed=42).select(range(1000))\ntest_subset = data[\"test\"].shuffle(seed=42).select(range(200))\n\nprint(\"Training classifier...\")\ntrain_embeddings = model.encode(train_subset[\"text\"], show_progress_bar=False)\ntest_embeddings = model.encode(test_subset[\"text\"], show_progress_bar=False)\n\nclf = LogisticRegression(random_state=42, max_iter=1000)\nclf.fit(train_embeddings, train_subset[\"label\"])\n\npredictions = clf.predict(test_embeddings)\nprobabilities = clf.predict_proba(test_embeddings)",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wm3p9MfHbyiO"
      },
      "source": "Analyze the errors."
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpSZkDVobyiP",
        "outputId": "13daf6ba-a1bf-4c0b-a55d-17c092799c25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": "# Find and collect errors\nerrors = []\n\nfor i in range(len(test_subset)):\n    if predictions[i] != test_subset[\"label\"][i]:\n        errors.append({\n            'text': test_subset[\"text\"][i],\n            'true_label': test_subset[\"label\"][i],\n            'predicted_label': predictions[i],\n            'confidence': probabilities[i][predictions[i]]\n        })\n\naccuracy = (len(test_subset) - len(errors)) / len(test_subset)\nprint(f\"Accuracy: {accuracy:.1%}\")\nprint(f\"Errors: {len(errors)}\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOX4A0LtbyiP"
      },
      "source": "Look at high-confidence errors (the most surprising mistakes)."
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kY0cgAP-byiP",
        "outputId": "0e5c361d-05c0-4e1d-ecfc-2f5909d9523b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": "# Look at high-confidence errors\nhigh_conf_errors = [e for e in errors if e['confidence'] > 0.7]\n\nprint(\"High-confidence errors:\")\nfor error in high_conf_errors[:5]:\n    true_sent = \"positive\" if error['true_label'] == 1 else \"negative\"\n    pred_sent = \"positive\" if error['predicted_label'] == 1 else \"negative\"\n    print(f\"\\n'{error['text'][:50]}...'\")\n    print(f\"True: {true_sent} | Predicted: {pred_sent} (conf: {error['confidence']:.3f})\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQmT2SO8byiQ"
      },
      "source": "Test on edge cases like sarcasm and mixed sentiment."
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbH3zcFMbyiQ",
        "outputId": "d26bed3a-ac6a-413c-d7b5-18b6080e1992",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": "edge_cases = [\n    (\"Sarcastic\", \"Oh great, another masterpiece. not!\", 0),\n    (\"Mixed\", \"The acting was great but the plot was terrible\", 0),\n    (\"Ambiguous\", \"It was a movie\", 0),\n]\n\nedge_embeddings = model.encode([text for _, text, _ in edge_cases])\nedge_predictions = clf.predict(edge_embeddings)\n\nprint(\"\\nEdge Cases:\")\ncorrect = 0\nfor i, (category, text, true_label) in enumerate(edge_cases):\n    pred = edge_predictions[i]\n    match = \"correct\" if pred == true_label else \"wrong\"\n    if pred == true_label:\n        correct += 1\n    print(f\"{category}: '{text}' -> {pred} ({match})\")\n\nprint(f\"\\nEdge case accuracy: {correct}/{len(edge_cases)}\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgJCrepxbyiR"
      },
      "source": "### Questions\n\n1. What do high-confidence errors have in common?\n\n2. Do errors tend to be shorter or longer than correct predictions?\n\n3. Which edge cases failed most - sarcasm, mixed sentiment, or double negatives?\n"
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "id": "0yNuuGK5cFj7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}