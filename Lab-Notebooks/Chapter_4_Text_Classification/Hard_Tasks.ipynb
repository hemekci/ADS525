{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Text Classification - Hard Tasks\n",
    "\n",
    "This notebook tackles advanced classification challenges. You'll implement hierarchical multi-level classifiers for complex taxonomies, use active learning to minimize labeling costs, build ensemble classifiers to improve robustness, and apply transfer learning across domains. These techniques are essential for production-level NLP systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "Run all cells in this section to set up the environment and load necessary data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LGW2SD-c864"
   },
   "source": [
    "### [OPTIONAL] - Installing Packages on <img src=\"https://colab.google/static/images/icons/colab.png\" width=100>\n",
    "\n",
    "\n",
    "If you are viewing this notebook on Google Colab (or any other cloud vendor), you need to **uncomment and run** the following codeblock to install the dependencies for this chapter:\n",
    "\n",
    "---\n",
    "\n",
    " **NOTE**: We will want to use a GPU to run the examples in this notebook. In Google Colab, go to\n",
    "**Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4**.\n",
    "\n",
    "---\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "N-PxmOIhc865"
   },
   "outputs": [],
   "source": [
    " %%capture\n",
    "!pip install transformers sentence-transformers openai\n",
    "!pip install -U datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 784,
     "referenced_widgets": [
      "169816892c8646e3888f213295349f00",
      "4a590f6ceb104873b97b9620d6107017",
      "2d14e5528bbd446e8a34c235201f88ac",
      "f4c9e362c7ff40559a1a5632a8b6e907",
      "3c4c633f0af84d099ffc74bac8901b07",
      "35226b53943b4db8a5d46aae09720818",
      "1a6faf9a5ed748f0809834cc52435b3d",
      "4fe179f7fab44513aeaa33cedf44f1bb",
      "5befa9362a09459695f4489c7173c34d",
      "458ad67bb385497889eab5bbdd977de5",
      "027a7c3f8a9e4114ac6d4e64fb62d440",
      "0cce924a44f94cbfaa69a78735ece1d5",
      "28ac2aeffcb6424f97fb47bfaee8e66f",
      "ab02cefe39774db3844d914b65a48790",
      "6bf6e7d710eb4924a3144e6e5d0679ca",
      "b7b774839b134ee687d9d89a0e15d166",
      "c583fda17dd44217875e86209fbad80a",
      "cd4eeedee4fc4f28acf650785bbaa5f3",
      "e5138fd5028f48f9ad8eea2b41b1592a",
      "82f4409e3f854415a871e3da31e9393e",
      "aeb337caa109443a9fc4f93e9f92753d",
      "e7e6399dec234052b7f30e3135a21b00",
      "44a764c1267042259b2c9597a69f6f76",
      "05e8f57e23ff47f0be24d3f9c18554d4",
      "616321e6e87a4d8589d9ed939e19c7b2",
      "8d7284d945004b98b28b6c631a2c4726",
      "97d864e9c07149aeb22a844ced3909d5",
      "dbadb6dfe23141ec8f38ffb747ea882e",
      "82d0c0a18606421f92ed77d8d4c61e10",
      "0c64a3bc4d4d461cbdc3c3c0acae2f94",
      "cf707e36593b4071b8253fd33d18b665",
      "079412e5345a4a09875ed0c86dde93e3",
      "1e2ffd31336946cf952abb5f62651c16",
      "8466edc3103c45d8a5d3fe8166508648",
      "f8d0d8f24551457e862d2e041f3699ab",
      "426852cccf704c58bdc7decbb1c583e6",
      "6b2663befc574a8e8154631c3ac95ff3",
      "091bd22cb6d94ae5b7c962d18e528ca2",
      "5ba5883cc2144929b9d7ddd980e4508d",
      "971d17ca015446b4bbf8a908835295cd",
      "c7086b545eae4b09af3b2d77b78af4b9",
      "5f47aeddde3449a5b5ed280d0ec9bc18",
      "43791444ad3e49f993d17887e6c15dbf",
      "c2c3403eb38b420cba7390d385f325be",
      "9097dd47971f445ea0fd55e7c42c15a0",
      "18cbf6cbaa4c436d9074224f83472863",
      "831180b810da4b549a8ca75536607e31",
      "d9779ccf530e49b1ac52172252ec5ef4",
      "78d69219fad44722a9bad3a12a3ccb61",
      "f08fa19e07644f4d9fff30d02040e315",
      "e1a2a1e765f042a7b9b081688688dab5",
      "76d299a97ab54dbe96f4525095afedcb",
      "73ac94bdcaf8427bbbcdd808f1428dff",
      "6f9a5d0dfe2f4aa49c57e942b927521b",
      "2d7810d299f649a2ac84541e7215d29a",
      "40dc8a185cf34e99a427f7c81bc76540",
      "bcac3e31740e4a2894b4e8bc9656a0c5",
      "87f29bce5b5d4df08b5337d62ffa9568",
      "3129b232f1e3411ead0d913654a11eaa",
      "4e7c3e87552640018f89c6cf9e070642",
      "01ccc633ad5d4350ad04651e360cc478",
      "735737e7c1eb477fae9d06d7324e81e8",
      "882b69fb84034d2dace625ef267f51cb",
      "8c77be8e5b74452bb028b97fd9edba36",
      "55a6296e28394f41b2a6d1f3d76e5944",
      "8f6c7a732bb6498db1de4d66f4b3c623",
      "22541d211626493e87c168143671e5ce",
      "111b9c6182994e2e85a323d07980b1ab",
      "3c53665ac233434fa899399da13650f0",
      "c4867626e6eb41bfaade350790de9f40",
      "a11866f208c4411dbc3d627ef6dbd74f",
      "3485c0ebc7f24965a0fadd3a6570d51e",
      "d111b1f753554eeda57cdbe420335fda",
      "8fb77e0530e946d38c007c9012c37a79",
      "8d669f0719d04980a6f04ffc2f65cf7c",
      "d79a649275a14fdeb2cb01e5ab75021f",
      "14186c7dd75e43d99183d5c8b308c0de"
     ]
    },
    "id": "5phRS_z2U_3T",
    "outputId": "e51c9c23-a48a-4c61-e7ee-cd0ef03915a4"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "169816892c8646e3888f213295349f00"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "train.parquet:   0%|          | 0.00/699k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0cce924a44f94cbfaa69a78735ece1d5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "validation.parquet:   0%|          | 0.00/90.0k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "44a764c1267042259b2c9597a69f6f76"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "test.parquet:   0%|          | 0.00/92.2k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8466edc3103c45d8a5d3fe8166508648"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9097dd47971f445ea0fd55e7c42c15a0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating validation split:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "40dc8a185cf34e99a427f7c81bc76540"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating test split:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "22541d211626493e87c168143671e5ce"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 8530\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": "from datasets import load_dataset\n# Load our data\ndata = load_dataset(\"rotten_tomatoes\")\ndata"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "X0KyKHtqyjn3"
   },
   "outputs": [],
   "source": "from sklearn.metrics import classification_report\ndef evaluate_performance(y_true, y_pred):\n    \"\"\"Create and print the classification report\"\"\"\n    performance = classification_report(\n        y_true, y_pred,\n        target_names=[\"Negative Review\", \"Positive Review\"]\n    )\n    print(performance)"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Your Turn - Text Classification Experiments\n",
    "\n",
    "Run each task first to see the baseline results. Follow the instructions to modify and experiment."
   ],
   "metadata": {
    "id": "NKYNfoaVC4hU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This section is divided into EASY, MEDIUM, & HARD."
   ],
   "metadata": {
    "id": "hHVONn85DElL"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Hard Tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hard Tasks - Advanced Classification Challenges\n",
    "\n",
    "These tasks require significant modifications and deeper understanding. Take your time and experiment"
   ],
   "metadata": {
    "id": "sOcbrBwdGZbi"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hard Task 1: Hierarchical Multi-Level Classifier\nInstead of flat classification (choosing from all categories at once), hierarchical classification makes decisions in steps: first broad categories, then fine-grained ones. This mirrors how humans often reason.\nInstructions:\n1. Run the 2-level classifier (Sentiment \u2192 Specific Aspect)\n2. Compare with flat classification to see confidence differences\n3. Try adding a 3rd level by uncommenting the code\n4. Analyze whether breaking decisions into steps helps or hurts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Level 1: Broad sentiment\nlevel1_labels = [\n    \"negative sentiment review\",\n    \"positive sentiment review\"\n]\n# Level 2: Specific aspects (conditional on Level 1)\nlevel2_negative = [\n    \"review criticizing entertainment value and pacing\",\n    \"review criticizing technical quality and production\"\n]\nlevel2_positive = [\n    \"review praising technical quality and artistry\",\n    \"review praising entertainment value and enjoyment\"\n]\n# TODO: Add Level 3 for even finer granularity\n# level3_positive_quality = [...]\n# level3_positive_entertainment = [...]"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "test_reviews = [\n",
    "    \"Amazing cinematography and brilliant direction!\",\n",
    "    \"Terrible pacing, very boring throughout\",\n",
    "    \"Excellent acting but weak storyline\",\n",
    "    \"Poor production quality, disappointing visuals\"\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def hierarchical_classify_2level(review):\n",
    "    \"\"\"\n",
    "    Classify review using 2-level hierarchy:\n",
    "    Level 1: Sentiment (positive/negative)\n",
    "    Level 2: Specific aspect (quality/entertainment)\n",
    "    \"\"\"\n",
    "    # Level 1: Determine sentiment\n",
    "    level1_emb = model.encode(level1_labels)\n",
    "    review_emb = model.encode([review])\n",
    "    level1_sim = cosine_similarity(review_emb, level1_emb)[0]\n",
    "    \n",
    "    level1_pred = np.argmax(level1_sim)\n",
    "    level1_conf = level1_sim[level1_pred]\n",
    "    level1_label = level1_labels[level1_pred]\n",
    "    \n",
    "    # Level 2: Conditional on Level 1\n",
    "    if level1_pred == 0:  # Negative\n",
    "        level2_labels = level2_negative\n",
    "        path = \"Negative \u2192 \"\n",
    "    else:  # Positive\n",
    "        level2_labels = level2_positive\n",
    "        path = \"Positive \u2192 \"\n",
    "    \n",
    "    level2_emb = model.encode(level2_labels)\n",
    "    level2_sim = cosine_similarity(review_emb, level2_emb)[0]\n",
    "    \n",
    "    level2_pred = np.argmax(level2_sim)\n",
    "    level2_conf = level2_sim[level2_pred]\n",
    "    level2_label = level2_labels[level2_pred]\n",
    "    path += level2_label\n",
    "    \n",
    "    return {\n",
    "        \"level1_label\": level1_label,\n",
    "        \"level1_conf\": level1_conf,\n",
    "        \"level2_label\": level2_label,\n",
    "        \"level2_conf\": level2_conf,\n",
    "        \"path\": path\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Classification function:\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Hard Task 2: Active Learning to Minimize Labeling Costs\nLabeling data is expensive. Active learning strategically selects the most informative samples to label, potentially saving 50%+ of labeling effort compared to random selection. Run the simulation to see active learning vs random sampling, observe which samples the model finds \"uncertain\", and track how many samples each approach needs to reach F1=0.85. Try implementing the alternative selection strategy (margin sampling) and compare labeling cost savings."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Simulate active learning vs random sampling\n",
    "print(\"ACTIVE LEARNING SIMULATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Setup\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "train_data = data[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "test_data = data[\"test\"].shuffle(seed=42).select(range(200))\n",
    "\n",
    "# Start with small labeled set\n",
    "labeled_size = 50\n",
    "target_f1 = 0.85\n",
    "\n",
    "print(f\"Starting with {labeled_size} labeled samples\")\n",
    "print(f\"Target F1: {target_f1}\")\n",
    "print(f\"\\nNote: This is a simplified simulation\")\n",
    "print(\"In real active learning, you'd query an oracle (human) for labels\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Classify the reviews:\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"HIERARCHICAL CLASSIFICATION (2 LEVELS)\")\nfor i, review in enumerate(test_reviews):\n    result = hierarchical_classify_2level(review)\n    print(f\"\\nReview {i+1}: '{review}'\")\n    print(f\"\\n  Level 1 (Sentiment):\")\n    print(f\"     {result['level1_label']}\")\n    print(f\"     Confidence: {result['level1_conf']:.3f}\")\n    print(f\"\\n  Level 2 (Specific Aspect):\")\n    print(f\"     {result['level2_label']}\")\n    print(f\"     Confidence: {result['level2_conf']:.3f}\")\n    print(f\"\\n  Final Classification Path:\")\n    print(f\"     {result['path']}\")\n# Compare with flat classification\nprint()\nprint(\"COMPARISON: Hierarchical vs Flat Classification\")\n# Flat: All 4 categories at once\nflat_labels = [\n    \"review criticizing entertainment value and pacing\",      # 0\n    \"review criticizing technical quality and production\",    # 1\n    \"review praising technical quality and artistry\",         # 2\n    \"review praising entertainment value and enjoyment\"       # 3\n]\nflat_embeddings = model.encode(flat_labels)\nreview_embeddings = model.encode(test_reviews)\nflat_sim = cosine_similarity(review_embeddings, flat_embeddings)\nprint(\"\\nShowing first 3 reviews:\")\nfor i in range(min(3, len(test_reviews))):\n    hier_result = hierarchical_classify_2level(test_reviews[i])\n    flat_pred = np.argmax(flat_sim[i])\n    flat_conf = flat_sim[i][flat_pred]\n    print(f\"\\nReview: '{test_reviews[i][:50]}...'\")\n    print(f\"  Hierarchical: {hier_result['level2_label']}\")\n    print(f\"     Confidence: {hier_result['level2_conf']:.3f}\")\n    print(f\"  Flat:         {flat_labels[flat_pred]}\")\n    print(f\"     Confidence: {flat_conf:.3f}\")\n    print(f\"  Confidence Diff: {hier_result['level2_conf'] - flat_conf:+.3f}\")\n# TODO: After implementing 3-level, uncomment to test it:\n# print()\n# print(\"TESTING 3-LEVEL HIERARCHICAL CLASSIFICATION\")\n# for i, review in enumerate(test_reviews):\n#     result = hierarchical_classify_3level(review)\n#     print(f\"\\n{i+1}. '{review[:60]}...'\")\n#     print(f\"   Path: {result['path']}\")\n#     print(f\"   Level 3 confidence: {result['level3_conf']:.3f}\")"
  },
  {
   "cell_type": "markdown",
   "source": [
    "the hierarchical approach makes decisions in stages:\n1. Level 1: Determines if the review is positive or negative\n2. Level 2: Based on that sentiment, classifies the specific aspect\n\nThe confidence scores at each level. The hierarchical classifier's final confidence is often higher than flat classification because it's making simpler decisions at each step.\n\nThe comparison** between hierarchical and flat approaches, you may notice:\n- Hierarchical often has higher confidence (makes easier per-step decisions)\n- But if Level 1 is wrong, Level 2 has no chance to correct it\n- Flat classification considers all options at once but may be less confident"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Hard Task 2: Active Learning to Minimize Labeling\n\nCompare active learning (picking uncertain samples) vs random selection.\n"
  },
  {
   "cell_type": "markdown",
   "source": [
    "active learning iteratively:\n1. Trains on currently labeled data\n2. Finds the most uncertain unlabeled samples  \n3. \"Labels\" those samples (adds them to training set)\n4. Repeats\n\nThe uncertain samples being selected - they typically have prediction probabilities close to 50-50. These are the most informative because they lie near the decision boundary.\n\nThe learning curves,** compare how quickly each approach improves. Active learning often reaches target performance with fewer labeled samples, saving significant labeling costs."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Hard Task 3: Ensemble Classifier for Improved Robustness\n\nEnsemble methods combine multiple models to reduce individual biases and improve reliability. The wisdom of crowds principle: multiple imperfect models together often beat any single model.\n\nInstructions:\n1. Run to see 3 individual models compared to ensemble methods\n2. Compare simple majority voting vs confidence-weighted voting\n3. Examine disagreement cases - when models disagree, which is usually right?\n4. Optionally add a 4th model and performance-weighted voting\n5. Determine if the ensemble beats the best individual model"
   ],
   "metadata": {
    "id": "yzYVwXTBHOrA"
   }
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "print(\"ENSEMBLE CLASSIFIER COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load 3 different models\n",
    "model1 = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "model2 = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model3 = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L3-v2')\n",
    "\n",
    "models = [model1, model2, model3]\n",
    "model_names = [\"MPNet\", \"MiniLM-L6\", \"MiniLM-L3\"]\n",
    "\n",
    "print(f\"Using {len(models)} models:\\n\")\n",
    "for name in model_names:\n",
    "    print(f\"  - {name}\")\n",
    "\n",
    "# Train each model\n",
    "train_subset = data[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "test_subset = data[\"test\"].shuffle(seed=42).select(range(200))\n",
    "\n",
    "print(f\"\\nTraining on {len(train_subset)} samples...\")\n",
    "print(\"Note: In practice, ensemble diversity comes from:\")\n",
    "print(\"  - Different model architectures\")\n",
    "print(\"  - Different training data subsets\")\n",
    "print(\"  - Different hyperparameters\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "Looking at the voting patterns, most samples have unanimous agreement (all models concur) - these are easy cases. The interesting cases are disagreements where models learned different patterns. Notice the ensemble performance: sometimes it beats all individual models by leveraging their diverse strengths.",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "e90e7c8a9ae641f9a5f2c9ab66248dbc",
      "e3af2532ddea450095ef34e3c7890344",
      "220d3bb538d84e23ba45bdb1ef20e68c",
      "be25c443c44c428eb271c02c2e34578a",
      "beb2fb51e5714276808eba4b576907e3",
      "8522a39f750149109702719a81a46bb0",
      "9d04edae2e034fb39a681ea8db1f79d8",
      "e97f097bfe454537b714d50cc3e1923e",
      "6fd1de03fef54dffba7b998edeac0776",
      "2d7a4067755849cd9cb94accfed4d9c0",
      "eb3cbb80ef9a42a8b31bf282cc426aeb"
     ]
    },
    "id": "kWDdM6kBHHr9",
    "outputId": "2f7082bc-801f-4d8d-9f17-78ae80b0cdce"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "the classifier trained on movies transfers to other domains with varying success:\n\nZero-shot transfer results show how well the model generalizes without any target domain examples. Domains closer to movies (like books) often transfer better than very different domains.\n\nFew-shot adaptation demonstrates the power of adding just a handful of target domain examples. Even 4 labeled samples can significantly improve performance.\n\nThe domain similarity analysis,** embedding distance correlates with transfer performance. Domains with smaller embedding distance from movies tend to have better zero-shot performance. This helps you predict which domains will transfer well before running experiments."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Hard Task 4: Cross-Domain Transfer Learning\n\nCan a classifier trained on movie reviews work on restaurant, product, or book reviews? Observe zero-shot transfer (no adaptation) performance on each domain, see which domains transfer well and which don't, then try few-shot adaptation (adding just 4 examples from target domain). Analyze domain similarity using embedding distances and optionally add your own custom domain to test."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dependencies and set up source/target domains:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define source domain (movies) and target domains:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "movie_data = load_dataset(\"rotten_tomatoes\")\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "movie_train = movie_data[\"train\"].shuffle(seed=42).select(range(2000))\n",
    "movie_test = movie_data[\"test\"].shuffle(seed=42).select(range(200))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "restaurant_reviews = {\n",
    "    'text': [\n",
    "        \"Amazing food and excellent service!\",\n",
    "        \"Best restaurant in town, highly recommend\",\n",
    "        \"Delicious meals and great atmosphere\",\n",
    "        \"Outstanding cuisine and friendly staff\",\n",
    "        \"Terrible food, very disappointing\",\n",
    "        \"Awful service and poor quality\",\n",
    "        \"Not worth the money, mediocre at best\",\n",
    "        \"Disgusting food and rude waiters\",\n",
    "        \"The pasta was okay but nothing special\",\n",
    "        \"Decent place for a quick meal\"\n",
    "    ],\n",
    "    'label': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
    "}\n",
    "\n",
    "product_reviews = {\n",
    "    'text': [\n",
    "        \"This product is amazing! Works perfectly\",\n",
    "        \"Excellent quality, very satisfied\",\n",
    "        \"Great value for money, highly recommend\",\n",
    "        \"Perfect! Exactly what I needed\",\n",
    "        \"Terrible product, broke immediately\",\n",
    "        \"Waste of money, very poor quality\",\n",
    "        \"Doesn't work as advertised, disappointed\",\n",
    "        \"Awful, don't buy this\",\n",
    "        \"It's okay, does the job\",\n",
    "        \"Average product, nothing special\"\n",
    "    ],\n",
    "    'label': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
    "}\n",
    "\n",
    "book_reviews = {\n",
    "    'text': [\n",
    "        \"Brilliant book! Couldn't put it down\",\n",
    "        \"Masterfully written, highly engaging\",\n",
    "        \"One of the best books I've read\",\n",
    "        \"Fantastic story and great characters\",\n",
    "        \"Boring and poorly written\",\n",
    "        \"Terrible book, waste of time\",\n",
    "        \"Disappointing, not worth reading\",\n",
    "        \"Awful plot and weak characters\",\n",
    "        \"Decent read but nothing groundbreaking\",\n",
    "        \"It was fine, not great not terrible\"\n",
    "    ],\n",
    "    'label': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train classifier on source domain (movies):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Training on movie reviews...\")\n",
    "train_embeddings = model.encode(movie_train[\"text\"], show_progress_bar=True)\n",
    "\n",
    "clf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "clf.fit(train_embeddings, movie_train[\"label\"])\n",
    "\n",
    "# Test on source domain\n",
    "movie_test_embeddings = model.encode(movie_test[\"text\"], show_progress_bar=False)\n",
    "movie_test_pred = clf.predict(movie_test_embeddings)\n",
    "source_f1 = f1_score(movie_test[\"label\"], movie_test_pred, average='weighted')\n",
    "\n",
    "print(f\"\\nSource domain F1: {source_f1:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test zero-shot transfer to target domains (no adaptation):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "target_domains = {\n",
    "    \"Restaurant\": restaurant_reviews,\n",
    "    \"Product\": product_reviews,\n",
    "    \"Book\": book_reviews,\n",
    "}\n",
    "\n",
    "transfer_results = {}\n",
    "\n",
    "print(\"\\nZero-shot transfer results:\")\n",
    "for domain_name, domain_data in target_domains.items():\n",
    "    domain_embeddings = model.encode(domain_data['text'])\n",
    "    domain_pred = clf.predict(domain_embeddings)\n",
    "    domain_f1 = f1_score(domain_data['label'], domain_pred, average='weighted')\n",
    "    domain_acc = accuracy_score(domain_data['label'], domain_pred)\n",
    "    \n",
    "    print(f\"\\n{domain_name}: F1={domain_f1:.4f}, Acc={domain_acc:.4f}\")\n",
    "    print(f\"  Drop from source: {source_f1 - domain_f1:.4f}\")\n",
    "    \n",
    "    transfer_results[domain_name] = {\n",
    "        'zero_shot_f1': domain_f1,\n",
    "        'embeddings': domain_embeddings\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few-shot adaptation (add 4 examples from target domain):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "adaptation_size = 4\n",
    "\n",
    "print(\"\\nFew-shot adaptation (4 examples):\")\n",
    "for domain_name, domain_data in target_domains.items():\n",
    "    # Split: first 4 for adaptation, rest for testing\n",
    "    adapt_texts = domain_data['text'][:adaptation_size]\n",
    "    adapt_labels = domain_data['label'][:adaptation_size]\n",
    "    test_texts = domain_data['text'][adaptation_size:]\n",
    "    test_labels = domain_data['label'][adaptation_size:]\n",
    "    \n",
    "    # Combine source + adaptation\n",
    "    adapt_embeddings = model.encode(adapt_texts)\n",
    "    combined_embeddings = np.vstack([train_embeddings, adapt_embeddings])\n",
    "    combined_labels = list(movie_train[\"label\"]) + adapt_labels\n",
    "    \n",
    "    # Retrain\n",
    "    clf_adapted = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    clf_adapted.fit(combined_embeddings, combined_labels)\n",
    "    \n",
    "    # Test\n",
    "    test_embeddings = model.encode(test_texts)\n",
    "    adapted_pred = clf_adapted.predict(test_embeddings)\n",
    "    adapted_f1 = f1_score(test_labels, adapted_pred, average='weighted')\n",
    "    \n",
    "    zero_shot_f1 = transfer_results[domain_name]['zero_shot_f1']\n",
    "    improvement = adapted_f1 - zero_shot_f1\n",
    "    \n",
    "    print(f\"\\n{domain_name}:\")\n",
    "    print(f\"  Zero-shot: {zero_shot_f1:.4f}\")\n",
    "    print(f\"  Adapted:   {adapted_f1:.4f}\")\n",
    "    print(f\"  Gain:      {improvement:+.4f}\")\n",
    "    \n",
    "    transfer_results[domain_name]['adapted_f1'] = adapted_f1\n",
    "    transfer_results[domain_name]['improvement'] = improvement"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze domain similarity (embedding distance):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "source_centroid = np.mean(train_embeddings, axis=0)\n",
    "\n",
    "print(\"\\nDomain distances from source:\")\n",
    "for domain_name in target_domains.keys():\n",
    "    domain_embeddings = transfer_results[domain_name]['embeddings']\n",
    "    domain_centroid = np.mean(domain_embeddings, axis=0)\n",
    "    distance = np.linalg.norm(source_centroid - domain_centroid)\n",
    "    \n",
    "    zero_f1 = transfer_results[domain_name]['zero_shot_f1']\n",
    "    drop = source_f1 - zero_f1\n",
    "    \n",
    "    print(f\"\\n{domain_name:12s}: distance={distance:.3f}, drop={drop:.3f}\")\n",
    "\n",
    "print(\"\\nObservation: Smaller distance = better transfer\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Questions\n\n1. Which domain transferred best from movies? Which worst?\n\n2. Do you see patterns in what transfers well vs what fails?\n\n3. After few-shot adaptation: Which domains benefited most from just 4 examples?\n"
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "yZLqm3S4IP2e"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}