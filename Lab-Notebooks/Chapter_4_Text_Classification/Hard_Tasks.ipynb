{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Text Classification - Hard Tasks\n",
    "\n",
    "This notebook tackles advanced classification challenges. You'll implement hierarchical multi-level classifiers for complex taxonomies, use active learning to minimize labeling costs, build ensemble classifiers to improve robustness, and apply transfer learning across domains. These techniques are essential for production-level NLP systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "Run all cells in this section to set up the environment and load necessary data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LGW2SD-c864"
   },
   "source": [
    "### [OPTIONAL] - Installing Packages on <img src=\"https://colab.google/static/images/icons/colab.png\" width=100>\n",
    "\n",
    "\n",
    "If you are viewing this notebook on Google Colab (or any other cloud vendor), you need to **uncomment and run** the following codeblock to install the dependencies for this chapter:\n",
    "\n",
    "---\n",
    "\n",
    " **NOTE**: We will want to use a GPU to run the examples in this notebook. In Google Colab, go to\n",
    "**Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4**.\n",
    "\n",
    "---\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "N-PxmOIhc865"
   },
   "outputs": [],
   "source": [
    " %%capture\n",
    "!pip install transformers sentence-transformers openai\n",
    "!pip install -U datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 784,
     "referenced_widgets": [
      "169816892c8646e3888f213295349f00",
      "4a590f6ceb104873b97b9620d6107017",
      "2d14e5528bbd446e8a34c235201f88ac",
      "f4c9e362c7ff40559a1a5632a8b6e907",
      "3c4c633f0af84d099ffc74bac8901b07",
      "35226b53943b4db8a5d46aae09720818",
      "1a6faf9a5ed748f0809834cc52435b3d",
      "4fe179f7fab44513aeaa33cedf44f1bb",
      "5befa9362a09459695f4489c7173c34d",
      "458ad67bb385497889eab5bbdd977de5",
      "027a7c3f8a9e4114ac6d4e64fb62d440",
      "0cce924a44f94cbfaa69a78735ece1d5",
      "28ac2aeffcb6424f97fb47bfaee8e66f",
      "ab02cefe39774db3844d914b65a48790",
      "6bf6e7d710eb4924a3144e6e5d0679ca",
      "b7b774839b134ee687d9d89a0e15d166",
      "c583fda17dd44217875e86209fbad80a",
      "cd4eeedee4fc4f28acf650785bbaa5f3",
      "e5138fd5028f48f9ad8eea2b41b1592a",
      "82f4409e3f854415a871e3da31e9393e",
      "aeb337caa109443a9fc4f93e9f92753d",
      "e7e6399dec234052b7f30e3135a21b00",
      "44a764c1267042259b2c9597a69f6f76",
      "05e8f57e23ff47f0be24d3f9c18554d4",
      "616321e6e87a4d8589d9ed939e19c7b2",
      "8d7284d945004b98b28b6c631a2c4726",
      "97d864e9c07149aeb22a844ced3909d5",
      "dbadb6dfe23141ec8f38ffb747ea882e",
      "82d0c0a18606421f92ed77d8d4c61e10",
      "0c64a3bc4d4d461cbdc3c3c0acae2f94",
      "cf707e36593b4071b8253fd33d18b665",
      "079412e5345a4a09875ed0c86dde93e3",
      "1e2ffd31336946cf952abb5f62651c16",
      "8466edc3103c45d8a5d3fe8166508648",
      "f8d0d8f24551457e862d2e041f3699ab",
      "426852cccf704c58bdc7decbb1c583e6",
      "6b2663befc574a8e8154631c3ac95ff3",
      "091bd22cb6d94ae5b7c962d18e528ca2",
      "5ba5883cc2144929b9d7ddd980e4508d",
      "971d17ca015446b4bbf8a908835295cd",
      "c7086b545eae4b09af3b2d77b78af4b9",
      "5f47aeddde3449a5b5ed280d0ec9bc18",
      "43791444ad3e49f993d17887e6c15dbf",
      "c2c3403eb38b420cba7390d385f325be",
      "9097dd47971f445ea0fd55e7c42c15a0",
      "18cbf6cbaa4c436d9074224f83472863",
      "831180b810da4b549a8ca75536607e31",
      "d9779ccf530e49b1ac52172252ec5ef4",
      "78d69219fad44722a9bad3a12a3ccb61",
      "f08fa19e07644f4d9fff30d02040e315",
      "e1a2a1e765f042a7b9b081688688dab5",
      "76d299a97ab54dbe96f4525095afedcb",
      "73ac94bdcaf8427bbbcdd808f1428dff",
      "6f9a5d0dfe2f4aa49c57e942b927521b",
      "2d7810d299f649a2ac84541e7215d29a",
      "40dc8a185cf34e99a427f7c81bc76540",
      "bcac3e31740e4a2894b4e8bc9656a0c5",
      "87f29bce5b5d4df08b5337d62ffa9568",
      "3129b232f1e3411ead0d913654a11eaa",
      "4e7c3e87552640018f89c6cf9e070642",
      "01ccc633ad5d4350ad04651e360cc478",
      "735737e7c1eb477fae9d06d7324e81e8",
      "882b69fb84034d2dace625ef267f51cb",
      "8c77be8e5b74452bb028b97fd9edba36",
      "55a6296e28394f41b2a6d1f3d76e5944",
      "8f6c7a732bb6498db1de4d66f4b3c623",
      "22541d211626493e87c168143671e5ce",
      "111b9c6182994e2e85a323d07980b1ab",
      "3c53665ac233434fa899399da13650f0",
      "c4867626e6eb41bfaade350790de9f40",
      "a11866f208c4411dbc3d627ef6dbd74f",
      "3485c0ebc7f24965a0fadd3a6570d51e",
      "d111b1f753554eeda57cdbe420335fda",
      "8fb77e0530e946d38c007c9012c37a79",
      "8d669f0719d04980a6f04ffc2f65cf7c",
      "d79a649275a14fdeb2cb01e5ab75021f",
      "14186c7dd75e43d99183d5c8b308c0de"
     ]
    },
    "id": "5phRS_z2U_3T",
    "outputId": "e51c9c23-a48a-4c61-e7ee-cd0ef03915a4"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "169816892c8646e3888f213295349f00"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "train.parquet:   0%|          | 0.00/699k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0cce924a44f94cbfaa69a78735ece1d5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "validation.parquet:   0%|          | 0.00/90.0k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "44a764c1267042259b2c9597a69f6f76"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "test.parquet:   0%|          | 0.00/92.2k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8466edc3103c45d8a5d3fe8166508648"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9097dd47971f445ea0fd55e7c42c15a0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating validation split:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "40dc8a185cf34e99a427f7c81bc76540"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating test split:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "22541d211626493e87c168143671e5ce"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 8530\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load our data\n",
    "data = load_dataset(\"rotten_tomatoes\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "X0KyKHtqyjn3"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_performance(y_true, y_pred):\n",
    "    \"\"\"Create and print the classification report\"\"\"\n",
    "    performance = classification_report(\n",
    "        y_true, y_pred,\n",
    "        target_names=[\"Negative Review\", \"Positive Review\"]\n",
    "    )\n",
    "    print(performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Your Turn - Text Classification Experiments\n",
    "\n",
    "Run each task first to see the baseline results. Follow the instructions to modify and experiment."
   ],
   "metadata": {
    "id": "NKYNfoaVC4hU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This section is divided into EASY, MEDIUM, & HARD."
   ],
   "metadata": {
    "id": "hHVONn85DElL"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Hard Tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hard Tasks - Advanced Classification Challenges\n",
    "\n",
    "These tasks require significant modifications and deeper understanding. Take your time and experiment"
   ],
   "metadata": {
    "id": "sOcbrBwdGZbi"
   }
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": "#### Hard Task 1: Hierarchical Multi-Level Classifier\n\nRun the 2-level classifier first. Then try adding a 3rd level.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": "from sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\nmodel = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n\ntest_reviews = [\n    # Positive - Quality\n    \"Brilliant performances and stunning cinematography\",\n    \"Exceptional directing and beautiful visuals\",\n\n    # Positive - Entertainment\n    \"So much fun! Had a great time watching\",\n    \"Really entertaining and enjoyable\",\n\n    # Negative - Boring\n    \"Incredibly dull and slow-paced\",\n    \"Boring, nothing happens for two hours\",\n\n    # Negative - Quality issues\n    \"Poor acting and terrible script\",\n    \"Awful production values and bad directing\",\n]\n\n# TODO: Add reviews for 3rd level\n# test_reviews.extend([...])"
  },
  {
   "cell_type": "markdown",
   "id": "cell-14b",
   "metadata": {},
   "source": "Define the hierarchy:\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14c",
   "metadata": {},
   "outputs": [],
   "source": "# Level 1: Sentiment\nlevel1_labels = [\n    \"negative sentiment review\",\n    \"positive sentiment review\"\n]\n\n# Level 2: Aspects\nlevel2_negative = [\n    \"review criticizing entertainment value and pacing\",\n    \"review criticizing technical quality and production\"\n]\n\nlevel2_positive = [\n    \"review praising technical quality and artistry\",\n    \"review praising entertainment value and enjoyment\"\n]\n\n# TODO: Add Level 3\n# level3_positive_quality = [...]\n# level3_positive_entertainment = [...]"
  },
  {
   "cell_type": "markdown",
   "id": "cell-14d",
   "metadata": {},
   "source": "Classification function:\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14e",
   "metadata": {},
   "outputs": [],
   "source": "def hierarchical_classify_2level(text):\n    \"\"\"Two-level classification: Sentiment -> Aspect\"\"\"\n    text_embedding = model.encode([text])\n\n    # Level 1: Determine sentiment\n    level1_embeddings = model.encode(level1_labels)\n    level1_sim = cosine_similarity(text_embedding, level1_embeddings)[0]\n    level1_pred = np.argmax(level1_sim)\n    level1_conf = level1_sim[level1_pred]\n\n    # Level 2: Determine specific aspect based on Level 1\n    if level1_pred == 0:  # Negative\n        level2_labels = level2_negative\n        sentiment = \"Negative\"\n    else:  # Positive\n        level2_labels = level2_positive\n        sentiment = \"Positive\"\n\n    level2_embeddings = model.encode(level2_labels)\n    level2_sim = cosine_similarity(text_embedding, level2_embeddings)[0]\n    level2_pred = np.argmax(level2_sim)\n    level2_conf = level2_sim[level2_pred]\n\n    return {\n        'level1_pred': level1_pred,\n        'level1_label': level1_labels[level1_pred],\n        'level1_conf': level1_conf,\n        'level2_pred': level2_pred,\n        'level2_label': level2_labels[level2_pred],\n        'level2_conf': level2_conf,\n        'sentiment': sentiment,\n        'path': f\"{sentiment} -> {level2_labels[level2_pred]}\"\n    }\n\n# TODO: Uncomment to implement 3-level classification\n# def hierarchical_classify_3level(text):\n#     \"\"\"Three-level classification: Sentiment -> Aspect -> Specific\"\"\"\n#     # Start with levels 1 and 2\n#     result = hierarchical_classify_2level(text)\n#\n#     text_embedding = model.encode([text])\n#\n#     # Level 3: Even more specific based on Level 2\n#     if result['sentiment'] == \"Positive\":\n#         if result['level2_pred'] == 0:  # Quality\n#             level3_labels = level3_positive_quality\n#         else:  # Entertainment\n#             level3_labels = level3_positive_entertainment\n#     else:  # Negative\n#         if result['level2_pred'] == 0:  # Entertainment\n#             level3_labels = level3_negative_entertainment\n#         else:  # Quality\n#             level3_labels = level3_negative_quality\n#\n#     level3_embeddings = model.encode(level3_labels)\n#     level3_sim = cosine_similarity(text_embedding, level3_embeddings)[0]\n#     level3_pred = np.argmax(level3_sim)\n#     level3_conf = level3_sim[level3_pred]\n#\n#     result['level3_pred'] = level3_pred\n#     result['level3_label'] = level3_labels[level3_pred]\n#     result['level3_conf'] = level3_conf\n#     result['path'] = f\"{result['sentiment']} -> L2 -> {level3_labels[level3_pred]}\"\n#\n#     return result\n\n# TODO: Implement 3-level version\n# def hierarchical_classify_3level(text):\n#     ..."
  },
  {
   "cell_type": "markdown",
   "id": "cell-14f",
   "metadata": {},
   "source": "Classify the reviews:\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14g",
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*80)\nprint(\"HIERARCHICAL CLASSIFICATION (2 LEVELS)\")\nprint(\"=\"*80)\n\nfor i, review in enumerate(test_reviews):\n    result = hierarchical_classify_2level(review)\n\n    print(f\"\\nReview {i+1}: '{review}'\")\n    print(f\"\\n  Level 1 (Sentiment):\")\n    print(f\"     {result['level1_label']}\")\n    print(f\"     Confidence: {result['level1_conf']:.3f}\")\n\n    print(f\"\\n  Level 2 (Specific Aspect):\")\n    print(f\"     {result['level2_label']}\")\n    print(f\"     Confidence: {result['level2_conf']:.3f}\")\n\n    print(f\"\\n  Final Classification Path:\")\n    print(f\"     {result['path']}\")\n    print(\"-\"*80)\n\n# Compare with flat classification\nprint(\"\\n\" + \"=\"*80)\nprint(\"COMPARISON: Hierarchical vs Flat Classification\")\nprint(\"=\"*80)\n\n# Flat: All 4 categories at once\nflat_labels = [\n    \"review criticizing entertainment value and pacing\",      # 0\n    \"review criticizing technical quality and production\",    # 1\n    \"review praising technical quality and artistry\",         # 2\n    \"review praising entertainment value and enjoyment\"       # 3\n]\n\nflat_embeddings = model.encode(flat_labels)\nreview_embeddings = model.encode(test_reviews)\nflat_sim = cosine_similarity(review_embeddings, flat_embeddings)\n\nprint(\"\\nShowing first 3 reviews:\")\nfor i in range(min(3, len(test_reviews))):\n    hier_result = hierarchical_classify_2level(test_reviews[i])\n    flat_pred = np.argmax(flat_sim[i])\n    flat_conf = flat_sim[i][flat_pred]\n\n    print(f\"\\nReview: '{test_reviews[i][:50]}...'\")\n    print(f\"  Hierarchical: {hier_result['level2_label']}\")\n    print(f\"     Confidence: {hier_result['level2_conf']:.3f}\")\n    print(f\"  Flat:         {flat_labels[flat_pred]}\")\n    print(f\"     Confidence: {flat_conf:.3f}\")\n    print(f\"  Confidence Diff: {hier_result['level2_conf'] - flat_conf:+.3f}\")\n\n# TODO: After implementing 3-level, uncomment to test it:\n# print(\"\\n\" + \"=\"*80)\n# print(\"TESTING 3-LEVEL HIERARCHICAL CLASSIFICATION\")\n# print(\"=\"*80)\n#\n# for i, review in enumerate(test_reviews):\n#     result = hierarchical_classify_3level(review)\n#     print(f\"\\n{i+1}. '{review[:60]}...'\")\n#     print(f\"   Path: {result['path']}\")\n#     print(f\"   Level 3 confidence: {result['level3_conf']:.3f}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": "### Questions\n\n1. Compare confidence scores for hierarchical vs flat. Does breaking into steps help?\n\n2. Can the classifier get Level 2 right even if Level 1 is wrong?\n\n3. After adding 3 levels: Did the extra granularity help or hurt?\n"
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": "#### Hard Task 2: Active Learning to Minimize Labeling\n\nCompare active learning (picking uncertain samples) vs random selection.\n"
  },
  {
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "data = load_dataset(\"rotten_tomatoes\")\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# Prepare datasets\n",
    "pool_size = 2000\n",
    "test_size = 300\n",
    "\n",
    "train_pool = data[\"train\"].shuffle(seed=42).select(range(pool_size))\n",
    "test_set = data[\"test\"].shuffle(seed=42).select(range(test_size))\n",
    "\n",
    "# Generate embeddings upfront (faster)\n",
    "print(\"Generating embeddings for 2000 training pool and 300 test samples...\")\n",
    "pool_embeddings = model.encode(train_pool[\"text\"], show_progress_bar=True)\n",
    "test_embeddings = model.encode(test_set[\"text\"], show_progress_bar=False)\n",
    "print(\" Embeddings ready\\n\")\n",
    "\n",
    "def uncertainty_sampling(clf, unlabeled_embeddings, n_samples=50):\n",
    "    \"\"\"\n",
    "    Select samples where model is most uncertain.\n",
    "    Strategy: Pick samples with lowest confidence (closest to 50-50)\n",
    "    \"\"\"\n",
    "    probs = clf.predict_proba(unlabeled_embeddings)\n",
    "    # Uncertainty = 1 - max(prob) = how close to 50-50 the prediction is\n",
    "    uncertainties = 1 - np.max(probs, axis=1)\n",
    "\n",
    "    # Get indices of most uncertain samples\n",
    "    most_uncertain_indices = np.argsort(uncertainties)[-n_samples:]\n",
    "    return most_uncertain_indices\n",
    "\n",
    "# TODO: Uncomment to implement alternative selection strategy\n",
    "# def uncertainty_sampling(clf, unlabeled_embeddings, n_samples=50):\n",
    "#     \"\"\"\n",
    "#     Alternative strategy: Margin sampling\n",
    "#     Select samples where top two predictions are closest\n",
    "#     \"\"\"\n",
    "#     probs = clf.predict_proba(unlabeled_embeddings)\n",
    "#     # Sort probabilities for each sample\n",
    "#     sorted_probs = np.sort(probs, axis=1)\n",
    "#     # Margin = difference between top two\n",
    "#     margins = sorted_probs[:, -1] - sorted_probs[:, -2]\n",
    "#\n",
    "#     # Get indices of smallest margins (most uncertain)\n",
    "#     most_uncertain_indices = np.argsort(margins)[:n_samples]\n",
    "#     return most_uncertain_indices\n",
    "\n",
    "def random_sampling(n_available, n_samples=50):\n",
    "    \"\"\"Baseline: Random selection\"\"\"\n",
    "    return np.random.choice(n_available, size=min(n_samples, n_available), replace=False)\n",
    "\n",
    "# Active Learning Simulation\n",
    "print(\"=\"*80)\n",
    "print(\"ACTIVE LEARNING SIMULATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"Strategy: Start with 100 labeled, then iteratively add 50 most uncertain samples\")\n",
    "print(\"Compare: Active Learning vs Random Sampling\\n\")\n",
    "\n",
    "# Configuration\n",
    "initial_size = 100\n",
    "samples_per_iteration = 50\n",
    "n_iterations = 10\n",
    "\n",
    "# Initialize\n",
    "labeled_indices = list(range(initial_size))\n",
    "unlabeled_indices = list(range(initial_size, pool_size))\n",
    "\n",
    "active_scores = []\n",
    "random_scores = []\n",
    "iteration_labeled_sizes = []\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    current_size = len(labeled_indices)\n",
    "    iteration_labeled_sizes.append(current_size)\n",
    "\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Iteration {iteration + 1}/{n_iterations} - Labeled samples: {current_size}\")\n",
    "    print('='*80)\n",
    "\n",
    "    # Get current labeled data\n",
    "    labeled_embeddings = pool_embeddings[labeled_indices]\n",
    "    labeled_labels = [train_pool[\"label\"][i] for i in labeled_indices]\n",
    "\n",
    "    # Train classifier\n",
    "    clf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    clf.fit(labeled_embeddings, labeled_labels)\n",
    "\n",
    "    # Evaluate\n",
    "    test_pred = clf.predict(test_embeddings)\n",
    "    active_f1 = f1_score(test_set[\"label\"], test_pred, average='weighted')\n",
    "    active_scores.append(active_f1)\n",
    "\n",
    "    print(f\"Active Learning F1: {active_f1:.4f}\")\n",
    "\n",
    "    # Compare with random sampling (same number of samples)\n",
    "    random_indices = list(range(initial_size)) + list(\n",
    "        np.random.choice(range(initial_size, pool_size),\n",
    "                        size=min(current_size - initial_size, pool_size - initial_size),\n",
    "                        replace=False)\n",
    "    )\n",
    "    random_embeddings = pool_embeddings[random_indices]\n",
    "    random_labels = [train_pool[\"label\"][i] for i in random_indices]\n",
    "\n",
    "    clf_random = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    clf_random.fit(random_embeddings, random_labels)\n",
    "    random_pred = clf_random.predict(test_embeddings)\n",
    "    random_f1 = f1_score(test_set[\"label\"], random_pred, average='weighted')\n",
    "    random_scores.append(random_f1)\n",
    "\n",
    "    print(f\"Random Sampling F1:  {random_f1:.4f}\")\n",
    "    print(f\"Improvement:         {active_f1 - random_f1:+.4f}\")\n",
    "\n",
    "    # Select next batch using active learning\n",
    "    if len(unlabeled_indices) < samples_per_iteration:\n",
    "        print(f\"\\n Stopping: Only {len(unlabeled_indices)} samples left\")\n",
    "        break\n",
    "\n",
    "    unlabeled_embeddings = pool_embeddings[unlabeled_indices]\n",
    "    uncertain_local_indices = uncertainty_sampling(clf, unlabeled_embeddings, samples_per_iteration)\n",
    "\n",
    "    # Convert to global indices\n",
    "    newly_labeled = [unlabeled_indices[i] for i in uncertain_local_indices]\n",
    "\n",
    "    # Show examples of selected samples\n",
    "    print(f\"\\nExamples of selected UNCERTAIN samples:\")\n",
    "    for i, idx in enumerate(newly_labeled[:3]):\n",
    "        probs = clf.predict_proba(pool_embeddings[idx].reshape(1, -1))[0]\n",
    "        uncertainty = 1 - np.max(probs)\n",
    "        print(f\"  {i+1}. '{train_pool['text'][idx][:60]}...'\")\n",
    "        print(f\"     Uncertainty: {uncertainty:.3f}\")\n",
    "        print(f\"     Probs: [neg={probs[0]:.3f}, pos={probs[1]:.3f}]\")\n",
    "\n",
    "    # Update sets\n",
    "    labeled_indices.extend(newly_labeled)\n",
    "    unlabeled_indices = [idx for idx in unlabeled_indices if idx not in newly_labeled]\n",
    "    print()\n",
    "\n",
    "# Results Summary\n",
    "print(\"=\"*80)\n",
    "print(\"FINAL RESULTS - LEARNING CURVES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n{'Labeled':<10s} {'Active F1':<12s} {'Random F1':<12s} {'Difference':<12s}\")\n",
    "print(\"-\"*50)\n",
    "for size, active, random in zip(iteration_labeled_sizes, active_scores, random_scores):\n",
    "    diff = active - random\n",
    "    marker = \"  \" if diff > 0.01 else \"\"\n",
    "    print(f\"{size:<10d} {active:.4f}       {random:.4f}       {diff:+.4f}{marker}\")\n",
    "\n",
    "avg_improvement = np.mean(np.array(active_scores) - np.array(random_scores))\n",
    "print(f\"\\nAverage Improvement: {avg_improvement:+.4f}\")\n",
    "\n",
    "# Find when active learning reaches target F1\n",
    "target_f1 = 0.85\n",
    "active_reached = next((size for size, f1 in zip(iteration_labeled_sizes, active_scores) if f1 >= target_f1), None)\n",
    "random_reached = next((size for size, f1 in zip(iteration_labeled_sizes, random_scores) if f1 >= target_f1), None)\n",
    "\n",
    "if active_reached or random_reached:\n",
    "    print(f\"\\nTo reach F1={target_f1}:\")\n",
    "    if active_reached:\n",
    "        print(f\"  Active Learning: {active_reached} labeled samples\")\n",
    "    else:\n",
    "        print(f\"  Active Learning: Did not reach {target_f1}\")\n",
    "\n",
    "    if random_reached:\n",
    "        print(f\"  Random Sampling: {random_reached} labeled samples\")\n",
    "    else:\n",
    "        print(f\"  Random Sampling: Did not reach {target_f1}\")\n",
    "\n",
    "    if active_reached and random_reached:\n",
    "        savings = random_reached - active_reached\n",
    "        print(f\"   Active Learning saved {savings} labeled samples ({savings/random_reached:.1%})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TODO: Try different selection strategies and record results:\")\n",
    "print(\"=\"*80)\n",
    "print(\"| Strategy              | Samples to F1=0.85 | Avg Improvement | Notes |\")\n",
    "print(\"|----------------------|-------------------|-----------------|-------|\")\n",
    "print(\"| Uncertainty (1-max)  | ???               | ???             |       |\")\n",
    "print(\"| Margin sampling      | ???               | ???             |       |\")\n",
    "print(\"| YOUR STRATEGY        | ???               | ???             |       |\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "31e52c083fbd45de8bc39eef296bfd2b",
      "19ff4554c5c04b7f8e4f921005ccdd9e",
      "27bb8412e000498295babac8fbe52c3f",
      "f49402608ba44a6e9518f44bcbbdf1e5",
      "3333b0243467411ea154c5f5696008a2",
      "3a6373e168304156ba0ed95c4a641fdb",
      "04f978025bf34d2ba1f5fe98c69f33f5",
      "9de9769d6370464199eed6489432614c",
      "bf33dc78e48d4cbfb6bc9c4793e9b179",
      "33d4750e1b904615ab015b7c940601b8",
      "00e048491fce4d10836ef45f0f307dee"
     ]
    },
    "id": "VSP2ei_2G4hH",
    "outputId": "746d8713-a669-465b-eb65-c1fb471f86a0"
   },
   "execution_count": 36,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Generating embeddings for 2000 training pool and 300 test samples...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/63 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "31e52c083fbd45de8bc39eef296bfd2b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2713 Embeddings ready\n",
      "\n",
      "================================================================================\n",
      "ACTIVE LEARNING SIMULATION\n",
      "================================================================================\n",
      "Strategy: Start with 100 labeled, then iteratively add 50 most uncertain samples\n",
      "Compare: Active Learning vs Random Sampling\n",
      "\n",
      "================================================================================\n",
      "Iteration 1/10 - Labeled samples: 100\n",
      "================================================================================\n",
      "Active Learning F1: 0.7535\n",
      "Random Sampling F1:  0.7535\n",
      "Improvement:         +0.0000\n",
      "\n",
      "Examples of selected UNCERTAIN samples:\n",
      "  1. 'it uses the pain and violence of war as background material ...'\n",
      "     Uncertainty: 0.495\n",
      "     Probs: [neg=0.505, pos=0.495]\n",
      "  2. '. . . the tale of her passionate , tumultuous affair with mu...'\n",
      "     Uncertainty: 0.495\n",
      "     Probs: [neg=0.505, pos=0.495]\n",
      "  3. 'inventive , fun , intoxicatingly sexy , violent , self-indul...'\n",
      "     Uncertainty: 0.495\n",
      "     Probs: [neg=0.505, pos=0.495]\n",
      "\n",
      "================================================================================\n",
      "Iteration 2/10 - Labeled samples: 150\n",
      "================================================================================\n",
      "Active Learning F1: 0.8023\n",
      "Random Sampling F1:  0.8222\n",
      "Improvement:         -0.0199\n",
      "\n",
      "Examples of selected UNCERTAIN samples:\n",
      "  1. 'a vile , incoherent mess . . . a scummy ripoff of david cron...'\n",
      "     Uncertainty: 0.492\n",
      "     Probs: [neg=0.492, pos=0.508]\n",
      "  2. 'why come up with something even quasi-original , when you ca...'\n",
      "     Uncertainty: 0.492\n",
      "     Probs: [neg=0.492, pos=0.508]\n",
      "  3. 'god is great , the movie's not ....'\n",
      "     Uncertainty: 0.492\n",
      "     Probs: [neg=0.508, pos=0.492]\n",
      "\n",
      "================================================================================\n",
      "Iteration 3/10 - Labeled samples: 200\n",
      "================================================================================\n",
      "Active Learning F1: 0.8298\n",
      "Random Sampling F1:  0.8329\n",
      "Improvement:         -0.0031\n",
      "\n",
      "Examples of selected UNCERTAIN samples:\n",
      "  1. 'light the candles , bring out the cake and don't fret about ...'\n",
      "     Uncertainty: 0.488\n",
      "     Probs: [neg=0.488, pos=0.512]\n",
      "  2. 'it's not difficult to spot the culprit early-on in this pred...'\n",
      "     Uncertainty: 0.489\n",
      "     Probs: [neg=0.511, pos=0.489]\n",
      "  3. 'mcconaughey's fun to watch , the dragons are okay , not much...'\n",
      "     Uncertainty: 0.489\n",
      "     Probs: [neg=0.489, pos=0.511]\n",
      "\n",
      "================================================================================\n",
      "Iteration 4/10 - Labeled samples: 250\n",
      "================================================================================\n",
      "Active Learning F1: 0.8332\n",
      "Random Sampling F1:  0.8100\n",
      "Improvement:         +0.0233\n",
      "\n",
      "Examples of selected UNCERTAIN samples:\n",
      "  1. 'i didn't find much fascination in the swinging . what they'r...'\n",
      "     Uncertainty: 0.489\n",
      "     Probs: [neg=0.511, pos=0.489]\n",
      "  2. 'a minor picture with a major identity crisis -- it's sort of...'\n",
      "     Uncertainty: 0.489\n",
      "     Probs: [neg=0.511, pos=0.489]\n",
      "  3. 'howard and his co-stars all give committed performances , bu...'\n",
      "     Uncertainty: 0.489\n",
      "     Probs: [neg=0.511, pos=0.489]\n",
      "\n",
      "================================================================================\n",
      "Iteration 5/10 - Labeled samples: 300\n",
      "================================================================================\n",
      "Active Learning F1: 0.8332\n",
      "Random Sampling F1:  0.8332\n",
      "Improvement:         +0.0001\n",
      "\n",
      "Examples of selected UNCERTAIN samples:\n",
      "  1. 'fred schepisi's tale of four englishmen facing the prospect ...'\n",
      "     Uncertainty: 0.485\n",
      "     Probs: [neg=0.515, pos=0.485]\n",
      "  2. 'a solid examination of the male midlife crisis ....'\n",
      "     Uncertainty: 0.485\n",
      "     Probs: [neg=0.485, pos=0.515]\n",
      "  3. 'a simmering psychological drama in which the bursts of sudde...'\n",
      "     Uncertainty: 0.486\n",
      "     Probs: [neg=0.486, pos=0.514]\n",
      "\n",
      "================================================================================\n",
      "Iteration 6/10 - Labeled samples: 350\n",
      "================================================================================\n",
      "Active Learning F1: 0.8299\n",
      "Random Sampling F1:  0.8492\n",
      "Improvement:         -0.0193\n",
      "\n",
      "Examples of selected UNCERTAIN samples:\n",
      "  1. 'a teasing drama whose relentless good-deed/bad-deed reversal...'\n",
      "     Uncertainty: 0.481\n",
      "     Probs: [neg=0.481, pos=0.519]\n",
      "  2. 'proof that a thriller can be sleekly shot , expertly cast , ...'\n",
      "     Uncertainty: 0.481\n",
      "     Probs: [neg=0.519, pos=0.481]\n",
      "  3. 'a deceivingly simple film , one that grows in power in retro...'\n",
      "     Uncertainty: 0.481\n",
      "     Probs: [neg=0.481, pos=0.519]\n",
      "\n",
      "================================================================================\n",
      "Iteration 7/10 - Labeled samples: 400\n",
      "================================================================================\n",
      "Active Learning F1: 0.8498\n",
      "Random Sampling F1:  0.8394\n",
      "Improvement:         +0.0105\n",
      "\n",
      "Examples of selected UNCERTAIN samples:\n",
      "  1. 'it's all very cute , though not terribly funny if you're mor...'\n",
      "     Uncertainty: 0.470\n",
      "     Probs: [neg=0.530, pos=0.470]\n",
      "  2. 'bound to appeal to women looking for a howlingly trashy time...'\n",
      "     Uncertainty: 0.470\n",
      "     Probs: [neg=0.530, pos=0.470]\n",
      "  3. 'the ring is worth a look , if you don't demand much more tha...'\n",
      "     Uncertainty: 0.471\n",
      "     Probs: [neg=0.471, pos=0.529]\n",
      "\n",
      "================================================================================\n",
      "Iteration 8/10 - Labeled samples: 450\n",
      "================================================================================\n",
      "Active Learning F1: 0.8465\n",
      "Random Sampling F1:  0.8232\n",
      "Improvement:         +0.0233\n",
      "\n",
      "Examples of selected UNCERTAIN samples:\n",
      "  1. 'it's a minor comedy that tries to balance sweetness with coa...'\n",
      "     Uncertainty: 0.454\n",
      "     Probs: [neg=0.454, pos=0.546]\n",
      "  2. 'as home movie gone haywire , it's pretty enjoyable , but as ...'\n",
      "     Uncertainty: 0.454\n",
      "     Probs: [neg=0.454, pos=0.546]\n",
      "  3. 'brainless , but enjoyably over-the-top , the retro gang melo...'\n",
      "     Uncertainty: 0.455\n",
      "     Probs: [neg=0.455, pos=0.545]\n",
      "\n",
      "================================================================================\n",
      "Iteration 9/10 - Labeled samples: 500\n",
      "================================================================================\n",
      "Active Learning F1: 0.8699\n",
      "Random Sampling F1:  0.8599\n",
      "Improvement:         +0.0101\n",
      "\n",
      "Examples of selected UNCERTAIN samples:\n",
      "  1. 'we root for [clara and paul] , even like them , though perha...'\n",
      "     Uncertainty: 0.437\n",
      "     Probs: [neg=0.563, pos=0.437]\n",
      "  2. 'in many ways , reminiscent of 1992's unforgiven which also u...'\n",
      "     Uncertainty: 0.437\n",
      "     Probs: [neg=0.437, pos=0.563]\n",
      "  3. 'any enjoyment will be hinge from a personal threshold of wat...'\n",
      "     Uncertainty: 0.438\n",
      "     Probs: [neg=0.438, pos=0.562]\n",
      "\n",
      "================================================================================\n",
      "Iteration 10/10 - Labeled samples: 550\n",
      "================================================================================\n",
      "Active Learning F1: 0.8632\n",
      "Random Sampling F1:  0.8430\n",
      "Improvement:         +0.0202\n",
      "\n",
      "Examples of selected UNCERTAIN samples:\n",
      "  1. 'playfully profound . . . and crazier than michael jackson on...'\n",
      "     Uncertainty: 0.425\n",
      "     Probs: [neg=0.425, pos=0.575]\n",
      "  2. 'mr . wedge and mr . saldanha handle the mix of verbal jokes ...'\n",
      "     Uncertainty: 0.425\n",
      "     Probs: [neg=0.425, pos=0.575]\n",
      "  3. 'i didn't laugh . i didn't smile . i survived ....'\n",
      "     Uncertainty: 0.426\n",
      "     Probs: [neg=0.426, pos=0.574]\n",
      "\n",
      "================================================================================\n",
      "FINAL RESULTS - LEARNING CURVES\n",
      "================================================================================\n",
      "\n",
      "Labeled    Active F1    Random F1    Difference  \n",
      "--------------------------------------------------\n",
      "100        0.7535       0.7535       +0.0000\n",
      "150        0.8023       0.8222       -0.0199\n",
      "200        0.8298       0.8329       -0.0031\n",
      "250        0.8332       0.8100       +0.0233  \u2713\n",
      "300        0.8332       0.8332       +0.0001\n",
      "350        0.8299       0.8492       -0.0193\n",
      "400        0.8498       0.8394       +0.0105  \u2713\n",
      "450        0.8465       0.8232       +0.0233  \u2713\n",
      "500        0.8699       0.8599       +0.0101  \u2713\n",
      "550        0.8632       0.8430       +0.0202  \u2713\n",
      "\n",
      "Average Improvement: +0.0045\n",
      "\n",
      "To reach F1=0.85:\n",
      "  Active Learning: 500 labeled samples\n",
      "  Random Sampling: 500 labeled samples\n",
      "  \u2192 Active Learning saved 0 labeled samples (0.0%)\n",
      "\n",
      "================================================================================\n",
      "TODO: Try different selection strategies and record results:\n",
      "================================================================================\n",
      "| Strategy              | Samples to F1=0.85 | Avg Improvement | Notes |\n",
      "|----------------------|-------------------|-----------------|-------|\n",
      "| Uncertainty (1-max)  | ???               | ???             |       |\n",
      "| Margin sampling      | ???               | ???             |       |\n",
      "| YOUR STRATEGY        | ???               | ???             |       |\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Questions\n",
    "\n",
    "1. What makes the uncertain samples uncertain? Are they using hedging language, mixed sentiment, or ambiguous wording?\n",
    "\n",
    "2. At what point did active learning pull ahead of random sampling? How much can active learning reduce labeling costs?\n",
    "\n",
    "3. Why are samples with probabilities like [0.52, 0.48] more valuable for training than confident samples?"
   ],
   "metadata": {
    "id": "yzYVwXTBHOrA"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About This Task:**\n\n",
    "Ensemble methods combine multiple classifiers to improve robustness and accuracy. By leveraging diverse models or training strategies, ensembles reduce individual model biases and achieve more reliable predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": "#### Hard Task 3: Ensemble Classifier\n\nCompare 3 different models individually vs combined as an ensemble.\n"
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "data = load_dataset(\"rotten_tomatoes\")\n",
    "\n",
    "# Configuration\n",
    "train_size = 1500\n",
    "test_size = 300\n",
    "\n",
    "train_subset = data[\"train\"].shuffle(seed=42).select(range(train_size))\n",
    "test_subset = data[\"test\"].shuffle(seed=42).select(range(test_size))\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"BUILDING ENSEMBLE OF CLASSIFIERS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Model 1: Task-Specific (Twitter RoBERTa)\n",
    "print(\"\\n[1/3] Loading Task-Specific Model (Twitter RoBERTa)...\")\n",
    "task_model = pipeline(\n",
    "    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    tokenizer=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    return_all_scores=True,\n",
    "    device=-1\n",
    ")\n",
    "\n",
    "# Model 2: Embedding + Logistic Regression\n",
    "print(\"[2/3] Training Embedding Classifier...\")\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "train_embeddings = embedding_model.encode(train_subset[\"text\"], show_progress_bar=True)\n",
    "test_embeddings = embedding_model.encode(test_subset[\"text\"], show_progress_bar=False)\n",
    "\n",
    "clf_embedding = LogisticRegression(random_state=42, max_iter=1000)\n",
    "clf_embedding.fit(train_embeddings, train_subset[\"label\"])\n",
    "\n",
    "# Model 3: Zero-Shot\n",
    "print(\"[3/3] Setting up Zero-Shot Classifier...\")\n",
    "zero_shot_labels = [\"A very negative movie review\", \"A very positive movie review\"]\n",
    "zero_shot_label_embeddings = embedding_model.encode(zero_shot_labels)\n",
    "\n",
    "# TODO: Uncomment to add Model 4 - Different embedding model\n",
    "# print(\"[4/4] Training with alternative embedding model...\")\n",
    "# embedding_model_alt = SentenceTransformer('all-MiniLM-L6-v2')  # Smaller, faster\n",
    "# train_embeddings_alt = embedding_model_alt.encode(train_subset[\"text\"], show_progress_bar=True)\n",
    "# test_embeddings_alt = embedding_model_alt.encode(test_subset[\"text\"], show_progress_bar=False)\n",
    "# clf_embedding_alt = LogisticRegression(random_state=42, max_iter=1000)\n",
    "# clf_embedding_alt.fit(train_embeddings_alt, train_subset[\"label\"])\n",
    "\n",
    "print(\"\\n All models ready\")\n",
    "\n",
    "# Get predictions from all models\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATING PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nModel 1: Task-Specific...\")\n",
    "pred_task = []\n",
    "conf_task = []\n",
    "for text in test_subset[\"text\"]:\n",
    "    output = task_model(text)[0]\n",
    "    neg_score = output[0][\"score\"]\n",
    "    pos_score = output[2][\"score\"]\n",
    "    pred_task.append(1 if pos_score > neg_score else 0)\n",
    "    conf_task.append(max(neg_score, pos_score))\n",
    "\n",
    "print(\"Model 2: Embedding Classifier...\")\n",
    "pred_embedding = clf_embedding.predict(test_embeddings)\n",
    "conf_embedding = np.max(clf_embedding.predict_proba(test_embeddings), axis=1)\n",
    "\n",
    "print(\"Model 3: Zero-Shot...\")\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "zero_shot_sim = cosine_similarity(test_embeddings, zero_shot_label_embeddings)\n",
    "pred_zero_shot = np.argmax(zero_shot_sim, axis=1)\n",
    "conf_zero_shot = np.max(zero_shot_sim, axis=1)\n",
    "\n",
    "# TODO: Uncomment if you added Model 4\n",
    "# print(\"Model 4: Alternative Embedding...\")\n",
    "# pred_alt = clf_embedding_alt.predict(test_embeddings_alt)\n",
    "# conf_alt = np.max(clf_embedding_alt.predict_proba(test_embeddings_alt), axis=1)\n",
    "\n",
    "# Evaluate individual models\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INDIVIDUAL MODEL PERFORMANCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "models = [\n",
    "    (\"Task-Specific\", pred_task),\n",
    "    (\"Embedding + LR\", pred_embedding),\n",
    "    (\"Zero-Shot\", pred_zero_shot),\n",
    "]\n",
    "\n",
    "# TODO: Uncomment if Model 4 added\n",
    "# models.append((\"Alternative Embedding\", pred_alt))\n",
    "\n",
    "individual_scores = []\n",
    "for name, predictions in models:\n",
    "    f1 = f1_score(test_subset[\"label\"], predictions, average='weighted')\n",
    "    acc = accuracy_score(test_subset[\"label\"], predictions)\n",
    "    individual_scores.append(f1)\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  F1 Score:  {f1:.4f}\")\n",
    "    print(f\"  Accuracy:  {acc:.4f}\")\n",
    "\n",
    "# Ensemble Methods\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENSEMBLE METHODS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Method 1: Simple Majority Voting\n",
    "ensemble_votes = np.array([pred_task, pred_embedding, pred_zero_shot])\n",
    "# TODO: Add Model 4 if available\n",
    "# ensemble_votes = np.array([pred_task, pred_embedding, pred_zero_shot, pred_alt])\n",
    "\n",
    "pred_majority = np.apply_along_axis(lambda x: np.bincount(x).argmax(), 0, ensemble_votes)\n",
    "\n",
    "maj_f1 = f1_score(test_subset[\"label\"], pred_majority, average='weighted')\n",
    "maj_acc = accuracy_score(test_subset[\"label\"], pred_majority)\n",
    "\n",
    "print(f\"\\n1. Simple Majority Voting:\")\n",
    "print(f\"   F1 Score:  {maj_f1:.4f}\")\n",
    "print(f\"   Accuracy:  {maj_acc:.4f}\")\n",
    "\n",
    "# Method 2: Confidence-Weighted Voting\n",
    "weights = np.array([conf_task, conf_embedding, conf_zero_shot])\n",
    "# TODO: Add Model 4 weights if available\n",
    "# weights = np.array([conf_task, conf_embedding, conf_zero_shot, conf_alt])\n",
    "\n",
    "weighted_votes = np.zeros((len(test_subset), 2))\n",
    "for i in range(len(test_subset)):\n",
    "    for model_idx in range(len(models)):\n",
    "        vote = ensemble_votes[model_idx, i]\n",
    "        weight = weights[model_idx, i]\n",
    "        weighted_votes[i, vote] += weight\n",
    "\n",
    "pred_weighted = np.argmax(weighted_votes, axis=1)\n",
    "\n",
    "weight_f1 = f1_score(test_subset[\"label\"], pred_weighted, average='weighted')\n",
    "weight_acc = accuracy_score(test_subset[\"label\"], pred_weighted)\n",
    "\n",
    "print(f\"\\n2. Confidence-Weighted Voting:\")\n",
    "print(f\"   F1 Score:  {weight_f1:.4f}\")\n",
    "print(f\"   Accuracy:  {weight_acc:.4f}\")\n",
    "\n",
    "# TODO: Uncomment to implement Method 3: Performance-Weighted Voting\n",
    "# Method 3: Weight models by their F1 scores\n",
    "# print(f\"\\n3. Performance-Weighted Voting:\")\n",
    "# model_weights = np.array(individual_scores)  # Use F1 scores as weights\n",
    "# model_weights = model_weights / model_weights.sum()  # Normalize\n",
    "#\n",
    "# perf_weighted_votes = np.zeros((len(test_subset), 2))\n",
    "# for i in range(len(test_subset)):\n",
    "#     for model_idx in range(len(models)):\n",
    "#         vote = ensemble_votes[model_idx, i]\n",
    "#         weight = model_weights[model_idx]\n",
    "#         perf_weighted_votes[i, vote] += weight\n",
    "#\n",
    "# pred_perf_weighted = np.argmax(perf_weighted_votes, axis=1)\n",
    "# perf_f1 = f1_score(test_subset[\"label\"], pred_perf_weighted, average='weighted')\n",
    "# perf_acc = accuracy_score(test_subset[\"label\"], pred_perf_weighted)\n",
    "# print(f\"   F1 Score:  {perf_f1:.4f}\")\n",
    "# print(f\"   Accuracy:  {perf_acc:.4f}\")\n",
    "\n",
    "# Comparison Table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = [\n",
    "    (\"Task-Specific (Model 1)\", individual_scores[0]),\n",
    "    (\"Embedding (Model 2)\", individual_scores[1]),\n",
    "    (\"Zero-Shot (Model 3)\", individual_scores[2]),\n",
    "    (\"\" * 30, None),\n",
    "    (\"Ensemble: Majority Vote\", maj_f1),\n",
    "    (\"Ensemble: Confidence-Weighted\", weight_f1),\n",
    "]\n",
    "\n",
    "# TODO: Add Model 4 and performance-weighted if implemented\n",
    "# results.insert(3, (\"Alternative Embedding (Model 4)\", individual_scores[3]))\n",
    "# results.append((\"Ensemble: Performance-Weighted\", perf_f1))\n",
    "\n",
    "best_individual = max(individual_scores)\n",
    "\n",
    "print(f\"\\n{'Method':<35s} {'F1 Score':<12s} {'vs Best Individual':<20s}\")\n",
    "print(\"-\"*70)\n",
    "for name, score in results:\n",
    "    if score is None:\n",
    "        print(name)\n",
    "    else:\n",
    "        diff = score - best_individual if score is not None else 0\n",
    "        improvement = \"\" if diff > 0.001 else \"\"\n",
    "        print(f\"{name:<35s} {score:.4f}       {diff:+.4f}  {improvement}\")\n",
    "\n",
    "# Analyze disagreements\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYZING MODEL DISAGREEMENTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "disagreements = []\n",
    "unanimous_correct = 0\n",
    "unanimous_wrong = 0\n",
    "\n",
    "for i in range(len(test_subset)):\n",
    "    votes = ensemble_votes[:, i]\n",
    "    unique_votes = len(set(votes))\n",
    "\n",
    "    if unique_votes > 1:  # Disagreement\n",
    "        disagreements.append({\n",
    "            'index': i,\n",
    "            'text': test_subset[\"text\"][i],\n",
    "            'true': test_subset[\"label\"][i],\n",
    "            'votes': votes,\n",
    "            'ensemble': pred_majority[i],\n",
    "            'models': [models[j][0] for j in range(len(models))]\n",
    "        })\n",
    "    else:  # Unanimous\n",
    "        if votes[0] == test_subset[\"label\"][i]:\n",
    "            unanimous_correct += 1\n",
    "        else:\n",
    "            unanimous_wrong += 1\n",
    "\n",
    "print(f\"\\nVoting Patterns:\")\n",
    "print(f\"  Unanimous Correct: {unanimous_correct} ({100*unanimous_correct/len(test_subset):.1f}%)\")\n",
    "print(f\"  Unanimous Wrong:   {unanimous_wrong} ({100*unanimous_wrong/len(test_subset):.1f}%)\")\n",
    "print(f\"  Disagreements:     {len(disagreements)} ({100*len(disagreements)/len(test_subset):.1f}%)\")\n",
    "\n",
    "print(f\"\\n\" + \"-\"*80)\n",
    "print(f\"Examples of Disagreements (first 5):\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for i, case in enumerate(disagreements[:5]):\n",
    "    true_label = \"Positive\" if case['true'] == 1 else \"Negative\"\n",
    "    ensemble_label = \"Positive\" if case['ensemble'] == 1 else \"Negative\"\n",
    "    ensemble_correct = \"\" if case['ensemble'] == case['true'] else \"\"\n",
    "\n",
    "    print(f\"\\n{i+1}. '{case['text'][:60]}...'\")\n",
    "    print(f\"   True: {true_label}\")\n",
    "\n",
    "    for j, model_name in enumerate(case['models']):\n",
    "        vote_label = \"Positive\" if case['votes'][j] == 1 else \"Negative\"\n",
    "        vote_correct = \"\" if case['votes'][j] == case['true'] else \"\"\n",
    "        print(f\"   {model_name:20s}: {vote_label:8s} {vote_correct}\")\n",
    "\n",
    "    print(f\"   Ensemble Decision:   {ensemble_label:8s} {ensemble_correct}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "e90e7c8a9ae641f9a5f2c9ab66248dbc",
      "e3af2532ddea450095ef34e3c7890344",
      "220d3bb538d84e23ba45bdb1ef20e68c",
      "be25c443c44c428eb271c02c2e34578a",
      "beb2fb51e5714276808eba4b576907e3",
      "8522a39f750149109702719a81a46bb0",
      "9d04edae2e034fb39a681ea8db1f79d8",
      "e97f097bfe454537b714d50cc3e1923e",
      "6fd1de03fef54dffba7b998edeac0776",
      "2d7a4067755849cd9cb94accfed4d9c0",
      "eb3cbb80ef9a42a8b31bf282cc426aeb"
     ]
    },
    "id": "kWDdM6kBHHr9",
    "outputId": "2f7082bc-801f-4d8d-9f17-78ae80b0cdce"
   },
   "execution_count": 37,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "================================================================================\n",
      "BUILDING ENSEMBLE OF CLASSIFIERS\n",
      "================================================================================\n",
      "\n",
      "[1/3] Loading Task-Specific Model (Twitter RoBERTa)...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[2/3] Training Embedding Classifier...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/47 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e90e7c8a9ae641f9a5f2c9ab66248dbc"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[3/3] Setting up Zero-Shot Classifier...\n",
      "\n",
      "\u2713 All models ready\n",
      "\n",
      "================================================================================\n",
      "GENERATING PREDICTIONS\n",
      "================================================================================\n",
      "\n",
      "Model 1: Task-Specific...\n",
      "Model 2: Embedding Classifier...\n",
      "Model 3: Zero-Shot...\n",
      "\n",
      "================================================================================\n",
      "INDIVIDUAL MODEL PERFORMANCE\n",
      "================================================================================\n",
      "\n",
      "Task-Specific:\n",
      "  F1 Score:  0.7709\n",
      "  Accuracy:  0.7733\n",
      "\n",
      "Embedding + LR:\n",
      "  F1 Score:  0.8699\n",
      "  Accuracy:  0.8700\n",
      "\n",
      "Zero-Shot:\n",
      "  F1 Score:  0.8255\n",
      "  Accuracy:  0.8267\n",
      "\n",
      "================================================================================\n",
      "ENSEMBLE METHODS\n",
      "================================================================================\n",
      "\n",
      "1. Simple Majority Voting:\n",
      "   F1 Score:  0.8667\n",
      "   Accuracy:  0.8667\n",
      "\n",
      "2. Confidence-Weighted Voting:\n",
      "   F1 Score:  0.8632\n",
      "   Accuracy:  0.8633\n",
      "\n",
      "================================================================================\n",
      "PERFORMANCE COMPARISON\n",
      "================================================================================\n",
      "\n",
      "Method                              F1 Score     vs Best Individual  \n",
      "----------------------------------------------------------------------\n",
      "Task-Specific (Model 1)             0.7709       -0.0990  \n",
      "Embedding (Model 2)                 0.8699       +0.0000  \n",
      "Zero-Shot (Model 3)                 0.8255       -0.0444  \n",
      "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
      "Ensemble: Majority Vote             0.8667       -0.0032  \n",
      "Ensemble: Confidence-Weighted       0.8632       -0.0066  \n",
      "\n",
      "================================================================================\n",
      "ANALYZING MODEL DISAGREEMENTS\n",
      "================================================================================\n",
      "\n",
      "Voting Patterns:\n",
      "  Unanimous Correct: 199 (66.3%)\n",
      "  Unanimous Wrong:   18 (6.0%)\n",
      "  Disagreements:     83 (27.7%)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Examples of Disagreements (first 5):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. 'we started to wonder if \u0085 some unpaid intern had just typed ...'\n",
      "   True: Negative\n",
      "   Task-Specific       : Negative \u2713\n",
      "   Embedding + LR      : Negative \u2713\n",
      "   Zero-Shot           : Positive \u2717\n",
      "   Ensemble Decision:   Negative \u2713\n",
      "\n",
      "2. 'the metaphors are provocative , but too often , the viewer i...'\n",
      "   True: Negative\n",
      "   Task-Specific       : Negative \u2713\n",
      "   Embedding + LR      : Negative \u2713\n",
      "   Zero-Shot           : Positive \u2717\n",
      "   Ensemble Decision:   Negative \u2713\n",
      "\n",
      "3. 'an uneasy mix of run-of-the-mill raunchy humor and seemingly...'\n",
      "   True: Negative\n",
      "   Task-Specific       : Negative \u2713\n",
      "   Embedding + LR      : Positive \u2717\n",
      "   Zero-Shot           : Positive \u2717\n",
      "   Ensemble Decision:   Positive \u2717\n",
      "\n",
      "4. 'it's like a \" big chill \" reunion of the baader-meinhof gang...'\n",
      "   True: Positive\n",
      "   Task-Specific       : Negative \u2717\n",
      "   Embedding + LR      : Negative \u2717\n",
      "   Zero-Shot           : Positive \u2713\n",
      "   Ensemble Decision:   Negative \u2717\n",
      "\n",
      "5. 'goldmember is funny enough to justify the embarrassment of b...'\n",
      "   True: Positive\n",
      "   Task-Specific       : Negative \u2717\n",
      "   Embedding + LR      : Negative \u2717\n",
      "   Zero-Shot           : Positive \u2713\n",
      "   Ensemble Decision:   Negative \u2717\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": "### Questions\n\n1. Did the ensemble beat the best individual model?\n\n2. When models disagree, which one is usually correct?\n\n3. Compare majority voting vs confidence-weighted voting. Which performed better?\n"
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": "#### Hard Task 4: Cross-Domain Transfer Learning\n\nTrain on movie reviews, test on restaurant/product/book reviews. See which domains transfer well.\n"
  },
  {
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "# Source domain: Movie reviews\n",
    "movie_data = load_dataset(\"rotten_tomatoes\")\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "movie_train = movie_data[\"train\"].shuffle(seed=42).select(range(2000))\n",
    "movie_test = movie_data[\"test\"].shuffle(seed=42).select(range(200))\n",
    "\n",
    "# Target domains with labeled examples\n",
    "restaurant_reviews = {\n",
    "    'text': [\n",
    "        \"Amazing food and excellent service!\",\n",
    "        \"Best restaurant in town, highly recommend\",\n",
    "        \"Delicious meals and great atmosphere\",\n",
    "        \"Outstanding cuisine and friendly staff\",\n",
    "        \"Terrible food, very disappointing\",\n",
    "        \"Awful service and poor quality\",\n",
    "        \"Not worth the money, mediocre at best\",\n",
    "        \"Disgusting food and rude waiters\",\n",
    "        \"The pasta was okay but nothing special\",\n",
    "        \"Decent place for a quick meal\"\n",
    "    ],\n",
    "    'label': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
    "}\n",
    "\n",
    "product_reviews = {\n",
    "    'text': [\n",
    "        \"This product is amazing! Works perfectly\",\n",
    "        \"Excellent quality, very satisfied\",\n",
    "        \"Great value for money, highly recommend\",\n",
    "        \"Perfect! Exactly what I needed\",\n",
    "        \"Terrible product, broke immediately\",\n",
    "        \"Waste of money, very poor quality\",\n",
    "        \"Doesn't work as advertised, disappointed\",\n",
    "        \"Awful, don't buy this\",\n",
    "        \"It's okay, does the job\",\n",
    "        \"Average product, nothing special\"\n",
    "    ],\n",
    "    'label': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
    "}\n",
    "\n",
    "book_reviews = {\n",
    "    'text': [\n",
    "        \"Brilliant book! Couldn't put it down\",\n",
    "        \"Masterfully written, highly engaging\",\n",
    "        \"One of the best books I've read\",\n",
    "        \"Fantastic story and great characters\",\n",
    "        \"Boring and poorly written\",\n",
    "        \"Terrible book, waste of time\",\n",
    "        \"Disappointing, not worth reading\",\n",
    "        \"Awful plot and weak characters\",\n",
    "        \"Decent read but nothing groundbreaking\",\n",
    "        \"It was fine, not great not terrible\"\n",
    "    ],\n",
    "    'label': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
    "}\n",
    "\n",
    "# TODO: Add your own domain - try something different!\n",
    "# YOUR_DOMAIN_reviews = {\n",
    "#     'text': [\n",
    "#         \"Positive example 1\",\n",
    "#         \"Positive example 2\",\n",
    "#         \"Positive example 3\",\n",
    "#         \"Positive example 4\",\n",
    "#         \"Negative example 1\",\n",
    "#         \"Negative example 2\",\n",
    "#         \"Negative example 3\",\n",
    "#         \"Negative example 4\",\n",
    "#         \"Neutral example 1\",\n",
    "#         \"Neutral example 2\",\n",
    "#     ],\n",
    "#     'label': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
    "# }\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CROSS-DOMAIN TRANSFER LEARNING EXPERIMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Train on source domain (movies)\n",
    "print(\"\\nTraining classifier on MOVIE REVIEWS (source domain)...\")\n",
    "train_embeddings = model.encode(movie_train[\"text\"], show_progress_bar=True)\n",
    "clf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "clf.fit(train_embeddings, movie_train[\"label\"])\n",
    "\n",
    "# Test on source domain (baseline)\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"BASELINE: Performance on Source Domain (Movies)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "movie_test_embeddings = model.encode(movie_test[\"text\"], show_progress_bar=False)\n",
    "movie_test_pred = clf.predict(movie_test_embeddings)\n",
    "source_f1 = f1_score(movie_test[\"label\"], movie_test_pred, average='weighted')\n",
    "\n",
    "print(f\"Source Domain F1: {source_f1:.4f}\")\n",
    "print(\"This is how well the classifier does on its training domain\")\n",
    "\n",
    "# Zero-shot transfer to target domains\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ZERO-SHOT TRANSFER TO TARGET DOMAINS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "target_domains = {\n",
    "    \"Restaurant Reviews\": restaurant_reviews,\n",
    "    \"Product Reviews\": product_reviews,\n",
    "    \"Book Reviews\": book_reviews,\n",
    "}\n",
    "\n",
    "# TODO: Add your domain if created\n",
    "# target_domains[\"YOUR DOMAIN\"] = YOUR_DOMAIN_reviews\n",
    "\n",
    "transfer_results = {}\n",
    "\n",
    "for domain_name, domain_data in target_domains.items():\n",
    "    print(f\"\\n{domain_name}:\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # Test without adaptation\n",
    "    domain_embeddings = model.encode(domain_data['text'])\n",
    "    domain_pred = clf.predict(domain_embeddings)\n",
    "\n",
    "    domain_f1 = f1_score(domain_data['label'], domain_pred, average='weighted')\n",
    "    domain_acc = accuracy_score(domain_data['label'], domain_pred)\n",
    "\n",
    "    print(f\"F1 Score: {domain_f1:.4f}\")\n",
    "    print(f\"Accuracy: {domain_acc:.4f}\")\n",
    "    print(f\"Performance Drop: {source_f1 - domain_f1:.4f} ({(source_f1-domain_f1)/source_f1*100:.1f}%)\")\n",
    "\n",
    "    # Show some predictions\n",
    "    print(f\"\\nExample predictions:\")\n",
    "    for i in range(3):\n",
    "        true_label = \"Positive\" if domain_data['label'][i] == 1 else \"Negative\"\n",
    "        pred_label = \"Positive\" if domain_pred[i] == 1 else \"Negative\"\n",
    "        correct = \"\" if domain_pred[i] == domain_data['label'][i] else \"\"\n",
    "\n",
    "        print(f\"  '{domain_data['text'][i][:50]}...'\")\n",
    "        print(f\"  True: {true_label} | Pred: {pred_label} {correct}\")\n",
    "\n",
    "    transfer_results[domain_name] = {\n",
    "        'zero_shot_f1': domain_f1,\n",
    "        'zero_shot_acc': domain_acc,\n",
    "        'embeddings': domain_embeddings,\n",
    "        'predictions': domain_pred\n",
    "    }\n",
    "\n",
    "# TODO: Uncomment to implement few-shot domain adaptation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEW-SHOT DOMAIN ADAPTATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"Strategy: Add first 4 examples from each target domain to training set\")\n",
    "\n",
    "adaptation_size = 4\n",
    "\n",
    "for domain_name, domain_data in target_domains.items():\n",
    "    print(f\"\\n{domain_name}:\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # Split domain data\n",
    "    adapt_texts = domain_data['text'][:adaptation_size]\n",
    "    adapt_labels = domain_data['label'][:adaptation_size]\n",
    "\n",
    "    test_texts = domain_data['text'][adaptation_size:]\n",
    "    test_labels = domain_data['label'][adaptation_size:]\n",
    "\n",
    "    # Combine source + adaptation examples\n",
    "    adapt_embeddings = model.encode(adapt_texts)\n",
    "    combined_embeddings = np.vstack([train_embeddings, adapt_embeddings])\n",
    "    combined_labels = list(movie_train[\"label\"]) + adapt_labels\n",
    "\n",
    "    # Retrain\n",
    "    clf_adapted = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    clf_adapted.fit(combined_embeddings, combined_labels)\n",
    "\n",
    "    # Test\n",
    "    test_embeddings = model.encode(test_texts)\n",
    "    adapted_pred = clf_adapted.predict(test_embeddings)\n",
    "    adapted_f1 = f1_score(test_labels, adapted_pred, average='weighted')\n",
    "\n",
    "    zero_shot_f1 = transfer_results[domain_name]['zero_shot_f1']\n",
    "    improvement = adapted_f1 - zero_shot_f1\n",
    "\n",
    "    print(f\"Zero-shot F1:  {zero_shot_f1:.4f}\")\n",
    "    print(f\"Adapted F1:    {adapted_f1:.4f}\")\n",
    "    print(f\"Improvement:   {improvement:+.4f}\")\n",
    "\n",
    "    if improvement > 0.05:\n",
    "        print(f\" Significant improvement! Domain adaptation helped a lot\")\n",
    "    elif improvement > 0:\n",
    "        print(f\" Slight improvement from adaptation\")\n",
    "    else:\n",
    "        print(f\" No improvement or slight degradation\")\n",
    "\n",
    "    transfer_results[domain_name]['adapted_f1'] = adapted_f1\n",
    "    transfer_results[domain_name]['improvement'] = improvement\n",
    "\n",
    "# Analyze domain similarity\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DOMAIN SIMILARITY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate domain centroids (average embedding)\n",
    "source_centroid = np.mean(train_embeddings, axis=0)\n",
    "\n",
    "print(\"\\nDomain distances from source (movie reviews):\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "domain_distances = []\n",
    "for domain_name, domain_data in target_domains.items():\n",
    "    domain_embeddings = transfer_results[domain_name]['embeddings']\n",
    "    domain_centroid = np.mean(domain_embeddings, axis=0)\n",
    "\n",
    "    distance = np.linalg.norm(source_centroid - domain_centroid)\n",
    "    zero_shot_f1 = transfer_results[domain_name]['zero_shot_f1']\n",
    "    drop = source_f1 - zero_shot_f1\n",
    "\n",
    "    domain_distances.append((domain_name, distance, drop))\n",
    "\n",
    "    print(f\"\\n{domain_name}:\")\n",
    "    print(f\"  Embedding distance: {distance:.4f}\")\n",
    "    print(f\"  Performance drop:   {drop:.4f}\")\n",
    "    print(f\"  Zero-shot F1:       {zero_shot_f1:.4f}\")\n",
    "\n",
    "    if 'improvement' in transfer_results[domain_name]:\n",
    "        improvement = transfer_results[domain_name]['improvement']\n",
    "        print(f\"  Adaptation gain:    {improvement:+.4f}\")\n",
    "\n",
    "# Correlation analysis\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Correlation: Distance vs Performance\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "domain_distances.sort(key=lambda x: x[1])\n",
    "print(\"\\nRanked by distance to source:\")\n",
    "for name, dist, drop in domain_distances:\n",
    "    print(f\"  {name:20s}: distance={dist:.3f}, drop={drop:.3f}\")\n",
    "\n",
    "print(\"\\nObservation:\")\n",
    "print(\"   Domains closer to movies in embedding space tend to transfer better\")\n",
    "print(\"   Larger embedding distance correlates with larger performance drop\")\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRANSFER LEARNING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n{'Domain':<20s} {'Zero-Shot F1':<15s} {'Adapted F1':<15s} {'Improvement':<12s}\")\n",
    "print(\"-\"*65)\n",
    "for domain_name in target_domains.keys():\n",
    "    zero_f1 = transfer_results[domain_name]['zero_shot_f1']\n",
    "    adapted_f1 = transfer_results[domain_name].get('adapted_f1', 0)\n",
    "    improvement = transfer_results[domain_name].get('improvement', 0)\n",
    "\n",
    "    marker = \"\" if improvement > 0.05 else \"\"\n",
    "    print(f\"{domain_name:<20s} {zero_f1:.4f}          {adapted_f1:.4f}          {improvement:+.4f}     {marker}\")\n",
    "\n",
    "print(f\"\\nSource (Movies):      {source_f1:.4f}          N/A             N/A\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "fa4e26da49ab490391057153659fe544",
      "199c9d5da4bf47baba405bc6555d9aba",
      "8a74df22f6e641f4812f2cc0f219039d",
      "0b2925cbaf3443758b52afbe41abd2f1",
      "aeda2644bfdd451198ea472fe8e7cf78",
      "86237397fec642049ff500bfa6cba51c",
      "ab5f53b292cd45f792ba5a717fa4720a",
      "318a40cb79bd49cc8c0a4b99cd9fe81b",
      "1582e826db4d4eecb28283bdbb088806",
      "cc5281d351a74d1db6f7a1c6845d61df",
      "593f99ba62cf4a2c980a706f21f6c4cf"
     ]
    },
    "id": "NWvx0lRMHmAv",
    "outputId": "434846c2-b49b-4a10-bb3e-d9d71c3b6761"
   },
   "execution_count": 38,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "================================================================================\n",
      "CROSS-DOMAIN TRANSFER LEARNING EXPERIMENT\n",
      "================================================================================\n",
      "\n",
      "Training classifier on MOVIE REVIEWS (source domain)...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/63 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fa4e26da49ab490391057153659fe544"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "BASELINE: Performance on Source Domain (Movies)\n",
      "--------------------------------------------------------------------------------\n",
      "Source Domain F1: 0.8497\n",
      "This is how well the classifier does on its training domain\n",
      "\n",
      "================================================================================\n",
      "ZERO-SHOT TRANSFER TO TARGET DOMAINS\n",
      "================================================================================\n",
      "\n",
      "Restaurant Reviews:\n",
      "------------------------------------------------------------\n",
      "F1 Score: 0.9010\n",
      "Accuracy: 0.9000\n",
      "Performance Drop: -0.0513 (-6.0%)\n",
      "\n",
      "Example predictions:\n",
      "  'Amazing food and excellent service!...'\n",
      "  True: Positive | Pred: Positive \u2713\n",
      "  'Best restaurant in town, highly recommend...'\n",
      "  True: Positive | Pred: Positive \u2713\n",
      "  'Delicious meals and great atmosphere...'\n",
      "  True: Positive | Pred: Positive \u2713\n",
      "\n",
      "Product Reviews:\n",
      "------------------------------------------------------------\n",
      "F1 Score: 1.0000\n",
      "Accuracy: 1.0000\n",
      "Performance Drop: -0.1503 (-17.7%)\n",
      "\n",
      "Example predictions:\n",
      "  'This product is amazing! Works perfectly...'\n",
      "  True: Positive | Pred: Positive \u2713\n",
      "  'Excellent quality, very satisfied...'\n",
      "  True: Positive | Pred: Positive \u2713\n",
      "  'Great value for money, highly recommend...'\n",
      "  True: Positive | Pred: Positive \u2713\n",
      "\n",
      "Book Reviews:\n",
      "------------------------------------------------------------\n",
      "F1 Score: 0.9010\n",
      "Accuracy: 0.9000\n",
      "Performance Drop: -0.0513 (-6.0%)\n",
      "\n",
      "Example predictions:\n",
      "  'Brilliant book! Couldn't put it down...'\n",
      "  True: Positive | Pred: Positive \u2713\n",
      "  'Masterfully written, highly engaging...'\n",
      "  True: Positive | Pred: Positive \u2713\n",
      "  'One of the best books I've read...'\n",
      "  True: Positive | Pred: Positive \u2713\n",
      "\n",
      "================================================================================\n",
      "FEW-SHOT DOMAIN ADAPTATION\n",
      "================================================================================\n",
      "Strategy: Add first 4 examples from each target domain to training set\n",
      "\n",
      "Restaurant Reviews:\n",
      "------------------------------------------------------------\n",
      "Zero-shot F1:  0.9010\n",
      "Adapted F1:    0.9091\n",
      "Improvement:   +0.0081\n",
      "\u2192 Slight improvement from adaptation\n",
      "\n",
      "Product Reviews:\n",
      "------------------------------------------------------------\n",
      "Zero-shot F1:  1.0000\n",
      "Adapted F1:    1.0000\n",
      "Improvement:   +0.0000\n",
      "\u2192 No improvement or slight degradation\n",
      "\n",
      "Book Reviews:\n",
      "------------------------------------------------------------\n",
      "Zero-shot F1:  0.9010\n",
      "Adapted F1:    0.9091\n",
      "Improvement:   +0.0081\n",
      "\u2192 Slight improvement from adaptation\n",
      "\n",
      "================================================================================\n",
      "DOMAIN SIMILARITY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Domain distances from source (movie reviews):\n",
      "------------------------------------------------------------\n",
      "\n",
      "Restaurant Reviews:\n",
      "  Embedding distance: 0.7160\n",
      "  Performance drop:   -0.0513\n",
      "  Zero-shot F1:       0.9010\n",
      "  Adaptation gain:    +0.0081\n",
      "\n",
      "Product Reviews:\n",
      "  Embedding distance: 0.6779\n",
      "  Performance drop:   -0.1503\n",
      "  Zero-shot F1:       1.0000\n",
      "  Adaptation gain:    +0.0000\n",
      "\n",
      "Book Reviews:\n",
      "  Embedding distance: 0.5339\n",
      "  Performance drop:   -0.0513\n",
      "  Zero-shot F1:       0.9010\n",
      "  Adaptation gain:    +0.0081\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Correlation: Distance vs Performance\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Ranked by distance to source:\n",
      "  Book Reviews        : distance=0.534, drop=-0.051\n",
      "  Product Reviews     : distance=0.678, drop=-0.150\n",
      "  Restaurant Reviews  : distance=0.716, drop=-0.051\n",
      "\n",
      "Observation:\n",
      "  \u2192 Domains closer to movies in embedding space tend to transfer better\n",
      "  \u2192 Larger embedding distance correlates with larger performance drop\n",
      "\n",
      "================================================================================\n",
      "TRANSFER LEARNING SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Domain               Zero-Shot F1    Adapted F1      Improvement \n",
      "-----------------------------------------------------------------\n",
      "Restaurant Reviews   0.9010          0.9091          +0.0081     \n",
      "Product Reviews      1.0000          1.0000          +0.0000     \n",
      "Book Reviews         0.9010          0.9091          +0.0081     \n",
      "\n",
      "Source (Movies):      0.8497          N/A             N/A\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": "### Questions\n\n1. Which domain transferred best from movies? Which worst?\n\n2. Do you see patterns in what transfers well vs what fails?\n\n3. After few-shot adaptation: Which domains benefited most from just 4 examples?\n"
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "yZLqm3S4IP2e"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}