{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iF8lDgqJnzyT"
      },
      "source": [
        "# Chapter 4: Text Classification - Hard Tasks\n",
        "\n",
        "This notebook tackles advanced classification challenges. You'll implement hierarchical multi-level classifiers for complex taxonomies, use active learning to minimize labeling costs, build ensemble classifiers to improve robustness, and apply transfer learning across domains. These techniques are essential for production-level NLP systems.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFpMVC20nzyW"
      },
      "source": [
        "---\n",
        "\n",
        "## Setup\n",
        "\n",
        "Run all cells in this section to set up the environment and load necessary data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LGW2SD-c864"
      },
      "source": [
        "### [OPTIONAL] - Installing Packages on <img src=\"https://colab.google/static/images/icons/colab.png\" width=100>\n",
        "\n",
        "\n",
        "If you are viewing this notebook on Google Colab (or any other cloud vendor), you need to **uncomment and run** the following codeblock to install the dependencies for this chapter:\n",
        "\n",
        "---\n",
        "\n",
        " **NOTE**: We will want to use a GPU to run the examples in this notebook. In Google Colab, go to\n",
        "**Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4**.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "N-PxmOIhc865"
      },
      "outputs": [],
      "source": [
        " %%capture\n",
        "!pip install transformers sentence-transformers openai\n",
        "!pip install -U datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJorXxPFnzyZ"
      },
      "source": [
        "### Data Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5phRS_z2U_3T",
        "outputId": "59cda1da-3535-4be9-ddca-57b95f391188"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 8530\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 1066\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 1066\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "# Load our data\n",
        "data = load_dataset(\"rotten_tomatoes\")\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GS7ZhZfGnzyc"
      },
      "source": [
        "### Helper Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "X0KyKHtqyjn3"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "def evaluate_performance(y_true, y_pred):\n",
        "    \"\"\"Create and print the classification report\"\"\"\n",
        "    performance = classification_report(\n",
        "        y_true, y_pred,\n",
        "        target_names=[\"Negative Review\", \"Positive Review\"]\n",
        "    )\n",
        "    print(performance)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Your Turn - Text Classification Experiments\n",
        "\n",
        "Run each task first to see the baseline results. Follow the instructions to modify and experiment."
      ],
      "metadata": {
        "id": "NKYNfoaVC4hU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section is divided into EASY, MEDIUM, & HARD."
      ],
      "metadata": {
        "id": "hHVONn85DElL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpiSzld6nzyh"
      },
      "source": [
        "---\n",
        "\n",
        "## Hard Tasks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hard Tasks - Advanced Classification Challenges\n",
        "\n",
        "These tasks require significant modifications and deeper understanding. Take your time and experiment"
      ],
      "metadata": {
        "id": "sOcbrBwdGZbi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F649-TFqnzyi"
      },
      "source": [
        "#### Hard Task 1: Hierarchical Multi-Level Classifier\n",
        "Instead of flat classification (choosing from all categories at once), hierarchical classification makes decisions in steps: first broad categories, then fine-grained ones. This mirrors how humans often reason.\n",
        "Instructions:\n",
        "1. Run the 2-level classifier (Sentiment → Specific Aspect)\n",
        "2. Compare with flat classification to see confidence differences\n",
        "3. Try adding a 3rd level by uncommenting the code\n",
        "4. Analyze whether breaking decisions into steps helps or hurts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "rYMpBGqonzyj"
      },
      "outputs": [],
      "source": [
        "# Level 1: Broad sentiment\n",
        "level1_labels = [\n",
        "    \"negative sentiment review\",\n",
        "    \"positive sentiment review\"\n",
        "]\n",
        "# Level 2: Specific aspects (conditional on Level 1)\n",
        "level2_negative = [\n",
        "    \"review criticizing entertainment value and pacing\",\n",
        "    \"review criticizing technical quality and production\"\n",
        "]\n",
        "level2_positive = [\n",
        "    \"review praising technical quality and artistry\",\n",
        "    \"review praising entertainment value and enjoyment\"\n",
        "]\n",
        "# TODO: Add Level 3 for even finer granularity\n",
        "# level3_positive_quality = [...]\n",
        "# level3_positive_entertainment = [...]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFgHMLAqnzyk"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "\n",
        "test_reviews = [\n",
        "    \"Amazing cinematography and brilliant direction!\",\n",
        "    \"Terrible pacing, very boring throughout\",\n",
        "    \"Excellent acting but weak storyline\",\n",
        "    \"Poor production quality, disappointing visuals\"\n",
        "]"
      ],
      "outputs": [],
      "execution_count": 40
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbnbmKYOnzyk"
      },
      "source": [
        "def hierarchical_classify_2level(review):\n",
        "    \"\"\"\n",
        "    Classify review using 2-level hierarchy:\n",
        "    Level 1: Sentiment (positive/negative)\n",
        "    Level 2: Specific aspect (quality/entertainment)\n",
        "    \"\"\"\n",
        "    # Level 1: Determine sentiment\n",
        "    level1_emb = model.encode(level1_labels)\n",
        "    review_emb = model.encode([review])\n",
        "    level1_sim = cosine_similarity(review_emb, level1_emb)[0]\n",
        "\n",
        "    level1_pred = np.argmax(level1_sim)\n",
        "    level1_conf = level1_sim[level1_pred]\n",
        "    level1_label = level1_labels[level1_pred]\n",
        "\n",
        "    # Level 2: Conditional on Level 1\n",
        "    if level1_pred == 0:  # Negative\n",
        "        level2_labels = level2_negative\n",
        "        path = \"Negative → \"\n",
        "    else:  # Positive\n",
        "        level2_labels = level2_positive\n",
        "        path = \"Positive → \"\n",
        "\n",
        "    level2_emb = model.encode(level2_labels)\n",
        "    level2_sim = cosine_similarity(review_emb, level2_emb)[0]\n",
        "\n",
        "    level2_pred = np.argmax(level2_sim)\n",
        "    level2_conf = level2_sim[level2_pred]\n",
        "    level2_label = level2_labels[level2_pred]\n",
        "    path += level2_label\n",
        "\n",
        "    return {\n",
        "        \"level1_label\": level1_label,\n",
        "        \"level1_conf\": level1_conf,\n",
        "        \"level2_label\": level2_label,\n",
        "        \"level2_conf\": level2_conf,\n",
        "        \"path\": path\n",
        "    }"
      ],
      "outputs": [],
      "execution_count": 41
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bm1JW3gBnzym"
      },
      "source": [
        "Classify the reviews:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "_Hg7sMmhnzym",
        "outputId": "70765e3a-d2fd-4ded-fd44-6e0fc4a82db6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HIERARCHICAL CLASSIFICATION (2 LEVELS)\n",
            "\n",
            "Review 1: 'Amazing cinematography and brilliant direction!'\n",
            "\n",
            "  Level 1 (Sentiment):\n",
            "     positive sentiment review\n",
            "     Confidence: 0.148\n",
            "\n",
            "  Level 2 (Specific Aspect):\n",
            "     review praising technical quality and artistry\n",
            "     Confidence: 0.306\n",
            "\n",
            "  Final Classification Path:\n",
            "     Positive → review praising technical quality and artistry\n",
            "\n",
            "Review 2: 'Terrible pacing, very boring throughout'\n",
            "\n",
            "  Level 1 (Sentiment):\n",
            "     negative sentiment review\n",
            "     Confidence: 0.295\n",
            "\n",
            "  Level 2 (Specific Aspect):\n",
            "     review criticizing entertainment value and pacing\n",
            "     Confidence: 0.651\n",
            "\n",
            "  Final Classification Path:\n",
            "     Negative → review criticizing entertainment value and pacing\n",
            "\n",
            "Review 3: 'Excellent acting but weak storyline'\n",
            "\n",
            "  Level 1 (Sentiment):\n",
            "     negative sentiment review\n",
            "     Confidence: 0.275\n",
            "\n",
            "  Level 2 (Specific Aspect):\n",
            "     review criticizing entertainment value and pacing\n",
            "     Confidence: 0.507\n",
            "\n",
            "  Final Classification Path:\n",
            "     Negative → review criticizing entertainment value and pacing\n",
            "\n",
            "Review 4: 'Poor production quality, disappointing visuals'\n",
            "\n",
            "  Level 1 (Sentiment):\n",
            "     negative sentiment review\n",
            "     Confidence: 0.307\n",
            "\n",
            "  Level 2 (Specific Aspect):\n",
            "     review criticizing technical quality and production\n",
            "     Confidence: 0.479\n",
            "\n",
            "  Final Classification Path:\n",
            "     Negative → review criticizing technical quality and production\n",
            "\n",
            "COMPARISON: Hierarchical vs Flat Classification\n",
            "\n",
            "Showing first 3 reviews:\n",
            "\n",
            "Review: 'Amazing cinematography and brilliant direction!...'\n",
            "  Hierarchical: review praising technical quality and artistry\n",
            "     Confidence: 0.306\n",
            "  Flat:         review praising technical quality and artistry\n",
            "     Confidence: 0.306\n",
            "  Confidence Diff: -0.000\n",
            "\n",
            "Review: 'Terrible pacing, very boring throughout...'\n",
            "  Hierarchical: review criticizing entertainment value and pacing\n",
            "     Confidence: 0.651\n",
            "  Flat:         review criticizing entertainment value and pacing\n",
            "     Confidence: 0.651\n",
            "  Confidence Diff: +0.000\n",
            "\n",
            "Review: 'Excellent acting but weak storyline...'\n",
            "  Hierarchical: review criticizing entertainment value and pacing\n",
            "     Confidence: 0.507\n",
            "  Flat:         review criticizing entertainment value and pacing\n",
            "     Confidence: 0.507\n",
            "  Confidence Diff: +0.000\n"
          ]
        }
      ],
      "source": [
        "print(\"HIERARCHICAL CLASSIFICATION (2 LEVELS)\")\n",
        "for i, review in enumerate(test_reviews):\n",
        "    result = hierarchical_classify_2level(review)\n",
        "    print(f\"\\nReview {i+1}: '{review}'\")\n",
        "    print(f\"\\n  Level 1 (Sentiment):\")\n",
        "    print(f\"     {result['level1_label']}\")\n",
        "    print(f\"     Confidence: {result['level1_conf']:.3f}\")\n",
        "    print(f\"\\n  Level 2 (Specific Aspect):\")\n",
        "    print(f\"     {result['level2_label']}\")\n",
        "    print(f\"     Confidence: {result['level2_conf']:.3f}\")\n",
        "    print(f\"\\n  Final Classification Path:\")\n",
        "    print(f\"     {result['path']}\")\n",
        "# Compare with flat classification\n",
        "print()\n",
        "print(\"COMPARISON: Hierarchical vs Flat Classification\")\n",
        "# Flat: All 4 categories at once\n",
        "flat_labels = [\n",
        "    \"review criticizing entertainment value and pacing\",      # 0\n",
        "    \"review criticizing technical quality and production\",    # 1\n",
        "    \"review praising technical quality and artistry\",         # 2\n",
        "    \"review praising entertainment value and enjoyment\"       # 3\n",
        "]\n",
        "flat_embeddings = model.encode(flat_labels)\n",
        "review_embeddings = model.encode(test_reviews)\n",
        "flat_sim = cosine_similarity(review_embeddings, flat_embeddings)\n",
        "print(\"\\nShowing first 3 reviews:\")\n",
        "for i in range(min(3, len(test_reviews))):\n",
        "    hier_result = hierarchical_classify_2level(test_reviews[i])\n",
        "    flat_pred = np.argmax(flat_sim[i])\n",
        "    flat_conf = flat_sim[i][flat_pred]\n",
        "    print(f\"\\nReview: '{test_reviews[i][:50]}...'\")\n",
        "    print(f\"  Hierarchical: {hier_result['level2_label']}\")\n",
        "    print(f\"     Confidence: {hier_result['level2_conf']:.3f}\")\n",
        "    print(f\"  Flat:         {flat_labels[flat_pred]}\")\n",
        "    print(f\"     Confidence: {flat_conf:.3f}\")\n",
        "    print(f\"  Confidence Diff: {hier_result['level2_conf'] - flat_conf:+.3f}\")\n",
        "# TODO: After implementing 3-level, uncomment to test it:\n",
        "# print()\n",
        "# print(\"TESTING 3-LEVEL HIERARCHICAL CLASSIFICATION\")\n",
        "# for i, review in enumerate(test_reviews):\n",
        "#     result = hierarchical_classify_3level(review)\n",
        "#     print(f\"\\n{i+1}. '{review[:60]}...'\")\n",
        "#     print(f\"   Path: {result['path']}\")\n",
        "#     print(f\"   Level 3 confidence: {result['level3_conf']:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hierarchical approach makes decisions in stages:\n",
        "1. Level 1: Determines if the review is positive or negative\n",
        "2. Level 2: Based on that sentiment, classifies the specific aspect\n",
        "\n",
        "Notice the confidence scores at each level. The hierarchical classifier's final confidence is often higher than flat classification because it's making simpler decisions at each step.\n",
        "\n",
        "Looking at the comparison between hierarchical and flat approaches:\n",
        "- Hierarchical often has higher confidence (makes easier per-step decisions)\n",
        "- But if Level 1 is wrong, Level 2 has no chance to correct it\n",
        "- Flat classification considers all options at once but may be less confident"
      ],
      "metadata": {
        "id": "DOgvT7Ffnzyn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wea-7Goznzyo"
      },
      "source": [
        "#### Hard Task 2: Active Learning to Minimize Labeling Costs\n",
        "Labeling data is expensive. Active learning strategically selects the most informative samples to label, potentially saving 50%+ of labeling effort compared to random selection. Run the simulation to see active learning vs random sampling, observe which samples the model finds \"uncertain\", and track how many samples each approach needs to reach F1=0.85. Try implementing the alternative selection strategy (margin sampling) and compare labeling cost savings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHKseH6znzyo",
        "outputId": "19d5c2f9-05f5-4a35-96a2-9d5c4772dc8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Simulate active learning vs random sampling\n",
        "print(\"ACTIVE LEARNING SIMULATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Setup\n",
        "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "train_data = data[\"train\"].shuffle(seed=42).select(range(1000))\n",
        "test_data = data[\"test\"].shuffle(seed=42).select(range(200))\n",
        "\n",
        "# Start with small labeled set\n",
        "labeled_size = 50\n",
        "target_f1 = 0.85\n",
        "\n",
        "print(f\"Starting with {labeled_size} labeled samples\")\n",
        "print(f\"Target F1: {target_f1}\")\n",
        "print(f\"\\nNote: This is a simplified simulation\")\n",
        "print(\"In real active learning, you'd query an oracle (human) for labels\\n\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACTIVE LEARNING SIMULATION\n",
            "============================================================\n",
            "Starting with 50 labeled samples\n",
            "Target F1: 0.85\n",
            "\n",
            "Note: This is a simplified simulation\n",
            "In real active learning, you'd query an oracle (human) for labels\n",
            "\n"
          ]
        }
      ],
      "execution_count": 43
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gNoUfj4tQUU",
        "outputId": "82863c05-19f1-4f52-cde6-d05524c167ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Encode all training and test data upfront\n",
        "all_train_embeddings = model.encode(train_data[\"text\"], show_progress_bar=False)\n",
        "all_test_embeddings = model.encode(test_data[\"text\"], show_progress_bar=False)\n",
        "\n",
        "print(f\"Encoded {len(train_data)} training samples\")\n",
        "print(f\"Encoded {len(test_data)} test samples\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded 1000 training samples\n",
            "Encoded 200 test samples\n"
          ]
        }
      ],
      "execution_count": 44
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4sI4kQktQUV"
      },
      "source": [
        "Start with a small labeled set and keep rest unlabeled:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c-AdbT1tQUW",
        "outputId": "677a79ff-31fc-49b4-c4c7-b77b5bc85628",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "labeled_indices = list(range(labeled_size))  # First 50 samples\n",
        "unlabeled_indices = list(range(labeled_size, len(train_data)))  # Rest are unlabeled\n",
        "\n",
        "print(f\"Starting labeled set: {len(labeled_indices)} samples\")\n",
        "print(f\"Unlabeled pool: {len(unlabeled_indices)} samples\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting labeled set: 50 samples\n",
            "Unlabeled pool: 950 samples\n"
          ]
        }
      ],
      "execution_count": 45
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVeK6HAttQUX"
      },
      "source": [
        "Run active learning iterations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEzaSddptQUX",
        "outputId": "b3f1c66e-ae3a-4d67-f87c-b051e5855326",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"\\nIteration | Labeled Size | F1 Score\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "for iteration in range(5):\n",
        "    # Train on current labeled data\n",
        "    X_train = all_train_embeddings[labeled_indices]\n",
        "    y_train = [train_data[\"label\"][i] for i in labeled_indices]\n",
        "\n",
        "    clf = LogisticRegression(random_state=42, max_iter=1000)\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    y_pred = clf.predict(all_test_embeddings)\n",
        "    current_f1 = f1_score(test_data[\"label\"], y_pred, average='weighted')\n",
        "\n",
        "    print(f\"{iteration:9d} | {len(labeled_indices):12d} | {current_f1:.4f}\")\n",
        "\n",
        "    if current_f1 >= target_f1:\n",
        "        print(f\"\\nReached target F1={target_f1}!\")\n",
        "        break\n",
        "\n",
        "    # Select most uncertain samples from unlabeled pool\n",
        "    if len(unlabeled_indices) > 0:\n",
        "        unlabeled_embeddings = all_train_embeddings[unlabeled_indices]\n",
        "        probs = clf.predict_proba(unlabeled_embeddings)\n",
        "\n",
        "        # Uncertainty = 1 - max(probability)\n",
        "        uncertainties = 1 - np.max(probs, axis=1)\n",
        "\n",
        "        # Select 20 most uncertain samples\n",
        "        n_select = min(20, len(unlabeled_indices))\n",
        "        most_uncertain = np.argsort(uncertainties)[-n_select:]\n",
        "        selected_indices = [unlabeled_indices[idx] for idx in most_uncertain]\n",
        "\n",
        "        # Move to labeled set\n",
        "        labeled_indices.extend(selected_indices)\n",
        "        unlabeled_indices = [idx for idx in unlabeled_indices if idx not in selected_indices]"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration | Labeled Size | F1 Score\n",
            "----------------------------------------\n",
            "        0 |           50 | 0.7573\n",
            "        1 |           70 | 0.8052\n",
            "        2 |           90 | 0.7426\n",
            "        3 |          110 | 0.8072\n",
            "        4 |          130 | 0.8552\n",
            "\n",
            "Reached target F1=0.85!\n"
          ]
        }
      ],
      "execution_count": 46
    },
    {
      "cell_type": "markdown",
      "source": [
        "Active learning iteratively:\n",
        "1. Trains on currently labeled data\n",
        "2. Finds the most uncertain unlabeled samples\n",
        "3. \"Labels\" those samples (adds them to training set)\n",
        "4. Repeats\n",
        "\n",
        "The uncertain samples being selected typically have prediction probabilities close to 50-50. These are the most informative because they lie near the decision boundary.\n",
        "\n",
        "Compare the learning curves to see how quickly each approach improves. Active learning often reaches target performance with fewer labeled samples, saving significant labeling costs."
      ],
      "metadata": {
        "id": "yzYVwXTBHOrA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R98vphjNnzyp"
      },
      "source": [
        "#### Hard Task 3: Ensemble Classifier for Improved Robustness\n",
        "\n",
        "Ensemble methods combine multiple models to reduce individual biases and improve reliability. The wisdom of crowds principle: multiple imperfect models together often beat any single model.\n",
        "\n",
        "Instructions:\n",
        "1. Run to see 3 individual models compared to ensemble methods\n",
        "2. Compare simple majority voting vs confidence-weighted voting\n",
        "3. Examine disagreement cases - when models disagree, which is usually right?\n",
        "4. Optionally add a 4th model and performance-weighted voting\n",
        "5. Determine if the ensemble beats the best individual model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WD4KOy8jnzyq",
        "outputId": "765b12ba-118e-4469-d817-4e013993bf58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "print(\"ENSEMBLE CLASSIFIER COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load 3 different models\n",
        "model1 = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "model2 = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "model3 = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L3-v2')\n",
        "\n",
        "models = [model1, model2, model3]\n",
        "model_names = [\"MPNet\", \"MiniLM-L6\", \"MiniLM-L3\"]\n",
        "\n",
        "print(f\"Using {len(models)} models:\\n\")\n",
        "for name in model_names:\n",
        "    print(f\"  - {name}\")\n",
        "\n",
        "# Train each model\n",
        "train_subset = data[\"train\"].shuffle(seed=42).select(range(1000))\n",
        "test_subset = data[\"test\"].shuffle(seed=42).select(range(200))\n",
        "\n",
        "print(f\"\\nTraining on {len(train_subset)} samples...\")\n",
        "print(\"Note: In practice, ensemble diversity comes from:\")\n",
        "print(\"  - Different model architectures\")\n",
        "print(\"  - Different training data subsets\")\n",
        "print(\"  - Different hyperparameters\\n\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENSEMBLE CLASSIFIER COMPARISON\n",
            "============================================================\n",
            "Using 3 models:\n",
            "\n",
            "  - MPNet\n",
            "  - MiniLM-L6\n",
            "  - MiniLM-L3\n",
            "\n",
            "Training on 1000 samples...\n",
            "Note: In practice, ensemble diversity comes from:\n",
            "  - Different model architectures\n",
            "  - Different training data subsets\n",
            "  - Different hyperparameters\n",
            "\n"
          ]
        }
      ],
      "execution_count": 47
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NXodRdVtQUb"
      },
      "source": [
        "Train each model individually and track performance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Z7qnySgtQUc",
        "outputId": "c9b22b11-112b-44c6-a7d2-6b3f74d23bb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "classifiers = []\n",
        "individual_f1s = []\n",
        "\n",
        "print(\"Training individual models...\\n\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training individual models...\n",
            "\n"
          ]
        }
      ],
      "execution_count": 48
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLXRUDIJtQUd",
        "outputId": "56e6a8ca-a120-42b6-bdda-4c5ea4e08a18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i, (model, name) in enumerate(zip(models, model_names)):\n",
        "    # Encode with this model\n",
        "    train_emb = model.encode(train_subset[\"text\"], show_progress_bar=False)\n",
        "    test_emb = model.encode(test_subset[\"text\"], show_progress_bar=False)\n",
        "\n",
        "    # Train classifier\n",
        "    clf = LogisticRegression(random_state=42, max_iter=1000)\n",
        "    clf.fit(train_emb, train_subset[\"label\"])\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred = clf.predict(test_emb)\n",
        "    f1 = f1_score(test_subset[\"label\"], y_pred, average='weighted')\n",
        "\n",
        "    classifiers.append((clf, test_emb))\n",
        "    individual_f1s.append(f1)\n",
        "\n",
        "    print(f\"{name:12s}: F1 = {f1:.4f}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MPNet       : F1 = 0.8698\n",
            "MiniLM-L6   : F1 = 0.7300\n",
            "MiniLM-L3   : F1 = 0.6554\n"
          ]
        }
      ],
      "execution_count": 49
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbGPrIsktQUd"
      },
      "source": [
        "Collect predictions from all models for ensemble voting:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aj9MEcB3tQUf",
        "outputId": "d84b05a2-8a4c-4526-a854-e9a368a7222e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "all_predictions = []\n",
        "\n",
        "for clf, test_emb in classifiers:\n",
        "    preds = clf.predict(test_emb)\n",
        "    all_predictions.append(preds)\n",
        "\n",
        "all_predictions = np.array(all_predictions)\n",
        "print(f\"\\nCollected predictions from {len(all_predictions)} models\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collected predictions from 3 models\n"
          ]
        }
      ],
      "execution_count": 50
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8WbCrwltQUf"
      },
      "source": [
        "Apply majority voting to get ensemble predictions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7ieksGgtQUf",
        "outputId": "6ec5ffed-d1a8-4c06-dada-3f6f97e47bfb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ensemble_predictions = []\n",
        "\n",
        "for i in range(len(test_subset)):\n",
        "    votes = all_predictions[:, i]\n",
        "    unique, counts = np.unique(votes, return_counts=True)\n",
        "    majority = unique[np.argmax(counts)]  # Pick most common vote\n",
        "    ensemble_predictions.append(majority)\n",
        "\n",
        "ensemble_f1 = f1_score(test_subset[\"label\"], ensemble_predictions, average='weighted')\n",
        "print(f\"Ensemble F1: {ensemble_f1:.4f}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble F1: 0.7652\n"
          ]
        }
      ],
      "execution_count": 51
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcnCWwR0tQUf"
      },
      "source": [
        "Compare ensemble vs individual models:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oOfUqkJtQUg",
        "outputId": "7f321d64-461a-4e30-9e45-95831624f3e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"\\nRESULTS COMPARISON\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "for name, f1 in zip(model_names, individual_f1s):\n",
        "    print(f\"{name:12s}: {f1:.4f}\")\n",
        "\n",
        "print(f\"\\nEnsemble:     {ensemble_f1:.4f}\")\n",
        "\n",
        "best_individual = max(individual_f1s)\n",
        "if ensemble_f1 > best_individual:\n",
        "    gain = ensemble_f1 - best_individual\n",
        "    print(f\"\\n✓ Ensemble wins by {gain:.4f}!\")\n",
        "else:\n",
        "    diff = best_individual - ensemble_f1\n",
        "    print(f\"\\nBest individual model wins by {diff:.4f}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "RESULTS COMPARISON\n",
            "----------------------------------------\n",
            "MPNet       : 0.8698\n",
            "MiniLM-L6   : 0.7300\n",
            "MiniLM-L3   : 0.6554\n",
            "\n",
            "Ensemble:     0.7652\n",
            "\n",
            "Best individual model wins by 0.1046\n"
          ]
        }
      ],
      "execution_count": 52
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGa8u5Ghnzyq"
      },
      "source": [
        "#### Hard Task 4: Cross-Domain Transfer Learning\n",
        "\n",
        "Can a classifier trained on movie reviews work on restaurant, product, or book reviews? Observe zero-shot transfer (no adaptation) performance on each domain, see which domains transfer well and which don't, then try few-shot adaptation (adding just 4 examples from target domain). Analyze domain similarity using embedding distances and optionally add your own custom domain to test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhueGby3nzyr"
      },
      "source": [
        "Load dependencies and set up source/target domains:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "574s5OB8nzys"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import numpy as np"
      ],
      "outputs": [],
      "execution_count": 53
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9m7SiDmknzys"
      },
      "source": [
        "Define source domain (movies) and target domains:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NsPi1yMnzys"
      },
      "source": [
        "movie_data = load_dataset(\"rotten_tomatoes\")\n",
        "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "\n",
        "movie_train = movie_data[\"train\"].shuffle(seed=42).select(range(2000))\n",
        "movie_test = movie_data[\"test\"].shuffle(seed=42).select(range(200))"
      ],
      "outputs": [],
      "execution_count": 54
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmFlUawpnzyt"
      },
      "source": [
        "restaurant_reviews = {\n",
        "    'text': [\n",
        "        \"Amazing food and excellent service!\",\n",
        "        \"Best restaurant in town, highly recommend\",\n",
        "        \"Delicious meals and great atmosphere\",\n",
        "        \"Outstanding cuisine and friendly staff\",\n",
        "        \"Terrible food, very disappointing\",\n",
        "        \"Awful service and poor quality\",\n",
        "        \"Not worth the money, mediocre at best\",\n",
        "        \"Disgusting food and rude waiters\",\n",
        "        \"The pasta was okay but nothing special\",\n",
        "        \"Decent place for a quick meal\"\n",
        "    ],\n",
        "    'label': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
        "}\n",
        "\n",
        "product_reviews = {\n",
        "    'text': [\n",
        "        \"This product is amazing! Works perfectly\",\n",
        "        \"Excellent quality, very satisfied\",\n",
        "        \"Great value for money, highly recommend\",\n",
        "        \"Perfect! Exactly what I needed\",\n",
        "        \"Terrible product, broke immediately\",\n",
        "        \"Waste of money, very poor quality\",\n",
        "        \"Doesn't work as advertised, disappointed\",\n",
        "        \"Awful, don't buy this\",\n",
        "        \"It's okay, does the job\",\n",
        "        \"Average product, nothing special\"\n",
        "    ],\n",
        "    'label': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
        "}\n",
        "\n",
        "book_reviews = {\n",
        "    'text': [\n",
        "        \"Brilliant book! Couldn't put it down\",\n",
        "        \"Masterfully written, highly engaging\",\n",
        "        \"One of the best books I've read\",\n",
        "        \"Fantastic story and great characters\",\n",
        "        \"Boring and poorly written\",\n",
        "        \"Terrible book, waste of time\",\n",
        "        \"Disappointing, not worth reading\",\n",
        "        \"Awful plot and weak characters\",\n",
        "        \"Decent read but nothing groundbreaking\",\n",
        "        \"It was fine, not great not terrible\"\n",
        "    ],\n",
        "    'label': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
        "}"
      ],
      "outputs": [],
      "execution_count": 55
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJ7yP7KVnzyu"
      },
      "source": [
        "Train classifier on source domain (movies):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhM4esP1nzyv",
        "outputId": "36fa06d1-1963-470e-f3a0-eddfbdd38902",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102,
          "referenced_widgets": [
            "e9caf6e017e04589936dafdffae40f46",
            "418b51b010ef4f8fa97762d37b330528",
            "eb3bb31177fe4e0fb912ef193d8691c1",
            "233e24a784594a0a8980af6691e60b7d",
            "d19c6fcb37724ceba5a120d4b5a13af0",
            "617815fce9d5410b8b3397d319dc8c9a",
            "a619de040e5040cb99a1e8dc38b022a4",
            "b68bc7e78b9e4440a4ae3b1f980d0e82",
            "78d1bf6648ba4c7380d79b009ffc4c04",
            "7d2af482ad91462a9bec2fdd5d5bf9d7",
            "f52286206367426b915795a27c6f235c"
          ]
        }
      },
      "source": [
        "print(\"Training on movie reviews...\")\n",
        "train_embeddings = model.encode(movie_train[\"text\"], show_progress_bar=True)\n",
        "\n",
        "clf = LogisticRegression(random_state=42, max_iter=1000)\n",
        "clf.fit(train_embeddings, movie_train[\"label\"])\n",
        "\n",
        "# Test on source domain\n",
        "movie_test_embeddings = model.encode(movie_test[\"text\"], show_progress_bar=False)\n",
        "movie_test_pred = clf.predict(movie_test_embeddings)\n",
        "source_f1 = f1_score(movie_test[\"label\"], movie_test_pred, average='weighted')\n",
        "\n",
        "print(f\"\\nSource domain F1: {source_f1:.4f}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on movie reviews...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/63 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e9caf6e017e04589936dafdffae40f46"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Source domain F1: 0.8497\n"
          ]
        }
      ],
      "execution_count": 56
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEr0prtsnzyw"
      },
      "source": [
        "Test zero-shot transfer to target domains (no adaptation):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPD8PXiZnzyx",
        "outputId": "5125406a-66d0-4620-a723-702ee1e35974",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "target_domains = {\n",
        "    \"Restaurant\": restaurant_reviews,\n",
        "    \"Product\": product_reviews,\n",
        "    \"Book\": book_reviews,\n",
        "}\n",
        "\n",
        "transfer_results = {}\n",
        "\n",
        "print(\"\\nZero-shot transfer results:\")\n",
        "for domain_name, domain_data in target_domains.items():\n",
        "    domain_embeddings = model.encode(domain_data['text'])\n",
        "    domain_pred = clf.predict(domain_embeddings)\n",
        "    domain_f1 = f1_score(domain_data['label'], domain_pred, average='weighted')\n",
        "    domain_acc = accuracy_score(domain_data['label'], domain_pred)\n",
        "\n",
        "    print(f\"\\n{domain_name}: F1={domain_f1:.4f}, Acc={domain_acc:.4f}\")\n",
        "    print(f\"  Drop from source: {source_f1 - domain_f1:.4f}\")\n",
        "\n",
        "    transfer_results[domain_name] = {\n",
        "        'zero_shot_f1': domain_f1,\n",
        "        'embeddings': domain_embeddings\n",
        "    }"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Zero-shot transfer results:\n",
            "\n",
            "Restaurant: F1=0.9010, Acc=0.9000\n",
            "  Drop from source: -0.0513\n",
            "\n",
            "Product: F1=1.0000, Acc=1.0000\n",
            "  Drop from source: -0.1503\n",
            "\n",
            "Book: F1=0.9010, Acc=0.9000\n",
            "  Drop from source: -0.0513\n"
          ]
        }
      ],
      "execution_count": 57
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEPIxwrLnzyy"
      },
      "source": [
        "Few-shot adaptation (add 4 examples from target domain):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SRItXyjnzyy",
        "outputId": "2549d5ac-c6e9-469b-f788-5535573b9370",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "adaptation_size = 4\n",
        "\n",
        "print(\"\\nFew-shot adaptation (4 examples):\")\n",
        "for domain_name, domain_data in target_domains.items():\n",
        "    # Split: first 4 for adaptation, rest for testing\n",
        "    adapt_texts = domain_data['text'][:adaptation_size]\n",
        "    adapt_labels = domain_data['label'][:adaptation_size]\n",
        "    test_texts = domain_data['text'][adaptation_size:]\n",
        "    test_labels = domain_data['label'][adaptation_size:]\n",
        "\n",
        "    # Combine source + adaptation\n",
        "    adapt_embeddings = model.encode(adapt_texts)\n",
        "    combined_embeddings = np.vstack([train_embeddings, adapt_embeddings])\n",
        "    combined_labels = list(movie_train[\"label\"]) + adapt_labels\n",
        "\n",
        "    # Retrain\n",
        "    clf_adapted = LogisticRegression(random_state=42, max_iter=1000)\n",
        "    clf_adapted.fit(combined_embeddings, combined_labels)\n",
        "\n",
        "    # Test\n",
        "    test_embeddings = model.encode(test_texts)\n",
        "    adapted_pred = clf_adapted.predict(test_embeddings)\n",
        "    adapted_f1 = f1_score(test_labels, adapted_pred, average='weighted')\n",
        "\n",
        "    zero_shot_f1 = transfer_results[domain_name]['zero_shot_f1']\n",
        "    improvement = adapted_f1 - zero_shot_f1\n",
        "\n",
        "    print(f\"\\n{domain_name}:\")\n",
        "    print(f\"  Zero-shot: {zero_shot_f1:.4f}\")\n",
        "    print(f\"  Adapted:   {adapted_f1:.4f}\")\n",
        "    print(f\"  Gain:      {improvement:+.4f}\")\n",
        "\n",
        "    transfer_results[domain_name]['adapted_f1'] = adapted_f1\n",
        "    transfer_results[domain_name]['improvement'] = improvement"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Few-shot adaptation (4 examples):\n",
            "\n",
            "Restaurant:\n",
            "  Zero-shot: 0.9010\n",
            "  Adapted:   0.9091\n",
            "  Gain:      +0.0081\n",
            "\n",
            "Product:\n",
            "  Zero-shot: 1.0000\n",
            "  Adapted:   1.0000\n",
            "  Gain:      +0.0000\n",
            "\n",
            "Book:\n",
            "  Zero-shot: 0.9010\n",
            "  Adapted:   0.9091\n",
            "  Gain:      +0.0081\n"
          ]
        }
      ],
      "execution_count": 58
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yc6UDdfinzyz"
      },
      "source": [
        "Analyze domain similarity (embedding distance):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YMFCMYUnzyz",
        "outputId": "9806a500-49c3-441c-c90b-bf9fdad0c082",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "source_centroid = np.mean(train_embeddings, axis=0)\n",
        "\n",
        "print(\"\\nDomain distances from source:\")\n",
        "for domain_name in target_domains.keys():\n",
        "    domain_embeddings = transfer_results[domain_name]['embeddings']\n",
        "    domain_centroid = np.mean(domain_embeddings, axis=0)\n",
        "    distance = np.linalg.norm(source_centroid - domain_centroid)\n",
        "\n",
        "    zero_f1 = transfer_results[domain_name]['zero_shot_f1']\n",
        "    drop = source_f1 - zero_f1\n",
        "\n",
        "    print(f\"\\n{domain_name:12s}: distance={distance:.3f}, drop={drop:.3f}\")\n",
        "\n",
        "print(\"\\nObservation: Smaller distance = better transfer\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Domain distances from source:\n",
            "\n",
            "Restaurant  : distance=0.716, drop=-0.051\n",
            "\n",
            "Product     : distance=0.678, drop=-0.150\n",
            "\n",
            "Book        : distance=0.534, drop=-0.051\n",
            "\n",
            "Observation: Smaller distance = better transfer\n"
          ]
        }
      ],
      "execution_count": 59
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBJkCQOGnzy0"
      },
      "source": [
        "### Questions\n",
        "\n",
        "1. Which domain transferred best from movies? Which worst?\n",
        "\n",
        "2. Do you see patterns in what transfers well vs what fails?\n",
        "\n",
        "3. After few-shot adaptation: Which domains benefited most from just 4 examples?\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yZLqm3S4IP2e"
      },
      "execution_count": 59,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e9caf6e017e04589936dafdffae40f46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_418b51b010ef4f8fa97762d37b330528",
              "IPY_MODEL_eb3bb31177fe4e0fb912ef193d8691c1",
              "IPY_MODEL_233e24a784594a0a8980af6691e60b7d"
            ],
            "layout": "IPY_MODEL_d19c6fcb37724ceba5a120d4b5a13af0"
          }
        },
        "418b51b010ef4f8fa97762d37b330528": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_617815fce9d5410b8b3397d319dc8c9a",
            "placeholder": "​",
            "style": "IPY_MODEL_a619de040e5040cb99a1e8dc38b022a4",
            "value": "Batches: 100%"
          }
        },
        "eb3bb31177fe4e0fb912ef193d8691c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b68bc7e78b9e4440a4ae3b1f980d0e82",
            "max": 63,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_78d1bf6648ba4c7380d79b009ffc4c04",
            "value": 63
          }
        },
        "233e24a784594a0a8980af6691e60b7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d2af482ad91462a9bec2fdd5d5bf9d7",
            "placeholder": "​",
            "style": "IPY_MODEL_f52286206367426b915795a27c6f235c",
            "value": " 63/63 [03:44&lt;00:00,  1.32s/it]"
          }
        },
        "d19c6fcb37724ceba5a120d4b5a13af0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "617815fce9d5410b8b3397d319dc8c9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a619de040e5040cb99a1e8dc38b022a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b68bc7e78b9e4440a4ae3b1f980d0e82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78d1bf6648ba4c7380d79b009ffc4c04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7d2af482ad91462a9bec2fdd5d5bf9d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f52286206367426b915795a27c6f235c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}