{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Chapter 4: Text Classification - Solutions\n\nConcise solutions for all tasks (Easy, Medium, Hard)."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Setup"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "from datasets import load_dataset\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\n\ndata = load_dataset(\"rotten_tomatoes\")\nmodel = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Easy Task 1: Zero-Shot Classification"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "test_reviews = [\n    \"This movie was absolutely fantastic! A masterpiece!\",\n    \"Terrible waste of time. Very disappointing.\",\n    \"An okay film, nothing special but watchable.\",\n]\n\nlabels = [\"A negative movie review\", \"A positive movie review\"]\n\nlabel_embeddings = model.encode(labels)\nreview_embeddings = model.encode(test_reviews)\nsim_matrix = cosine_similarity(review_embeddings, label_embeddings)\n\nprint(\"Classification Results:\")\nfor i, review in enumerate(test_reviews):\n    prediction = np.argmax(sim_matrix[i])\n    confidence = sim_matrix[i][prediction]\n    margin = abs(sim_matrix[i][0] - sim_matrix[i][1])\n    print(f\"\\nReview {i+1}: '{review}'\")\n    print(f\"Predicted: {labels[prediction]}\")\n    print(f\"Confidence: {confidence:.3f}\")\n    print(f\"Margin: {margin:.3f}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Easy Task 2: Classifier Strategy"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "y_true = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])\nclassifier_conservative = np.array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1])\nclassifier_aggressive = np.array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1])\nclassifier_balanced = np.array([0, 0, 0, 1, 0, 0, 1, 1, 1, 1])\n\ndef analyze_classifier(name, y_true, y_pred):\n    cm = confusion_matrix(y_true, y_pred)\n    precision = precision_score(y_true, y_pred, zero_division=0)\n    recall = recall_score(y_true, y_pred, zero_division=0)\n    f1 = f1_score(y_true, y_pred, zero_division=0)\n    \n    print(f\"\\n{name}\")\n    print(f\"Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}\")\n    return precision, recall, f1\n\nresults = {}\nfor name, clf in [(\"Conservative\", classifier_conservative), \n                   (\"Aggressive\", classifier_aggressive),\n                   (\"Balanced\", classifier_balanced)]:\n    p, r, f = analyze_classifier(name, y_true, clf)\n    results[name] = (p, r, f)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Easy Task 3: Temperature Effects"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "original_probs = np.array([0.50, 0.30, 0.12, 0.05, 0.03])\ntokens = [\"positive\", \"negative\", \"neutral\", \"good\", \"bad\"]\n\ndef apply_temperature(probs, temperature):\n    if temperature == 0:\n        result = np.zeros_like(probs)\n        result[np.argmax(probs)] = 1.0\n        return result\n    logits = np.log(probs + 1e-10)\n    scaled_logits = logits / temperature\n    exp_logits = np.exp(scaled_logits)\n    return exp_logits / np.sum(exp_logits)\n\ntemperatures = [0, 0.5, 1.0, 2.0]\n\nfor temp in temperatures:\n    new_probs = apply_temperature(original_probs, temp)\n    print(f\"\\nTemperature = {temp}\")\n    for i, token in enumerate(tokens):\n        print(f\"  {token:10s}: {new_probs[i]:.3f}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Easy Task 4: Embedding Similarity"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "texts = [\n    \"Amazing movie! Absolutely loved it!\",\n    \"Fantastic film, highly recommend!\",\n    \"Great cinematography and acting\",\n    \"Terrible waste of time\",\n    \"Very disappointing and boring\",\n    \"Poor acting and weak plot\",\n    \"It was okay, nothing special\",\n    \"Some good parts, some bad parts\",\n    \"The weather is nice today\",\n    \"I like eating pizza\"\n]\n\nembeddings = model.encode(texts)\nsimilarity_matrix = cosine_similarity(embeddings)\n\nprint(f\"Each text: {embeddings.shape[1]}-dimensional vector\")\nprint(f\"\\nSimilarity between Text 1 and Text 2: {similarity_matrix[0][1]:.3f}\")\nprint(f\"Similarity between Text 1 and Text 4: {similarity_matrix[0][3]:.3f}\")\n\n# Find similar texts\npositive_idx = 0\nsimilar_to_positive = []\nfor i in range(len(texts)):\n    if i != positive_idx and similarity_matrix[positive_idx][i] > 0.5:\n        similar_to_positive.append((i, similarity_matrix[positive_idx][i]))\n\nprint(f\"\\nTexts similar to '{texts[positive_idx]}':\")\nfor idx, sim in sorted(similar_to_positive, key=lambda x: x[1], reverse=True):\n    print(f\"  Text {idx+1} (sim={sim:.3f}): '{texts[idx]}'\")"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}