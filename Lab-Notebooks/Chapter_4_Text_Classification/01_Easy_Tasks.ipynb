{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlH0PB_t39yg"
      },
      "source": "# Chapter 4: Text Classification - Easy Tasks\n\nThis notebook covers the basic text classification concepts: zero-shot classification, classifier strategies, temperature effects, and embedding similarity.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xus_y4Ga39yk"
      },
      "source": "\n## Setup\n\nRun all cells in this section to set up the environment and load necessary data.\n\nBefore running these cells, it is advised to first run and try to get familiar with the codes and concepts from the main Chapter 4 Notebook (`Start_Here.ipynb`).\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LGW2SD-c864"
      },
      "source": "### [OPTIONAL] - Installing Packages on <img src=\"https://colab.google/static/images/icons/colab.png\" width=100>\n\nIf you are viewing this notebook on Google Colab (or any other cloud vendor), you need to **uncomment and run** the following codeblock to install the dependencies for this chapter:\n\n---\n\n **NOTE**: We will want to use a GPU to run the examples in this notebook. In Google Colab, go to\n**Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4**.\n\n---\n"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "N-PxmOIhc865"
      },
      "outputs": [],
      "source": " %%capture\n!pip install transformers sentence-transformers openai\n!pip install -U datasets"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WToW_Psg39ys"
      },
      "source": "### Data Loading\n"
    },
    {
      "cell_type": "markdown",
      "source": "We use the same data as in Start_Here.ipynb notebook",
      "metadata": {
        "id": "TBJUHeDD4eAy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5phRS_z2U_3T",
        "outputId": "98e2ff6a-a108-4188-912e-3827295e9787"
      },
      "outputs": [],
      "source": "from datasets import load_dataset\n\n# Load our data\ndata = load_dataset(\"rotten_tomatoes\")\ndata"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXl6LASB39yw"
      },
      "source": "### Helper Functions\n"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "X0KyKHtqyjn3"
      },
      "outputs": [],
      "source": "from sklearn.metrics import classification_report\n\ndef evaluate_performance(y_true, y_pred):\n    \"\"\"Create and print the classification report\"\"\"\n    performance = classification_report(\n        y_true, y_pred,\n        target_names=[\"Negative Review\", \"Positive Review\"]\n    )\n    print(performance)"
    },
    {
      "cell_type": "markdown",
      "source": "## Challenges\n\nComplete the following tasks by implementing the TODO sections. Solutions are provided below each challenge (commented out).\n\nEach task includes:\n- Clear objective\n- Starter code with TODOs\n- Hints\n- Test assertions\n- Solution code (commented)",
      "metadata": {
        "id": "NKYNfoaVC4hU"
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Level: Easy\n\nThese challenges introduce core concepts. Implement the TODO sections to practice.",
      "metadata": {
        "id": "gwqUbziIDNee"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD982uIW39y-"
      },
      "source": "**About This Task:**\nZero-shot classification classifies text without training examples."
    },
    {
      "cell_type": "markdown",
      "source": "#### Easy Task 1: Zero-Shot Classifier\n\n### Instructions\n\n1. Execute the code to see baseline predictions for 3 basic reviews\n2. Uncomment the larger `test_reviews` list and run again to test harder cases\n3. Uncomment one label option to see how wording affects predictions\n4. Compare which label style works best for ambiguous reviews",
      "metadata": {
        "id": "TnHfnLsxDQcA"
      }
    },
    {
      "cell_type": "code",
      "source": "from sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np",
      "metadata": {
        "id": "ZC6N2A3LC6JZ"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')",
      "metadata": {
        "id": "_81W_w1d7Wc3"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Test reviews - THESE WORK AS-IS\ntest_reviews = [\n    \"This movie was absolutely fantastic! A masterpiece!\",\n    \"Terrible waste of time. Very disappointing.\",\n    \"An okay film, nothing special but watchable.\",\n]\n\n# Your task: Uncomment to test harder cases\n# test_reviews = [\n#     \"This movie was absolutely fantastic! A masterpiece!\",\n#     \"Terrible waste of time. Very disappointing.\",\n#     \"An okay film, nothing special but watchable.\",\n#     \"Oh great, another masterpiece... NOT!\",  # Sarcastic\n#     \"Boring.\",  # Very short\n#     \"Great acting but terrible plot.\",  # Mixed sentiment\n# ]",
      "metadata": {
        "id": "8d4ZihTT7Wc4"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5aFZ79f7Wc5"
      },
      "source": "# Label descriptions - THESE WORK AS-IS\nlabels = [\n    \"A negative movie review\",\n    \"A positive movie review\"\n]\n\n# Implement: Try different label options\n# labels = [\"negative\", \"positive\"]  # Option 1: Simple\n# labels = [\"bad movie review\", \"good movie review\"]  # Option 2: Different wording\n# labels = [\"a scathing negative movie review\", \"an enthusiastic positive movie review\"]  # Option 3: Detailed",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYlHWbyP7Wc7"
      },
      "source": "# Complete this: Encode the labels and reviews, then calculate cosine similarity\n# Hint: Use model.encode() for embeddings and cosine_similarity() for the matrix\nlabel_embeddings = None  # YOUR CODE HERE\nreview_embeddings = None  # YOUR CODE HERE\nsim_matrix = None  # YOUR CODE HERE\n\n# Test your implementation\nassert label_embeddings is not None, \"Encode the labels first!\"\nassert review_embeddings is not None, \"Encode the reviews!\"\nassert sim_matrix is not None, \"Calculate the similarity matrix!\"\n\n# Solution (uncomment if stuck):\n# label_embeddings = model.encode(labels)\n# review_embeddings = model.encode(test_reviews)\n# sim_matrix = cosine_similarity(review_embeddings, label_embeddings)",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "In the above code cell, we simply load and use the model on the test reviews. You are encouraged to run the above cell with the different labels option to see the difference similarity.",
      "metadata": {
        "id": "B-NX5qe-53ph"
      }
    },
    {
      "cell_type": "code",
      "source": "# Fill in: For each review, find the predicted label and print results\n# Hint: Use np.argmax() to find the highest similarity score\n\nprint(\"Classification Results:\")\n\nfor i, review in enumerate(test_reviews):\n    # Write code to: Find which label has the highest similarity\n    prediction = None  # YOUR CODE HERE (use np.argmax on sim_matrix[i])\n    confidence = None  # YOUR CODE HERE (get the max similarity score)\n    margin = None  # YOUR CODE HERE (difference between the two label scores)\n    \n    print(f\"\\nReview {i+1}: '{review}'\")\n    if prediction is not None:\n        print(f\"Predicted: {labels[prediction]}\")\n        print(f\"Confidence: {confidence:.3f}\")\n        print(f\"Scores -> Negative: {sim_matrix[i][0]:.3f}, Positive: {sim_matrix[i][1]:.3f}\")\n        print(f\"Margin: {margin:.3f}\")\n\n# Solution (uncomment if stuck):\n# for i, review in enumerate(test_reviews):\n#     prediction = np.argmax(sim_matrix[i])\n#     confidence = sim_matrix[i][prediction]\n#     margin = abs(sim_matrix[i][0] - sim_matrix[i][1])\n#     print(f\"\\nReview {i+1}: '{review}'\")\n#     print(f\"Predicted: {labels[prediction]}\")\n#     print(f\"Confidence: {confidence:.3f}\")\n#     print(f\"Scores -> Negative: {sim_matrix[i][0]:.3f}, Positive: {sim_matrix[i][1]:.3f}\")\n#     print(f\"Margin: {margin:.3f}\")",
      "metadata": {
        "id": "DeRNmhHE513y",
        "outputId": "e460cf8d-2812-4cc9-9ae5-94cd8885d68a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "To test other `test_reviews` or `labels`, comment the previous one. Make sure only one of the sets is uncommented.\n\nTip: to uncomment/comment lines, use `Ctrl + /`",
      "metadata": {
        "id": "NOucKIpW6kU1"
      }
    },
    {
      "cell_type": "markdown",
      "source": "\n\n---\n\n### Questions\n\n1. Why did the classifier fail on the sarcastic review (\"*Oh great, another masterpiece... NOT*\")? What semantic features did embeddings miss?\n\n2. Which reviews changed predictions when you modified label descriptions? Why are some reviews more sensitive to label wording than others?\n\n3. Which reviews have low confidence margins (<0.1)? What linguistic features make certain reviews harder to classify?\n\n---\n\n",
      "metadata": {
        "id": "hAfvoaoTDidJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1V2I1IWn39zB"
      },
      "source": "**About This Task:**\nDifferent classification strategies affect model behavior and performance."
    },
    {
      "cell_type": "markdown",
      "source": "#### Easy Task 2: Classifier Strategy Analysis\n\n### Instructions\n\n1. Execute code to see three pre-built classifiers (conservative, aggressive, balanced)\n2. Study each confusion matrix to identify error patterns\n3. Modify `classifier_yours` to create a very conservative classifier (precision > 0.9)\n4. Uncomment the TODO section to analyze your classifier\n5. Experiment with creating different strategy combinations",
      "metadata": {
        "id": "Rhuy6BY6DxDr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hq2Klrbc7WdA"
      },
      "source": "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\nimport numpy as np",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rp-T8r17WdB"
      },
      "source": "# True labels: first 5 are negative (0), last 5 are positive (1)\ny_true = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsisRIOY7WdD"
      },
      "source": "# Pre-built classifiers to analyze\nclassifier_conservative = np.array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1])  # Rarely predicts positive\nclassifier_aggressive = np.array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1])     # Often predicts positive\nclassifier_balanced = np.array([0, 0, 0, 1, 0, 0, 1, 1, 1, 1])       # Balanced approach",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5W09dFnS7WdE"
      },
      "source": "# Your task: Modify these predictions to make YOUR classifier\n# Try to achieve precision > 0.9 (be very selective about predicting 1)\nclassifier_yours = np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1])",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGiWgoFw7WdE"
      },
      "source": "def analyze_classifier(name, y_true, y_pred):\n    \"\"\"Analyze classifier performance with detailed breakdown\"\"\"\n    print(f\"\\n{name}\")\n    \n    # Implement: Calculate the confusion matrix\n    # Hint: Use confusion_matrix(y_true, y_pred)\n    cm = None  # YOUR CODE HERE\n    \n    # Test\n    assert cm is not None, \"Calculate the confusion matrix!\"\n    \n    # Show confusion matrix with labels\n    print(f\"\\nConfusion Matrix:\")\n    print(f\"                    Predicted Neg | Predicted Pos\")\n    print(f\"Actual Neg (0):          {cm[0][0]}       |       {cm[0][1]}  (False Positives)\")\n    print(f\"Actual Pos (1):          {cm[1][0]}       |       {cm[1][1]}  (True Positives)\")\n    print(f\"                   (False Negatives)\")\n    \n    # Complete this: Calculate precision, recall, and f1\n    # Hint: Use precision_score, recall_score, f1_score from sklearn\n    precision = None  # YOUR CODE HERE\n    recall = None  # YOUR CODE HERE\n    f1 = None  # YOUR CODE HERE\n    \n    if precision is not None and recall is not None and f1 is not None:\n        print(f\"\\nMetrics:\")\n        print(f\"Precision: {precision:.3f} = TP/(TP+FP) = {cm[1][1]}/({cm[1][1]}+{cm[0][1]})\")\n        print(f\"Recall:    {recall:.3f} = TP/(TP+FN) = {cm[1][1]}/({cm[1][1]}+{cm[1][0]})\")\n        print(f\"F1 Score:  {f1:.3f} = 2*(P*R)/(P+R)\")\n        \n        # Explain strategy\n        if precision > recall + 0.1:\n            print(f\"\\nStrategy: Conservative (few false alarms, misses some positives)\")\n        elif recall > precision + 0.1:\n            print(f\"\\nStrategy: Aggressive (finds most positives, many false alarms)\")\n        else:\n            print(f\"\\nStrategy: Balanced\")\n    \n    return precision, recall, f1\n\n# Solution (uncomment if stuck):\n# def analyze_classifier(name, y_true, y_pred):\n#     print(f\"\\n{name}\")\n#     cm = confusion_matrix(y_true, y_pred)\n#     precision = precision_score(y_true, y_pred, zero_division=0)\n#     recall = recall_score(y_true, y_pred, zero_division=0)\n#     f1 = f1_score(y_true, y_pred, zero_division=0)\n#     ... (rest of the function)",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhx0wVzS7WdG",
        "outputId": "90686dc7-2602-4531-8f41-fc315b6240bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": "# Analyze pre-built classifiers\nresults = {}\nfor name, classifier in [\n    (\"Conservative Classifier\", classifier_conservative),\n    (\"Aggressive Classifier\", classifier_aggressive),\n    (\"Balanced Classifier\", classifier_balanced),\n]:\n    p, r, f = analyze_classifier(name, y_true, classifier)\n    results[name] = (p, r, f)",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnHUyo877WdH"
      },
      "source": "# Fill in: Analyze your classifier\n# # print(\"Analyzing Your Classifier\")\n# # p, r, f = analyze_classifier(\"YOUR Classifier\", y_true, classifier_yours)\n# results[\"YOUR Classifier\"] = (p, r, f)",
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfgyvCyN7WdI",
        "outputId": "5c1571f8-8c73-47f2-cd1a-80db9bfd4947",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": "# Summary\nprint(\"Summary\")\nprint(f\"{'Classifier':<25} {'Precision':<12} {'Recall':<12} {'F1':<12}\")\nfor name, (p, r, f) in results.items():\n    print(f\"{name:<25} {p:.3f}        {r:.3f}       {f:.3f}\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Questions\n\n1. The conservative classifier has 2 false negatives. What real-world mistake does this represent? Provide a movie review example.\n\n2. What strategy did you use to achieve high precision in `classifier_yours`? Why does predicting positive less frequently increase precision?\n\n3. Which classifier won on F1 score? Why doesn't the aggressive classifier win despite high recall?",
      "metadata": {
        "id": "GHv-B_ebD8CU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUmXASWa39zF"
      },
      "source": "**About This Task:**\nTemperature controls randomness in language model outputs."
    },
    {
      "cell_type": "markdown",
      "source": "#### Easy Task 3: Temperature Effects on Text Generation\n\n### Instructions\n\n1. Execute code to see how temperature affects token selection with a confident model\n2. Observe how probabilities and samples change across temperatures\n3. Uncomment the uncertain probability distribution and run again\n4. Compare temperature effects on confident vs uncertain models\n5. Uncomment TODO to add a new temperature value and analyze results",
      "metadata": {
        "id": "YkEaGnlqEEWU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEUCj_0a7WdM"
      },
      "source": "import numpy as np",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwZZIm5B7WdO"
      },
      "source": "original_probs = np.array([0.50, 0.30, 0.12, 0.05, 0.03])\ntokens = [\"positive\", \"negative\", \"neutral\", \"good\", \"bad\"]\n\n# Try uncertain distribution\n# original_probs = np.array([0.25, 0.24, 0.22, 0.18, 0.11]), much more balanced\n# Run again and compare the temperature effects",
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uj98unj67WdO"
      },
      "source": "def apply_temperature(probs, temperature):\n    \"\"\"Apply temperature scaling to change distribution sharpness\"\"\"\n    if temperature == 0:\n        # Write code to: For temperature=0, return deterministic distribution\n        # Hint: Create array of zeros, set the max probability index to 1.0\n        result = None  # YOUR CODE HERE\n        return result if result is not None else probs\n    \n    # Your task: Apply temperature scaling\n    # Hint: logits = log(probs), scale by temp, apply softmax\n    # Steps: log -> divide by temp -> exp -> normalize\n    logits = None  # YOUR CODE HERE\n    scaled_logits = None  # YOUR CODE HERE\n    exp_logits = None  # YOUR CODE HERE\n    normalized = None  # YOUR CODE HERE (sum should equal 1.0)\n    \n    return normalized if normalized is not None else probs\n\n# Solution (uncomment if stuck):\n# def apply_temperature(probs, temperature):\n#     if temperature == 0:\n#         result = np.zeros_like(probs)\n#         result[np.argmax(probs)] = 1.0\n#         return result\n#     logits = np.log(probs + 1e-10)\n#     scaled_logits = logits / temperature\n#     exp_logits = np.exp(scaled_logits)\n#     return exp_logits / np.sum(exp_logits)",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3EKkaa37WdP"
      },
      "source": "def visualize_distribution(probs, tokens):\n    \"\"\"Show probability distribution as bar chart\"\"\"\n    for i, token in enumerate(tokens):\n        bar_length = int(probs[i] * 100)\n        bar = '' * bar_length\n        print(f\"  {token:10s}: {probs[i]:.3f} {bar}\")",
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRgtzbGm7WdQ"
      },
      "source": "# Test different temperatures\ntemperatures = [0, 0.5, 1.0, 2.0]\n\n# Add temperature=3.0\n# temperatures = [0, 0.5, 1.0, 2.0, 3.0]",
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvnXZI2T7WdR",
        "outputId": "30042293-aea0-4971-dd3b-0472ff671241",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": "print(f\"Original (temperature=1.0) probabilities:\")\nvisualize_distribution(original_probs, tokens)",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHqoPBHi7WdS",
        "outputId": "5a336f00-720f-4a79-f3a7-e56483ce8e00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": "for temp in temperatures:\n    print(f\"\\n{'='*70}\")\n    print(f\"Temperature = {temp}\")\n    print('='*70)\n\n    # Apply temperature\n    new_probs = apply_temperature(original_probs, temp)\n\n    # Visualize\n    visualize_distribution(new_probs, tokens)\n\n    # Sample tokens\n    print(f\"\\n  Sampling 10 tokens:\")\n    if temp == 0:\n        samples = [tokens[np.argmax(new_probs)]] * 10\n    else:\n        samples = np.random.choice(tokens, size=10, p=new_probs)\n\n    print(f\"  {samples}\")\n\n    # Show diversity metric\n    unique_tokens = len(set(samples))\n    print(f\"   Diversity: {unique_tokens}/10 unique tokens\")\n\n    # Explain what's happening\n    if temp == 0:\n        print(f\"   Effect: Deterministic - always outputs '{samples[0]}'\")\n    elif temp < 1.0:\n        print(f\"   Effect: Sharpened - makes confident tokens more likely\")\n    elif temp == 1.0:\n        print(f\"   Effect: Unchanged - original distribution\")\n    else:\n        print(f\"   Effect: Flattened - makes all tokens more equally likely\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Questions\n\n1. Why is temperature=0 critical for classification tasks? What would go wrong with temperature=1.0?\n\n2. Compare temperature=0.5 vs 2.0. At what temperature did low-probability tokens like \"bad\" start appearing in samples?\n\n3. With the uncertain distribution ([0.25, 0.24, 0.22, 0.18, 0.11]), how did temperature effects differ from the confident model?",
      "metadata": {
        "id": "pA4FHgvIENtG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZDavrTL39zJ"
      },
      "source": "**About This Task:**\nEmbedding similarity measures how semantically close two texts are in vector space."
    },
    {
      "cell_type": "markdown",
      "source": "#### Easy Task 4: Embedding Similarity Analysis\n\n### Instructions\n\n1. Execute code to see similarity matrix for movie reviews\n2. Identify which reviews cluster together and which are distant\n3. Uncomment TODO to add reviews from different domains\n4. Analyze whether restaurant/product reviews cluster with movie reviews\n5. Add a random sentence to test similarity boundaries",
      "metadata": {
        "id": "Y2tgqir4Ecu0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcCuGi6N7WdU"
      },
      "source": "from sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np",
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQP5idM27WdW"
      },
      "source": "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')",
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpR2GweA7WdW"
      },
      "source": "# Movie reviews - different types\ntexts = [\n    # Positive reviews\n    \"Amazing movie! Absolutely loved it!\",\n    \"Fantastic film, highly recommend!\",\n    \"Great cinematography and acting\",\n\n    # Negative reviews\n    \"Terrible waste of time\",\n    \"Very disappointing and boring\",\n    \"Poor acting and weak plot\",\n\n    # Neutral reviews\n    \"It was okay, nothing special\",\n    \"Some good parts, some bad parts\",\n\n    # Off-topic\n    \"The weather is nice today\",\n    \"I like eating pizza\"\n]\n\n# Implement: Test domain transfer\n# texts = [\n#     # Positive movie reviews\n#     \"Amazing movie! Absolutely loved it!\",\n#     \"Fantastic film, highly recommend!\",\n#     \"Great cinematography and acting\",\n#\n#     # Negative movie reviews\n#     \"Terrible waste of time\",\n#     \"Very disappointing and boring\",\n#     \"Poor acting and weak plot\",\n#\n#     # Neutral movie reviews\n#     \"It was okay, nothing special\",\n#     \"Some good parts, some bad parts\",\n#\n#     # Positive restaurant review (different domain!)\n#     \"Amazing food! Absolutely loved it!\",\n#     \"Fantastic restaurant, highly recommend!\",\n#\n#     # Off-topic\n#     \"The weather is nice today\",\n#     \"I like eating pizza\",\n# ]",
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJTM2D7Y7WdX"
      },
      "source": "labels = [f\"Text {i+1}\" for i in range(len(texts))]",
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_KegI7i7Wda"
      },
      "source": "# Generate embeddings\nembeddings = model.encode(texts)\nsimilarity_matrix = cosine_similarity(embeddings)",
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcM9HaB_7Wdb",
        "outputId": "f07bcfea-10d3-409b-a38d-f103d179ce38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": "print(f\"Each text converted to {embeddings.shape[1]}-dimensional vector\")\nprint(f\"Comparing {embeddings.shape[0]} texts\\n\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0YJHXQe7Wdc",
        "outputId": "dac3204c-62dc-4280-f895-4469f2d8e32a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": "# Show full similarity matrix\nprint(\"Similarity Matrix (0=unrelated, 1=identical):\")\nprint(f\"{'':10s}\", end=\"\")\nfor i in range(len(texts)):\n    print(f\"T{i+1:2d} \", end=\"\")\nprint()\n\nfor i in range(len(texts)):\n    print(f\"Text {i+1:2d}:  \", end=\"\")\n    for j in range(len(texts)):\n        if i == j:\n            print(\"---- \", end=\"\")\n        else:\n            sim = similarity_matrix[i][j]\n            if sim > 0.6:\n                print(f\"{sim:.2f}*\", end=\"\")  # High similarity\n            else:\n                print(f\"{sim:.2f} \", end=\"\")\n            print(\" \", end=\"\")\n    print()\n\nprint(\"\\n* = High similarity (>0.6)\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRahl5n-7Wdd",
        "outputId": "622c55d1-02e7-4134-a06d-cda010ec46bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": "# Detailed comparisons\nprint(\"Detailed Comparisons\")\n\ncomparisons = [\n    (0, 1, \"Positive review vs Positive review\"),\n    (3, 4, \"Negative review vs Negative review\"),\n    (0, 3, \"Positive review vs Negative review\"),\n    (0, 8, \"Movie review vs Off-topic text\"),\n]\n\n# Complete this: Compare movie vs restaurant reviews\n# comparisons.append((0, 8, \"Positive MOVIE vs Positive RESTAURANT\"))\n\nfor i, j, description in comparisons:\n    if i < len(texts) and j < len(texts):\n        sim = similarity_matrix[i][j]\n        print(f\"\\n{description}:\")\n        print(f\"  Text {i+1}: '{texts[i]}'\")\n        print(f\"  Text {j+1}: '{texts[j]}'\")\n        print(f\"  Similarity: {sim:.3f}\")\n\n        if sim > 0.7:\n            print(f\"   Very similar! These texts are closely related in meaning\")\n        elif sim > 0.4:\n            print(f\"   Moderately similar. Some shared concepts\")\n        else:\n            print(f\"   Different topics or sentiments\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "innIFl7t7Wde",
        "outputId": "2d5ccc88-3477-49af-b4e8-b4a1cd864fc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": "# Fill in: Find texts that are similar to the first positive review\n# Hint: Loop through all texts, check if similarity > 0.5\n\nprint(\"\\nClusters (which texts group together?)\")\n\npositive_idx = 0\nsimilar_to_positive = []\n\n# Write code to: Loop through texts and find similar ones\n# for i in range(len(texts)):\n#     if i != positive_idx and similarity_matrix[positive_idx][i] > ???:\n#         similar_to_positive.append(???)\n\nprint(f\"\\nTexts similar to '{texts[positive_idx]}':\")\nfor idx, sim in sorted(similar_to_positive, key=lambda x: x[1], reverse=True):\n    print(f\"  Text {idx+1} (sim={sim:.3f}): '{texts[idx]}'\")\n\n# Solution (uncomment if stuck):\n# for i in range(len(texts)):\n#     if i != positive_idx and similarity_matrix[positive_idx][i] > 0.5:\n#         similar_to_positive.append((i, similarity_matrix[positive_idx][i]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Questions\n\n1. Compare similarity between Text 1 and Text 2 (both positive) vs Text 1 and Text 4 (positive vs negative). What aspects of semantic meaning do embeddings prioritize?\n\n2. Find similarity scores between two negative reviews (Text 4 and Text 5) and two positive reviews (Text 1 and Text 2). Why would averaging embeddings per class work for classification?\n\n3. After adding restaurant reviews: How similar was \"Amazing food\" to \"Amazing movie\"? What does this reveal about domain transfer?",
      "metadata": {
        "id": "TL1g6q0kEjPw"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}