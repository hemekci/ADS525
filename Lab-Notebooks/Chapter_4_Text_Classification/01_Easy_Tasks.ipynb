{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlH0PB_t39yg"
      },
      "source": [
        "# Chapter 4: Text Classification - Easy Tasks\n",
        "\n",
        "This notebook covers the basic text classification concepts: zero-shot classification, classifier strategies, temperature effects, and embedding similarity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xus_y4Ga39yk"
      },
      "source": [
        "\n",
        "## Setup\n",
        "\n",
        "Run all cells in this section to set up the environment and load necessary data.\n",
        "\n",
        "Before running these cells, it is advised to first run and try to get familiar with the codes and concepts from the main Chapter 4 Notebook (`Start_Here.ipynb`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LGW2SD-c864"
      },
      "source": [
        "### [OPTIONAL] - Installing Packages on <img src=\"https://colab.google/static/images/icons/colab.png\" width=100>\n",
        "\n",
        "\n",
        "If you are viewing this notebook on Google Colab (or any other cloud vendor), you need to **uncomment and run** the following codeblock to install the dependencies for this chapter:\n",
        "\n",
        "---\n",
        "\n",
        " **NOTE**: We will want to use a GPU to run the examples in this notebook. In Google Colab, go to\n",
        "**Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4**.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "N-PxmOIhc865"
      },
      "outputs": [],
      "source": [
        " %%capture\n",
        "!pip install transformers sentence-transformers openai\n",
        "!pip install -U datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WToW_Psg39ys"
      },
      "source": [
        "### Data Loading\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the same data as in Start_Here.ipynb notebook"
      ],
      "metadata": {
        "id": "TBJUHeDD4eAy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5phRS_z2U_3T",
        "outputId": "0721c545-1189-421c-e11b-d300191281f6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 8530\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 1066\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 1066\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load our data\n",
        "data = load_dataset(\"rotten_tomatoes\")\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXl6LASB39yw"
      },
      "source": [
        "### Helper Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "X0KyKHtqyjn3"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def evaluate_performance(y_true, y_pred):\n",
        "    \"\"\"Create and print the classification report\"\"\"\n",
        "    performance = classification_report(\n",
        "        y_true, y_pred,\n",
        "        target_names=[\"Negative Review\", \"Positive Review\"]\n",
        "    )\n",
        "    print(performance)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Your Turn - Text Classification Experiments\n",
        "\n",
        "Run each task first to see the results without the changes. Follow the instructions to modify and experiment.\n",
        "\n",
        "These tasks are mostly for observation and require minimal modification to the code."
      ],
      "metadata": {
        "id": "NKYNfoaVC4hU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Easy Tasks - Hands-On Experimentation\n",
        "\n"
      ],
      "metadata": {
        "id": "gwqUbziIDNee"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD982uIW39y-"
      },
      "source": [
        "**About This Task:**\n",
        "Zero-shot classification classifies text without training examples."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Easy Task 1: Zero-Shot Classifier\n",
        "\n",
        "### Instructions\n",
        "\n",
        "1. Execute the code to see baseline predictions for 3 basic reviews\n",
        "2. Uncomment the larger `test_reviews` list and run again to test harder cases\n",
        "3. Uncomment one label option to see how wording affects predictions\n",
        "4. Compare which label style works best for ambiguous reviews"
      ],
      "metadata": {
        "id": "TnHfnLsxDQcA"
      }
    },
    {
      "cell_type": "code",
      "source": "from sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np",
      "metadata": {
        "id": "ZC6N2A3LC6JZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the sentence transformer model for encoding text."
      ]
    },
    {
      "cell_type": "code",
      "source": "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the test reviews we want to classify."
      ]
    },
    {
      "cell_type": "code",
      "source": "# Test reviews - THESE WORK AS-IS\ntest_reviews = [\n    \"This movie was absolutely fantastic! A masterpiece!\",\n    \"Terrible waste of time. Very disappointing.\",\n    \"An okay film, nothing special but watchable.\",\n]\n\n# TODO: Uncomment to test harder cases\n# test_reviews = [\n#     \"This movie was absolutely fantastic! A masterpiece!\",\n#     \"Terrible waste of time. Very disappointing.\",\n#     \"An okay film, nothing special but watchable.\",\n#     \"Oh great, another masterpiece... NOT!\",  # Sarcastic\n#     \"Boring.\",  # Very short\n#     \"Great acting but terrible plot.\",  # Mixed sentiment\n# ]",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the label descriptions for classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Label descriptions - THESE WORK AS-IS\n",
        "labels = [\n",
        "    \"A negative movie review\",\n",
        "    \"A positive movie review\"\n",
        "]\n",
        "\n",
        "# TODO: Try different label options\n",
        "# labels = [\"negative\", \"positive\"]  # Option 1: Simple\n",
        "# labels = [\"bad movie review\", \"good movie review\"]  # Option 2: Different wording\n",
        "# labels = [\"a scathing negative movie review\", \"an enthusiastic positive movie review\"]  # Option 3: Detailed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Encode both labels and reviews into embeddings, then calculate similarity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create embeddings and calculate similarity\n",
        "label_embeddings = model.encode(labels)\n",
        "review_embeddings = model.encode(test_reviews)\n",
        "sim_matrix = cosine_similarity(review_embeddings, label_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code cell, we simply load and use the model on the test reviews. You are encouraged to run the above cell with the different labels option to see the difference similarity."
      ],
      "metadata": {
        "id": "B-NX5qe-53ph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classification Results:\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "for i, review in enumerate(test_reviews):\n",
        "    prediction = np.argmax(sim_matrix[i])\n",
        "    confidence = sim_matrix[i][prediction]\n",
        "    margin = abs(sim_matrix[i][0] - sim_matrix[i][1])\n",
        "\n",
        "    print(f\"\\nReview {i+1}: '{review}'\")\n",
        "    print(f\"Predicted: {labels[prediction]}\")\n",
        "    print(f\"Confidence: {confidence:.3f}\")\n",
        "    print(f\"Scores -> Negative: {sim_matrix[i][0]:.3f}, Positive: {sim_matrix[i][1]:.3f}\")\n",
        "    print(f\"Margin (certainty): {margin:.3f}\")"
      ],
      "metadata": {
        "id": "DeRNmhHE513y",
        "outputId": "14cf6192-25fe-4261-d3ab-074a8de0586a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Results:\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Review 1: 'This movie was absolutely fantastic! A masterpiece!'\n",
            "Predicted: A positive movie review\n",
            "Confidence: 0.493\n",
            "Scores -> Negative: 0.382, Positive: 0.493\n",
            "Margin (certainty): 0.111\n",
            "\n",
            "Review 2: 'Terrible waste of time. Very disappointing.'\n",
            "Predicted: A negative movie review\n",
            "Confidence: 0.439\n",
            "Scores -> Negative: 0.439, Positive: 0.299\n",
            "Margin (certainty): 0.140\n",
            "\n",
            "Review 3: 'An okay film, nothing special but watchable.'\n",
            "Predicted: A positive movie review\n",
            "Confidence: 0.542\n",
            "Scores -> Negative: 0.503, Positive: 0.542\n",
            "Margin (certainty): 0.039\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test other `test_reviews` or `labels`, comment the previous one. Make sure only one of the sets is uncommented.\n",
        "\n",
        "Tip: to uncomment/comment lines, use `Ctrl + /`"
      ],
      "metadata": {
        "id": "NOucKIpW6kU1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Questions\n",
        "\n",
        "1. Why did the classifier fail on the sarcastic review (\"*Oh great, another masterpiece... NOT*\")? What semantic features did embeddings miss?\n",
        "\n",
        "2. Which reviews changed predictions when you modified label descriptions? Why are some reviews more sensitive to label wording than others?\n",
        "\n",
        "3. Which reviews have low confidence margins (<0.1)? What linguistic features make certain reviews harder to classify?\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "hAfvoaoTDidJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1V2I1IWn39zB"
      },
      "source": [
        "**About This Task:**\n",
        "Different classification strategies affect model behavior and performance."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Easy Task 2: Classifier Strategy Analysis\n",
        "\n",
        "### Instructions\n",
        "\n",
        "1. Execute code to see three pre-built classifiers (conservative, aggressive, balanced)\n",
        "2. Study each confusion matrix to identify error patterns\n",
        "3. Modify `classifier_yours` to create a very conservative classifier (precision > 0.9)\n",
        "4. Uncomment the TODO section to analyze your classifier\n",
        "5. Experiment with creating different strategy combinations"
      ],
      "metadata": {
        "id": "Rhuy6BY6DxDr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set up the ground truth labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# True labels: first 5 are negative (0), last 5 are positive (1)\n",
        "y_true = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Three pre-built classifiers with different strategies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Pre-built classifiers to analyze\n",
        "classifier_conservative = np.array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1])  # Rarely predicts positive\n",
        "classifier_aggressive = np.array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1])     # Often predicts positive\n",
        "classifier_balanced = np.array([0, 0, 0, 1, 0, 0, 1, 1, 1, 1])       # Balanced approach"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create your own classifier to experiment with precision/recall tradeoffs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TO-DO: Modify these predictions to make YOUR classifier\n",
        "# Try to achieve precision > 0.9 (be very selective about predicting 1)\n",
        "classifier_yours = np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a function to analyze classifier performance with confusion matrix and metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def analyze_classifier(name, y_true, y_pred):\n",
        "    \"\"\"Analyze classifier performance with detailed breakdown\"\"\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"{name}\")\n",
        "    print('='*70)\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Show confusion matrix with labels\n",
        "    print(f\"\\nConfusion Matrix:\")\n",
        "    print(f\"                    Predicted Neg | Predicted Pos\")\n",
        "    print(f\"Actual Neg (0):          {cm[0][0]}       |       {cm[0][1]}  False Positives (BAD)\")\n",
        "    print(f\"Actual Pos (1):          {cm[1][0]}       |       {cm[1][1]}  True Positives (GOOD)\")\n",
        "    print(f\"                                                     \")\n",
        "    print(f\"                   False Negatives (BAD)              \")\n",
        "\n",
        "    # Calculate metrics\n",
        "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "\n",
        "    print(f\"\\nMetrics:\")\n",
        "    print(f\"Precision: {precision:.3f} = TP/(TP+FP) = {cm[1][1]}/({cm[1][1]}+{cm[0][1]})\")\n",
        "    print(f\"            Of {cm[1][1]+cm[0][1]} positive predictions, {cm[1][1]} were correct\")\n",
        "    print(f\"\\nRecall:    {recall:.3f} = TP/(TP+FN) = {cm[1][1]}/({cm[1][1]}+{cm[1][0]})\")\n",
        "    print(f\"            Of {cm[1][1]+cm[1][0]} actual positives, found {cm[1][1]}\")\n",
        "    print(f\"\\nF1 Score:  {f1:.3f} = 2*(P*R)/(P+R)\")\n",
        "\n",
        "    # Explain strategy\n",
        "    if precision > recall + 0.1:\n",
        "        print(f\"\\n Strategy: CONSERVATIVE (careful about predicting positive)\")\n",
        "        print(f\"   Few false alarms (only {cm[0][1]} false positives)\")\n",
        "        print(f\"   Misses actual positives ({cm[1][0]} false negatives)\")\n",
        "    elif recall > precision + 0.1:\n",
        "        print(f\"\\n Strategy: AGGRESSIVE (liberal about predicting positive)\")\n",
        "        print(f\"   Finds most positives (only {cm[1][0]} false negatives)\")\n",
        "        print(f\"   Many false alarms ({cm[0][1]} false positives)\")\n",
        "    else:\n",
        "        print(f\"\\n Strategy: BALANCED\")\n",
        "\n",
        "    return precision, recall, f1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the analysis on all three pre-built classifiers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Analyze pre-built classifiers\n",
        "results = {}\n",
        "for name, classifier in [\n",
        "    (\"Conservative Classifier\", classifier_conservative),\n",
        "    (\"Aggressive Classifier\", classifier_aggressive),\n",
        "    (\"Balanced Classifier\", classifier_balanced),\n",
        "]:\n",
        "    p, r, f = analyze_classifier(name, y_true, classifier)\n",
        "    results[name] = (p, r, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Uncomment this section to analyze your custom classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO: Analyze your classifier\n",
        "# print(\"\\n\" + \"=\"*70)\n",
        "# print(\"ANALYZING YOUR CLASSIFIER\")\n",
        "# print(\"=\"*70)\n",
        "# p, r, f = analyze_classifier(\"YOUR Classifier\", y_true, classifier_yours)\n",
        "# results[\"YOUR Classifier\"] = (p, r, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Print summary table comparing all classifiers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Summary\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"{'Classifier':<25} {'Precision':<12} {'Recall':<12} {'F1':<12}\")\n",
        "print(\"-\"*70)\n",
        "for name, (p, r, f) in results.items():\n",
        "    print(f\"{name:<25} {p:.3f}        {r:.3f}       {f:.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Questions\n",
        "\n",
        "1. The conservative classifier has 2 false negatives. What real-world mistake does this represent? Provide a movie review example.\n",
        "\n",
        "2. What strategy did you use to achieve high precision in `classifier_yours`? Why does predicting positive less frequently increase precision?\n",
        "\n",
        "3. Which classifier won on F1 score? Why doesn't the aggressive classifier win despite high recall?"
      ],
      "metadata": {
        "id": "GHv-B_ebD8CU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUmXASWa39zF"
      },
      "source": [
        "**About This Task:**\n",
        "Temperature controls randomness in language model outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Easy Task 3: Temperature Effects on Text Generation\n",
        "\n",
        "### Instructions\n",
        "\n",
        "1. Execute code to see how temperature affects token selection with a confident model\n",
        "2. Observe how probabilities and samples change across temperatures\n",
        "3. Uncomment the uncertain probability distribution and run again\n",
        "4. Compare temperature effects on confident vs uncertain models\n",
        "5. Uncomment TODO to add a new temperature value and analyze results"
      ],
      "metadata": {
        "id": "YkEaGnlqEEWU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set up probability distribution and token names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "original_probs = np.array([0.50, 0.30, 0.12, 0.05, 0.03])\n",
        "tokens = [\"positive\", \"negative\", \"neutral\", \"good\", \"bad\"]\n",
        "\n",
        "# Try uncertain distribution\n",
        "# original_probs = np.array([0.25, 0.24, 0.22, 0.18, 0.11]), much more balanced\n",
        "# Run again and compare the temperature effects"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Function to apply temperature scaling to probabilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def apply_temperature(probs, temperature):\n",
        "    \"\"\"Apply temperature scaling to change distribution sharpness\"\"\"\n",
        "    if temperature == 0:\n",
        "        # Deterministic: always pick the highest\n",
        "        result = np.zeros_like(probs)\n",
        "        result[np.argmax(probs)] = 1.0\n",
        "        return result\n",
        "\n",
        "    # Apply temperature scaling\n",
        "    logits = np.log(probs + 1e-10)\n",
        "    scaled_logits = logits / temperature\n",
        "    exp_logits = np.exp(scaled_logits)\n",
        "    return exp_logits / np.sum(exp_logits)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Function to visualize probability distributions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def visualize_distribution(probs, tokens):\n",
        "    \"\"\"Show probability distribution as bar chart\"\"\"\n",
        "    for i, token in enumerate(tokens):\n",
        "        bar_length = int(probs[i] * 100)\n",
        "        bar = '' * bar_length\n",
        "        print(f\"  {token:10s}: {probs[i]:.3f} {bar}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define which temperatures to test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test different temperatures\n",
        "temperatures = [0, 0.5, 1.0, 2.0]\n",
        "\n",
        "# Add temperature=3.0\n",
        "# temperatures = [0, 0.5, 1.0, 2.0, 3.0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display the original distribution before applying any temperature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"=\"*70)\n",
        "print(f\"Original (temperature=1.0) probabilities:\")\n",
        "print(\"=\"*70)\n",
        "visualize_distribution(original_probs, tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loop through each temperature and see how it affects token selection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for temp in temperatures:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Temperature = {temp}\")\n",
        "    print('='*70)\n",
        "\n",
        "    # Apply temperature\n",
        "    new_probs = apply_temperature(original_probs, temp)\n",
        "\n",
        "    # Visualize\n",
        "    visualize_distribution(new_probs, tokens)\n",
        "\n",
        "    # Sample tokens\n",
        "    print(f\"\\n  Sampling 10 tokens:\")\n",
        "    if temp == 0:\n",
        "        samples = [tokens[np.argmax(new_probs)]] * 10\n",
        "    else:\n",
        "        samples = np.random.choice(tokens, size=10, p=new_probs)\n",
        "\n",
        "    print(f\"  {samples}\")\n",
        "\n",
        "    # Show diversity metric\n",
        "    unique_tokens = len(set(samples))\n",
        "    print(f\"   Diversity: {unique_tokens}/10 unique tokens\")\n",
        "\n",
        "    # Explain what's happening\n",
        "    if temp == 0:\n",
        "        print(f\"   Effect: DETERMINISTIC - always outputs '{samples[0]}'\")\n",
        "    elif temp < 1.0:\n",
        "        print(f\"   Effect: SHARPENED - makes confident tokens more likely\")\n",
        "    elif temp == 1.0:\n",
        "        print(f\"   Effect: UNCHANGED - original distribution\")\n",
        "    else:\n",
        "        print(f\"   Effect: FLATTENED - makes all tokens more equally likely\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Questions\n",
        "\n",
        "1. Why is temperature=0 critical for classification tasks? What would go wrong with temperature=1.0?\n",
        "\n",
        "2. Compare temperature=0.5 vs 2.0. At what temperature did low-probability tokens like \"bad\" start appearing in samples?\n",
        "\n",
        "3. With the uncertain distribution ([0.25, 0.24, 0.22, 0.18, 0.11]), how did temperature effects differ from the confident model?"
      ],
      "metadata": {
        "id": "pA4FHgvIENtG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZDavrTL39zJ"
      },
      "source": [
        "**About This Task:**\n",
        "Embedding similarity measures how semantically close two texts are in vector space."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Easy Task 4: Embedding Similarity Analysis\n",
        "\n",
        "### Instructions\n",
        "\n",
        "1. Execute code to see similarity matrix for movie reviews\n",
        "2. Identify which reviews cluster together and which are distant\n",
        "3. Uncomment TODO to add reviews from different domains\n",
        "4. Analyze whether restaurant/product reviews cluster with movie reviews\n",
        "5. Add a random sentence to test similarity boundaries"
      ],
      "metadata": {
        "id": "Y2tgqir4Ecu0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the embedding model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the sentence transformer model for encoding text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a variety of texts to compare: positive, negative, neutral, and off-topic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the test reviews we want to classify."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Movie reviews - different types\n",
        "texts = [\n",
        "    # Positive reviews\n",
        "    \"Amazing movie! Absolutely loved it!\",\n",
        "    \"Fantastic film, highly recommend!\",\n",
        "    \"Great cinematography and acting\",\n",
        "\n",
        "    # Negative reviews\n",
        "    \"Terrible waste of time\",\n",
        "    \"Very disappointing and boring\",\n",
        "    \"Poor acting and weak plot\",\n",
        "\n",
        "    # Neutral reviews\n",
        "    \"It was okay, nothing special\",\n",
        "    \"Some good parts, some bad parts\",\n",
        "\n",
        "    # Off-topic\n",
        "    \"The weather is nice today\",\n",
        "    \"I like eating pizza\"\n",
        "]\n",
        "\n",
        "# TODO: Test domain transfer\n",
        "# texts = [\n",
        "#     # Positive movie reviews\n",
        "#     \"Amazing movie! Absolutely loved it!\",\n",
        "#     \"Fantastic film, highly recommend!\",\n",
        "#     \"Great cinematography and acting\",\n",
        "#\n",
        "#     # Negative movie reviews\n",
        "#     \"Terrible waste of time\",\n",
        "#     \"Very disappointing and boring\",\n",
        "#     \"Poor acting and weak plot\",\n",
        "#\n",
        "#     # Neutral movie reviews\n",
        "#     \"It was okay, nothing special\",\n",
        "#     \"Some good parts, some bad parts\",\n",
        "#\n",
        "#     # Positive restaurant review (different domain!)\n",
        "#     \"Amazing food! Absolutely loved it!\",\n",
        "#     \"Fantastic restaurant, highly recommend!\",\n",
        "#\n",
        "#     # Off-topic\n",
        "#     \"The weather is nice today\",\n",
        "#     \"I like eating pizza\",\n",
        "# ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create labels for each text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "labels = [f\"Text {i+1}\" for i in range(len(texts))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate embeddings and compute pairwise similarity matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Generate embeddings\n",
        "embeddings = model.encode(texts)\n",
        "similarity_matrix = cosine_similarity(embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display embedding dimensions and number of texts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"Each text converted to {embeddings.shape[1]}-dimensional vector\")\n",
        "print(f\"Comparing {embeddings.shape[0]} texts\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Print the full similarity matrix showing all pairwise comparisons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Show full similarity matrix\n",
        "print(\"Similarity Matrix (0=unrelated, 1=identical):\")\n",
        "print(\"=\"*80)\n",
        "print(f\"{'':10s}\", end=\"\")\n",
        "for i in range(len(texts)):\n",
        "    print(f\"T{i+1:2d} \", end=\"\")\n",
        "print()\n",
        "\n",
        "for i in range(len(texts)):\n",
        "    print(f\"Text {i+1:2d}:  \", end=\"\")\n",
        "    for j in range(len(texts)):\n",
        "        if i == j:\n",
        "            print(\"---- \", end=\"\")\n",
        "        else:\n",
        "            sim = similarity_matrix[i][j]\n",
        "            if sim > 0.6:\n",
        "                print(f\"{sim:.2f}*\", end=\"\")  # High similarity\n",
        "            else:\n",
        "                print(f\"{sim:.2f} \", end=\"\")\n",
        "            print(\" \", end=\"\")\n",
        "    print()\n",
        "\n",
        "print(\"\\n* = High similarity (>0.6)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Show detailed comparisons for specific text pairs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Detailed comparisons\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DETAILED COMPARISONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "comparisons = [\n",
        "    (0, 1, \"Positive review vs Positive review\"),\n",
        "    (3, 4, \"Negative review vs Negative review\"),\n",
        "    (0, 3, \"Positive review vs Negative review\"),\n",
        "    (0, 8, \"Movie review vs Off-topic text\"),\n",
        "]\n",
        "\n",
        "# TODO: Compare movie vs restaurant reviews\n",
        "# comparisons.append((0, 8, \"Positive MOVIE vs Positive RESTAURANT\"))\n",
        "\n",
        "for i, j, description in comparisons:\n",
        "    if i < len(texts) and j < len(texts):\n",
        "        sim = similarity_matrix[i][j]\n",
        "        print(f\"\\n{description}:\")\n",
        "        print(f\"  Text {i+1}: '{texts[i]}'\")\n",
        "        print(f\"  Text {j+1}: '{texts[j]}'\")\n",
        "        print(f\"  Similarity: {sim:.3f}\")\n",
        "\n",
        "        if sim > 0.7:\n",
        "            print(f\"   Very similar! These texts are closely related in meaning\")\n",
        "        elif sim > 0.4:\n",
        "            print(f\"   Moderately similar. Some shared concepts\")\n",
        "        else:\n",
        "            print(f\"   Different topics or sentiments\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Find which texts cluster together based on similarity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Find clusters\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CLUSTERS (which texts group together?)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Find texts similar to first positive review\n",
        "positive_idx = 0\n",
        "similar_to_positive = []\n",
        "for i in range(len(texts)):\n",
        "    if i != positive_idx and similarity_matrix[positive_idx][i] > 0.5:\n",
        "        similar_to_positive.append((i, similarity_matrix[positive_idx][i]))\n",
        "\n",
        "print(f\"\\nTexts similar to '{texts[positive_idx]}':\")\n",
        "for idx, sim in sorted(similar_to_positive, key=lambda x: x[1], reverse=True):\n",
        "    print(f\"  Text {idx+1} (sim={sim:.3f}): '{texts[idx]}'\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Questions\n",
        "\n",
        "1. Compare similarity between Text 1 and Text 2 (both positive) vs Text 1 and Text 4 (positive vs negative). What aspects of semantic meaning do embeddings prioritize?\n",
        "\n",
        "2. Find similarity scores between two negative reviews (Text 4 and Text 5) and two positive reviews (Text 1 and Text 2). Why would averaging embeddings per class work for classification?\n",
        "\n",
        "3. After adding restaurant reviews: How similar was \"Amazing food\" to \"Amazing movie\"? What does this reveal about domain transfer?"
      ],
      "metadata": {
        "id": "TL1g6q0kEjPw"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}