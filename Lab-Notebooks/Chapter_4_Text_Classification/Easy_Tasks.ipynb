{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Text Classification - Easy Tasks\n",
    "\n",
    "This notebook covers fundamental text classification concepts: zero-shot classification, classifier strategies, temperature effects, and embedding similarity.\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "Run all cells in this section to set up the environment and load necessary data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LGW2SD-c864"
   },
   "source": [
    "### [OPTIONAL] - Installing Packages on <img src=\"https://colab.google/static/images/icons/colab.png\" width=100>\n",
    "\n",
    "\n",
    "If you are viewing this notebook on Google Colab (or any other cloud vendor), you need to **uncomment and run** the following codeblock to install the dependencies for this chapter:\n",
    "\n",
    "---\n",
    "\n",
    " **NOTE**: We will want to use a GPU to run the examples in this notebook. In Google Colab, go to\n",
    "**Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4**.\n",
    "\n",
    "---\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "N-PxmOIhc865"
   },
   "outputs": [],
   "source": [
    " %%capture\n",
    "!pip install transformers sentence-transformers openai\n",
    "!pip install -U datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 784,
     "referenced_widgets": [
      "169816892c8646e3888f213295349f00",
      "4a590f6ceb104873b97b9620d6107017",
      "2d14e5528bbd446e8a34c235201f88ac",
      "f4c9e362c7ff40559a1a5632a8b6e907",
      "3c4c633f0af84d099ffc74bac8901b07",
      "35226b53943b4db8a5d46aae09720818",
      "1a6faf9a5ed748f0809834cc52435b3d",
      "4fe179f7fab44513aeaa33cedf44f1bb",
      "5befa9362a09459695f4489c7173c34d",
      "458ad67bb385497889eab5bbdd977de5",
      "027a7c3f8a9e4114ac6d4e64fb62d440",
      "0cce924a44f94cbfaa69a78735ece1d5",
      "28ac2aeffcb6424f97fb47bfaee8e66f",
      "ab02cefe39774db3844d914b65a48790",
      "6bf6e7d710eb4924a3144e6e5d0679ca",
      "b7b774839b134ee687d9d89a0e15d166",
      "c583fda17dd44217875e86209fbad80a",
      "cd4eeedee4fc4f28acf650785bbaa5f3",
      "e5138fd5028f48f9ad8eea2b41b1592a",
      "82f4409e3f854415a871e3da31e9393e",
      "aeb337caa109443a9fc4f93e9f92753d",
      "e7e6399dec234052b7f30e3135a21b00",
      "44a764c1267042259b2c9597a69f6f76",
      "05e8f57e23ff47f0be24d3f9c18554d4",
      "616321e6e87a4d8589d9ed939e19c7b2",
      "8d7284d945004b98b28b6c631a2c4726",
      "97d864e9c07149aeb22a844ced3909d5",
      "dbadb6dfe23141ec8f38ffb747ea882e",
      "82d0c0a18606421f92ed77d8d4c61e10",
      "0c64a3bc4d4d461cbdc3c3c0acae2f94",
      "cf707e36593b4071b8253fd33d18b665",
      "079412e5345a4a09875ed0c86dde93e3",
      "1e2ffd31336946cf952abb5f62651c16",
      "8466edc3103c45d8a5d3fe8166508648",
      "f8d0d8f24551457e862d2e041f3699ab",
      "426852cccf704c58bdc7decbb1c583e6",
      "6b2663befc574a8e8154631c3ac95ff3",
      "091bd22cb6d94ae5b7c962d18e528ca2",
      "5ba5883cc2144929b9d7ddd980e4508d",
      "971d17ca015446b4bbf8a908835295cd",
      "c7086b545eae4b09af3b2d77b78af4b9",
      "5f47aeddde3449a5b5ed280d0ec9bc18",
      "43791444ad3e49f993d17887e6c15dbf",
      "c2c3403eb38b420cba7390d385f325be",
      "9097dd47971f445ea0fd55e7c42c15a0",
      "18cbf6cbaa4c436d9074224f83472863",
      "831180b810da4b549a8ca75536607e31",
      "d9779ccf530e49b1ac52172252ec5ef4",
      "78d69219fad44722a9bad3a12a3ccb61",
      "f08fa19e07644f4d9fff30d02040e315",
      "e1a2a1e765f042a7b9b081688688dab5",
      "76d299a97ab54dbe96f4525095afedcb",
      "73ac94bdcaf8427bbbcdd808f1428dff",
      "6f9a5d0dfe2f4aa49c57e942b927521b",
      "2d7810d299f649a2ac84541e7215d29a",
      "40dc8a185cf34e99a427f7c81bc76540",
      "bcac3e31740e4a2894b4e8bc9656a0c5",
      "87f29bce5b5d4df08b5337d62ffa9568",
      "3129b232f1e3411ead0d913654a11eaa",
      "4e7c3e87552640018f89c6cf9e070642",
      "01ccc633ad5d4350ad04651e360cc478",
      "735737e7c1eb477fae9d06d7324e81e8",
      "882b69fb84034d2dace625ef267f51cb",
      "8c77be8e5b74452bb028b97fd9edba36",
      "55a6296e28394f41b2a6d1f3d76e5944",
      "8f6c7a732bb6498db1de4d66f4b3c623",
      "22541d211626493e87c168143671e5ce",
      "111b9c6182994e2e85a323d07980b1ab",
      "3c53665ac233434fa899399da13650f0",
      "c4867626e6eb41bfaade350790de9f40",
      "a11866f208c4411dbc3d627ef6dbd74f",
      "3485c0ebc7f24965a0fadd3a6570d51e",
      "d111b1f753554eeda57cdbe420335fda",
      "8fb77e0530e946d38c007c9012c37a79",
      "8d669f0719d04980a6f04ffc2f65cf7c",
      "d79a649275a14fdeb2cb01e5ab75021f",
      "14186c7dd75e43d99183d5c8b308c0de"
     ]
    },
    "id": "5phRS_z2U_3T",
    "outputId": "e51c9c23-a48a-4c61-e7ee-cd0ef03915a4"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "169816892c8646e3888f213295349f00"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "train.parquet:   0%|          | 0.00/699k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0cce924a44f94cbfaa69a78735ece1d5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "validation.parquet:   0%|          | 0.00/90.0k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "44a764c1267042259b2c9597a69f6f76"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "test.parquet:   0%|          | 0.00/92.2k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8466edc3103c45d8a5d3fe8166508648"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9097dd47971f445ea0fd55e7c42c15a0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating validation split:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "40dc8a185cf34e99a427f7c81bc76540"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating test split:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "22541d211626493e87c168143671e5ce"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 8530\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load our data\n",
    "data = load_dataset(\"rotten_tomatoes\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "X0KyKHtqyjn3"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_performance(y_true, y_pred):\n",
    "    \"\"\"Create and print the classification report\"\"\"\n",
    "    performance = classification_report(\n",
    "        y_true, y_pred,\n",
    "        target_names=[\"Negative Review\", \"Positive Review\"]\n",
    "    )\n",
    "    print(performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Your Turn - Text Classification Experiments\n",
    "\n",
    "Run each task first to see the baseline results. Follow the instructions to modify and experiment."
   ],
   "metadata": {
    "id": "NKYNfoaVC4hU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This section is divided into EASY, MEDIUM, & HARD."
   ],
   "metadata": {
    "id": "hHVONn85DElL"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Easy Tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Easy Tasks - Hands-On Exploration"
   ],
   "metadata": {
    "id": "gwqUbziIDNee"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About This Task:**\n",
    "Zero-shot classification classifies text without training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Easy Task 1: Zero-Shot Classifier\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Execute the code to see baseline predictions for 3 basic reviews\n",
    "2. Uncomment the larger `test_reviews` list and run again to test harder cases\n",
    "3. Uncomment one label option to see how wording affects predictions\n",
    "4. Compare which label style works best for ambiguous reviews"
   ],
   "metadata": {
    "id": "TnHfnLsxDQcA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# Test reviews - THESE WORK AS-IS\n",
    "test_reviews = [\n",
    "    \"This movie was absolutely fantastic! A masterpiece!\",\n",
    "    \"Terrible waste of time. Very disappointing.\",\n",
    "    \"An okay film, nothing special but watchable.\",\n",
    "]\n",
    "\n",
    "# TODO: Uncomment to test harder cases\n",
    "# test_reviews = [\n",
    "#     \"This movie was absolutely fantastic! A masterpiece!\",\n",
    "#     \"Terrible waste of time. Very disappointing.\",\n",
    "#     \"An okay film, nothing special but watchable.\",\n",
    "#     \"Oh great, another masterpiece... NOT!\",  # Sarcastic\n",
    "#     \"Boring.\",  # Very short\n",
    "#     \"Great acting but terrible plot.\",  # Mixed sentiment\n",
    "# ]\n",
    "\n",
    "# Label descriptions - THESE WORK AS-IS\n",
    "labels = [\n",
    "    \"A negative movie review\",\n",
    "    \"A positive movie review\"\n",
    "]\n",
    "\n",
    "# TODO: Try different label options\n",
    "# labels = [\"negative\", \"positive\"]  # Option 1: Simple\n",
    "# labels = [\"bad movie review\", \"good movie review\"]  # Option 2: Different wording\n",
    "# labels = [\"a scathing negative movie review\", \"an enthusiastic positive movie review\"]  # Option 3: Detailed\n",
    "\n",
    "# Create embeddings and calculate similarity\n",
    "label_embeddings = model.encode(labels)\n",
    "review_embeddings = model.encode(test_reviews)\n",
    "sim_matrix = cosine_similarity(review_embeddings, label_embeddings)\n",
    "\n",
    "print(\"Classification Results:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, review in enumerate(test_reviews):\n",
    "    prediction = np.argmax(sim_matrix[i])\n",
    "    confidence = sim_matrix[i][prediction]\n",
    "    margin = abs(sim_matrix[i][0] - sim_matrix[i][1])\n",
    "\n",
    "    print(f\"\\nReview {i+1}: '{review}'\")\n",
    "    print(f\"Predicted: {labels[prediction]}\")\n",
    "    print(f\"Confidence: {confidence:.3f}\")\n",
    "    print(f\"Scores -> Negative: {sim_matrix[i][0]:.3f}, Positive: {sim_matrix[i][1]:.3f}\")\n",
    "    print(f\"Margin (certainty): {margin:.3f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZC6N2A3LC6JZ",
    "outputId": "f1423ca6-a625-4355-d290-3cd8b5ab4371"
   },
   "execution_count": 26,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classification Results:\n",
      "================================================================================\n",
      "\n",
      "Review 1: 'This movie was absolutely fantastic! A masterpiece!'\n",
      "Predicted: A positive movie review\n",
      "Confidence: 0.493\n",
      "Scores -> Negative: 0.382, Positive: 0.493\n",
      "Margin (certainty): 0.111\n",
      "\n",
      "Review 2: 'Terrible waste of time. Very disappointing.'\n",
      "Predicted: A negative movie review\n",
      "Confidence: 0.439\n",
      "Scores -> Negative: 0.439, Positive: 0.299\n",
      "Margin (certainty): 0.140\n",
      "\n",
      "Review 3: 'An okay film, nothing special but watchable.'\n",
      "Predicted: A positive movie review\n",
      "Confidence: 0.542\n",
      "Scores -> Negative: 0.503, Positive: 0.542\n",
      "Margin (certainty): 0.039\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Questions\n",
    "\n",
    "1. Why did the classifier fail on the sarcastic review (\"Oh great, another masterpiece... NOT\")? What semantic features did embeddings miss?\n",
    "\n",
    "2. Which reviews changed predictions when you modified label descriptions? Why are some reviews more sensitive to label wording than others?\n",
    "\n",
    "3. Which reviews have low confidence margins (<0.1)? What linguistic features make certain reviews harder to classify?"
   ],
   "metadata": {
    "id": "hAfvoaoTDidJ"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About This Task:**\n",
    "Different classification strategies affect model behavior and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Easy Task 2: Classifier Strategy Analysis\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Execute code to see three pre-built classifiers (conservative, aggressive, balanced)\n",
    "2. Study each confusion matrix to identify error patterns\n",
    "3. Modify `classifier_yours` to create a very conservative classifier (precision > 0.9)\n",
    "4. Uncomment the TODO section to analyze your classifier\n",
    "5. Experiment with creating different strategy combinations"
   ],
   "metadata": {
    "id": "Rhuy6BY6DxDr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# True labels: first 5 are negative (0), last 5 are positive (1)\n",
    "y_true = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])\n",
    "\n",
    "# Pre-built classifiers to analyze\n",
    "classifier_conservative = np.array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1])  # Rarely predicts positive\n",
    "classifier_aggressive = np.array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1])     # Often predicts positive\n",
    "classifier_balanced = np.array([0, 0, 0, 1, 0, 0, 1, 1, 1, 1])       # Balanced approach\n",
    "\n",
    "# TODO: Modify these predictions to make YOUR classifier\n",
    "# Goal: Try to achieve precision > 0.9 (be very selective about predicting 1)\n",
    "classifier_yours = np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1])\n",
    "\n",
    "def analyze_classifier(name, y_true, y_pred):\n",
    "    \"\"\"Analyze classifier performance with detailed breakdown\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{name}\")\n",
    "    print('='*70)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Show confusion matrix with labels\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"                    Predicted Neg | Predicted Pos\")\n",
    "    print(f\"Actual Neg (0):          {cm[0][0]}       |       {cm[0][1]} <- False Positives (BAD)\")\n",
    "    print(f\"Actual Pos (1):          {cm[1][0]}       |       {cm[1][1]} <- True Positives (GOOD)\")\n",
    "    print(f\"                                                     \")\n",
    "    print(f\"                   False Negatives (BAD)              \")\n",
    "\n",
    "    # Calculate metrics\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "    print(f\"\\nMetrics:\")\n",
    "    print(f\"Precision: {precision:.3f} = TP/(TP+FP) = {cm[1][1]}/({cm[1][1]}+{cm[0][1]})\")\n",
    "    print(f\"            Of {cm[1][1]+cm[0][1]} positive predictions, {cm[1][1]} were correct\")\n",
    "    print(f\"\\nRecall:    {recall:.3f} = TP/(TP+FN) = {cm[1][1]}/({cm[1][1]}+{cm[1][0]})\")\n",
    "    print(f\"            Of {cm[1][1]+cm[1][0]} actual positives, found {cm[1][1]}\")\n",
    "    print(f\"\\nF1 Score:  {f1:.3f} = 2*(P*R)/(P+R)\")\n",
    "\n",
    "    # Explain strategy\n",
    "    if precision > recall + 0.1:\n",
    "        print(f\"\\n Strategy: CONSERVATIVE (careful about predicting positive)\")\n",
    "        print(f\"   Few false alarms (only {cm[0][1]} false positives)\")\n",
    "        print(f\"   Misses actual positives ({cm[1][0]} false negatives)\")\n",
    "    elif recall > precision + 0.1:\n",
    "        print(f\"\\n Strategy: AGGRESSIVE (liberal about predicting positive)\")\n",
    "        print(f\"   Finds most positives (only {cm[1][0]} false negatives)\")\n",
    "        print(f\"   Many false alarms ({cm[0][1]} false positives)\")\n",
    "    else:\n",
    "        print(f\"\\n Strategy: BALANCED\")\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "# Analyze pre-built classifiers\n",
    "results = {}\n",
    "for name, classifier in [\n",
    "    (\"Conservative Classifier\", classifier_conservative),\n",
    "    (\"Aggressive Classifier\", classifier_aggressive),\n",
    "    (\"Balanced Classifier\", classifier_balanced),\n",
    "]:\n",
    "    p, r, f = analyze_classifier(name, y_true, classifier)\n",
    "    results[name] = (p, r, f)\n",
    "\n",
    "# TODO: Analyze your classifier\n",
    "# print(\"\\n\" + \"=\"*70)\n",
    "# print(\"ANALYZING YOUR CLASSIFIER\")\n",
    "# print(\"=\"*70)\n",
    "# p, r, f = analyze_classifier(\"YOUR Classifier\", y_true, classifier_yours)\n",
    "# results[\"YOUR Classifier\"] = (p, r, f)\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Classifier':<25} {'Precision':<12} {'Recall':<12} {'F1':<12}\")\n",
    "print(\"-\"*70)\n",
    "for name, (p, r, f) in results.items():\n",
    "    print(f\"{name:<25} {p:.3f}        {r:.3f}       {f:.3f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Sw3mcCXDddB",
    "outputId": "9c042055-b756-419d-ec81-ce1e08ce5958"
   },
   "execution_count": 27,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "======================================================================\n",
      "Conservative Classifier\n",
      "======================================================================\n",
      "\n",
      "Confusion Matrix:\n",
      "                    Predicted Neg | Predicted Pos\n",
      "Actual Neg (0):          5       |       0 <- False Positives (BAD)\n",
      "Actual Pos (1):          2       |       3 <- True Positives (GOOD)\n",
      "                          \u2191                           \n",
      "                   False Negatives (BAD)              \n",
      "\n",
      "Metrics:\n",
      "Precision: 1.000 = TP/(TP+FP) = 3/(3+0)\n",
      "           \u2192 Of 3 positive predictions, 3 were correct\n",
      "\n",
      "Recall:    0.600 = TP/(TP+FN) = 3/(3+2)\n",
      "           \u2192 Of 5 actual positives, found 3\n",
      "\n",
      "F1 Score:  0.750 = 2*(P*R)/(P+R)\n",
      "\n",
      "\u2192 Strategy: CONSERVATIVE (careful about predicting positive)\n",
      "  \u2713 Few false alarms (only 0 false positives)\n",
      "  \u2717 Misses actual positives (2 false negatives)\n",
      "\n",
      "======================================================================\n",
      "Aggressive Classifier\n",
      "======================================================================\n",
      "\n",
      "Confusion Matrix:\n",
      "                    Predicted Neg | Predicted Pos\n",
      "Actual Neg (0):          1       |       4 <- False Positives (BAD)\n",
      "Actual Pos (1):          0       |       5 <- True Positives (GOOD)\n",
      "                          \u2191                           \n",
      "                   False Negatives (BAD)              \n",
      "\n",
      "Metrics:\n",
      "Precision: 0.556 = TP/(TP+FP) = 5/(5+4)\n",
      "           \u2192 Of 9 positive predictions, 5 were correct\n",
      "\n",
      "Recall:    1.000 = TP/(TP+FN) = 5/(5+0)\n",
      "           \u2192 Of 5 actual positives, found 5\n",
      "\n",
      "F1 Score:  0.714 = 2*(P*R)/(P+R)\n",
      "\n",
      "\u2192 Strategy: AGGRESSIVE (liberal about predicting positive)\n",
      "  \u2713 Finds most positives (only 0 false negatives)\n",
      "  \u2717 Many false alarms (4 false positives)\n",
      "\n",
      "======================================================================\n",
      "Balanced Classifier\n",
      "======================================================================\n",
      "\n",
      "Confusion Matrix:\n",
      "                    Predicted Neg | Predicted Pos\n",
      "Actual Neg (0):          4       |       1 <- False Positives (BAD)\n",
      "Actual Pos (1):          1       |       4 <- True Positives (GOOD)\n",
      "                          \u2191                           \n",
      "                   False Negatives (BAD)              \n",
      "\n",
      "Metrics:\n",
      "Precision: 0.800 = TP/(TP+FP) = 4/(4+1)\n",
      "           \u2192 Of 5 positive predictions, 4 were correct\n",
      "\n",
      "Recall:    0.800 = TP/(TP+FN) = 4/(4+1)\n",
      "           \u2192 Of 5 actual positives, found 4\n",
      "\n",
      "F1 Score:  0.800 = 2*(P*R)/(P+R)\n",
      "\n",
      "\u2192 Strategy: BALANCED\n",
      "\n",
      "======================================================================\n",
      "SUMMARY COMPARISON\n",
      "======================================================================\n",
      "Classifier                Precision    Recall       F1          \n",
      "----------------------------------------------------------------------\n",
      "Conservative Classifier   1.000        0.600       0.750\n",
      "Aggressive Classifier     0.556        1.000       0.714\n",
      "Balanced Classifier       0.800        0.800       0.800\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Questions\n",
    "\n",
    "1. The conservative classifier has 2 false negatives. What real-world mistake does this represent? Provide a movie review example.\n",
    "\n",
    "2. What strategy did you use to achieve high precision in `classifier_yours`? Why does predicting positive less frequently increase precision?\n",
    "\n",
    "3. Which classifier won on F1 score? Why doesn't the aggressive classifier win despite high recall?"
   ],
   "metadata": {
    "id": "GHv-B_ebD8CU"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About This Task:**\n",
    "Temperature controls randomness in language model outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Easy Task 3: Temperature Effects on Text Generation\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Execute code to see how temperature affects token selection with a confident model\n",
    "2. Observe how probabilities and samples change across temperatures\n",
    "3. Uncomment the uncertain probability distribution and run again\n",
    "4. Compare temperature effects on confident vs uncertain models\n",
    "5. Uncomment TODO to add a new temperature value and analyze results"
   ],
   "metadata": {
    "id": "YkEaGnlqEEWU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Starting with a CONFIDENT model (one token much more likely)\n",
    "original_probs = np.array([0.50, 0.30, 0.12, 0.05, 0.03])\n",
    "tokens = [\"positive\", \"negative\", \"neutral\", \"good\", \"bad\"]\n",
    "\n",
    "# TODO: Try uncertain distribution\n",
    "# original_probs = np.array([0.25, 0.24, 0.22, 0.18, 0.11])  # Much more balanced!\n",
    "# Run again and compare the temperature effects\n",
    "\n",
    "def apply_temperature(probs, temperature):\n",
    "    \"\"\"Apply temperature scaling to change distribution sharpness\"\"\"\n",
    "    if temperature == 0:\n",
    "        # Deterministic: always pick the highest\n",
    "        result = np.zeros_like(probs)\n",
    "        result[np.argmax(probs)] = 1.0\n",
    "        return result\n",
    "\n",
    "    # Apply temperature scaling\n",
    "    logits = np.log(probs + 1e-10)\n",
    "    scaled_logits = logits / temperature\n",
    "    exp_logits = np.exp(scaled_logits)\n",
    "    return exp_logits / np.sum(exp_logits)\n",
    "\n",
    "def visualize_distribution(probs, tokens):\n",
    "    \"\"\"Show probability distribution as bar chart\"\"\"\n",
    "    for i, token in enumerate(tokens):\n",
    "        bar_length = int(probs[i] * 100)\n",
    "        bar = '' * bar_length\n",
    "        print(f\"  {token:10s}: {probs[i]:.3f} {bar}\")\n",
    "\n",
    "# Test different temperatures\n",
    "temperatures = [0, 0.5, 1.0, 2.0]\n",
    "\n",
    "# TODO: Add temperature=3.0\n",
    "# temperatures = [0, 0.5, 1.0, 2.0, 3.0]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"Original (temperature=1.0) probabilities:\")\n",
    "print(\"=\"*70)\n",
    "visualize_distribution(original_probs, tokens)\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Temperature = {temp}\")\n",
    "    print('='*70)\n",
    "\n",
    "    # Apply temperature\n",
    "    new_probs = apply_temperature(original_probs, temp)\n",
    "\n",
    "    # Visualize\n",
    "    visualize_distribution(new_probs, tokens)\n",
    "\n",
    "    # Sample tokens\n",
    "    print(f\"\\n  Sampling 10 tokens:\")\n",
    "    if temp == 0:\n",
    "        samples = [tokens[np.argmax(new_probs)]] * 10\n",
    "    else:\n",
    "        samples = np.random.choice(tokens, size=10, p=new_probs)\n",
    "\n",
    "    print(f\"  {samples}\")\n",
    "\n",
    "    # Show diversity metric\n",
    "    unique_tokens = len(set(samples))\n",
    "    print(f\"   Diversity: {unique_tokens}/10 unique tokens\")\n",
    "\n",
    "    # Explain what's happening\n",
    "    if temp == 0:\n",
    "        print(f\"   Effect: DETERMINISTIC - always outputs '{samples[0]}'\")\n",
    "    elif temp < 1.0:\n",
    "        print(f\"   Effect: SHARPENED - makes confident tokens more likely\")\n",
    "    elif temp == 1.0:\n",
    "        print(f\"   Effect: UNCHANGED - original distribution\")\n",
    "    else:\n",
    "        print(f\"   Effect: FLATTENED - makes all tokens more equally likely\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ls7jVLYD1qa",
    "outputId": "441af855-8a20-4d81-88c6-127538c77ecd"
   },
   "execution_count": 28,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "======================================================================\n",
      "Original (temperature=1.0) probabilities:\n",
      "======================================================================\n",
      "  positive  : 0.500 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n",
      "  negative  : 0.300 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n",
      "  neutral   : 0.120 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n",
      "  good      : 0.050 \u2588\u2588\u2588\u2588\u2588\n",
      "  bad       : 0.030 \u2588\u2588\u2588\n",
      "\n",
      "======================================================================\n",
      "Temperature = 0\n",
      "======================================================================\n",
      "  positive  : 1.000 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n",
      "  negative  : 0.000 \n",
      "  neutral   : 0.000 \n",
      "  good      : 0.000 \n",
      "  bad       : 0.000 \n",
      "\n",
      "  Sampling 10 tokens:\n",
      "  ['positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive']\n",
      "  \u2192 Diversity: 1/10 unique tokens\n",
      "  \u2192 Effect: DETERMINISTIC - always outputs 'positive'\n",
      "\n",
      "======================================================================\n",
      "Temperature = 0.5\n",
      "======================================================================\n",
      "  positive  : 0.699 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n",
      "  negative  : 0.252 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n",
      "  neutral   : 0.040 \u2588\u2588\u2588\u2588\n",
      "  good      : 0.007 \n",
      "  bad       : 0.003 \n",
      "\n",
      "  Sampling 10 tokens:\n",
      "  ['positive' 'positive' 'positive' 'neutral' 'positive' 'positive'\n",
      " 'positive' 'positive' 'negative' 'positive']\n",
      "  \u2192 Diversity: 3/10 unique tokens\n",
      "  \u2192 Effect: SHARPENED - makes confident tokens more likely\n",
      "\n",
      "======================================================================\n",
      "Temperature = 1.0\n",
      "======================================================================\n",
      "  positive  : 0.500 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n",
      "  negative  : 0.300 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n",
      "  neutral   : 0.120 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n",
      "  good      : 0.050 \u2588\u2588\u2588\u2588\u2588\n",
      "  bad       : 0.030 \u2588\u2588\u2588\n",
      "\n",
      "  Sampling 10 tokens:\n",
      "  ['positive' 'bad' 'positive' 'positive' 'good' 'good' 'negative'\n",
      " 'negative' 'positive' 'good']\n",
      "  \u2192 Diversity: 4/10 unique tokens\n",
      "  \u2192 Effect: UNCHANGED - original distribution\n",
      "\n",
      "======================================================================\n",
      "Temperature = 2.0\n",
      "======================================================================\n",
      "  positive  : 0.354 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n",
      "  negative  : 0.274 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n",
      "  neutral   : 0.173 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n",
      "  good      : 0.112 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n",
      "  bad       : 0.087 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n",
      "\n",
      "  Sampling 10 tokens:\n",
      "  ['positive' 'negative' 'positive' 'negative' 'positive' 'neutral'\n",
      " 'negative' 'positive' 'negative' 'negative']\n",
      "  \u2192 Diversity: 3/10 unique tokens\n",
      "  \u2192 Effect: FLATTENED - makes all tokens more equally likely\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Questions\n",
    "\n",
    "1. Why is temperature=0 critical for classification tasks? What would go wrong with temperature=1.0?\n",
    "\n",
    "2. Compare temperature=0.5 vs 2.0. At what temperature did low-probability tokens like \"bad\" start appearing in samples?\n",
    "\n",
    "3. With the uncertain distribution ([0.25, 0.24, 0.22, 0.18, 0.11]), how did temperature effects differ from the confident model?"
   ],
   "metadata": {
    "id": "pA4FHgvIENtG"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About This Task:**\n",
    "Embedding similarity measures how semantically close two texts are in vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Easy Task 4: Embedding Similarity Analysis\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Execute code to see similarity matrix for movie reviews\n",
    "2. Identify which reviews cluster together and which are distant\n",
    "3. Uncomment TODO to add reviews from different domains\n",
    "4. Analyze whether restaurant/product reviews cluster with movie reviews\n",
    "5. Add a random sentence to test similarity boundaries"
   ],
   "metadata": {
    "id": "Y2tgqir4Ecu0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# Movie reviews - different types\n",
    "texts = [\n",
    "    # Positive reviews\n",
    "    \"Amazing movie! Absolutely loved it!\",\n",
    "    \"Fantastic film, highly recommend!\",\n",
    "    \"Great cinematography and acting\",\n",
    "\n",
    "    # Negative reviews\n",
    "    \"Terrible waste of time\",\n",
    "    \"Very disappointing and boring\",\n",
    "    \"Poor acting and weak plot\",\n",
    "\n",
    "    # Neutral reviews\n",
    "    \"It was okay, nothing special\",\n",
    "    \"Some good parts, some bad parts\",\n",
    "\n",
    "    # Off-topic\n",
    "    \"The weather is nice today\",\n",
    "    \"I like eating pizza\"\n",
    "]\n",
    "\n",
    "# TODO: Test domain transfer\n",
    "# texts = [\n",
    "#     # Positive movie reviews\n",
    "#     \"Amazing movie! Absolutely loved it!\",\n",
    "#     \"Fantastic film, highly recommend!\",\n",
    "#     \"Great cinematography and acting\",\n",
    "#\n",
    "#     # Negative movie reviews\n",
    "#     \"Terrible waste of time\",\n",
    "#     \"Very disappointing and boring\",\n",
    "#     \"Poor acting and weak plot\",\n",
    "#\n",
    "#     # Neutral movie reviews\n",
    "#     \"It was okay, nothing special\",\n",
    "#     \"Some good parts, some bad parts\",\n",
    "#\n",
    "#     # Positive restaurant review (different domain!)\n",
    "#     \"Amazing food! Absolutely loved it!\",\n",
    "#     \"Fantastic restaurant, highly recommend!\",\n",
    "#\n",
    "#     # Off-topic\n",
    "#     \"The weather is nice today\",\n",
    "#     \"I like eating pizza\",\n",
    "# ]\n",
    "\n",
    "labels = [f\"Text {i+1}\" for i in range(len(texts))]\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = model.encode(texts)\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "print(f\"Each text converted to {embeddings.shape[1]}-dimensional vector\")\n",
    "print(f\"Comparing {embeddings.shape[0]} texts\\n\")\n",
    "\n",
    "# Show full similarity matrix\n",
    "print(\"Similarity Matrix (0=unrelated, 1=identical):\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'':10s}\", end=\"\")\n",
    "for i in range(len(texts)):\n",
    "    print(f\"T{i+1:2d} \", end=\"\")\n",
    "print()\n",
    "\n",
    "for i in range(len(texts)):\n",
    "    print(f\"Text {i+1:2d}:  \", end=\"\")\n",
    "    for j in range(len(texts)):\n",
    "        if i == j:\n",
    "            print(\"---- \", end=\"\")\n",
    "        else:\n",
    "            sim = similarity_matrix[i][j]\n",
    "            if sim > 0.6:\n",
    "                print(f\"{sim:.2f}*\", end=\"\")  # High similarity\n",
    "            else:\n",
    "                print(f\"{sim:.2f} \", end=\"\")\n",
    "            print(\" \", end=\"\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n* = High similarity (>0.6)\")\n",
    "\n",
    "# Detailed comparisons\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED COMPARISONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparisons = [\n",
    "    (0, 1, \"Positive review vs Positive review\"),\n",
    "    (3, 4, \"Negative review vs Negative review\"),\n",
    "    (0, 3, \"Positive review vs Negative review\"),\n",
    "    (0, 8, \"Movie review vs Off-topic text\"),\n",
    "]\n",
    "\n",
    "# TODO: Compare movie vs restaurant reviews\n",
    "# comparisons.append((0, 8, \"Positive MOVIE vs Positive RESTAURANT\"))\n",
    "\n",
    "for i, j, description in comparisons:\n",
    "    if i < len(texts) and j < len(texts):\n",
    "        sim = similarity_matrix[i][j]\n",
    "        print(f\"\\n{description}:\")\n",
    "        print(f\"  Text {i+1}: '{texts[i]}'\")\n",
    "        print(f\"  Text {j+1}: '{texts[j]}'\")\n",
    "        print(f\"  Similarity: {sim:.3f}\")\n",
    "\n",
    "        if sim > 0.7:\n",
    "            print(f\"   Very similar! These texts are closely related in meaning\")\n",
    "        elif sim > 0.4:\n",
    "            print(f\"   Moderately similar. Some shared concepts\")\n",
    "        else:\n",
    "            print(f\"   Different topics or sentiments\")\n",
    "\n",
    "# Find clusters\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLUSTERS (which texts group together?)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find texts similar to first positive review\n",
    "positive_idx = 0\n",
    "similar_to_positive = []\n",
    "for i in range(len(texts)):\n",
    "    if i != positive_idx and similarity_matrix[positive_idx][i] > 0.5:\n",
    "        similar_to_positive.append((i, similarity_matrix[positive_idx][i]))\n",
    "\n",
    "print(f\"\\nTexts similar to '{texts[positive_idx]}':\")\n",
    "for idx, sim in sorted(similar_to_positive, key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  Text {idx+1} (sim={sim:.3f}): '{texts[idx]}'\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mahDQbyaEHUn",
    "outputId": "228a42e9-6c33-457d-966c-6e3c3004dc16"
   },
   "execution_count": 29,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Each text converted to 768-dimensional vector\n",
      "Comparing 10 texts\n",
      "\n",
      "Similarity Matrix (0=unrelated, 1=identical):\n",
      "================================================================================\n",
      "          T 1 T 2 T 3 T 4 T 5 T 6 T 7 T 8 T 9 T10 \n",
      "Text  1:  ---- 0.77* 0.52  0.09  0.24  0.23  0.31  0.16  0.10  0.03  \n",
      "Text  2:  0.77* ---- 0.47  0.11  0.26  0.23  0.21  0.14  0.09  0.01  \n",
      "Text  3:  0.52  0.47  ---- 0.10  0.32  0.51  0.36  0.26  0.12  0.06  \n",
      "Text  4:  0.09  0.11  0.10  ---- 0.55  0.29  0.34  0.21  0.03  0.07  \n",
      "Text  5:  0.24  0.26  0.32  0.55  ---- 0.50  0.55  0.36  0.09  0.13  \n",
      "Text  6:  0.23  0.23  0.51  0.29  0.50  ---- 0.37  0.25  -0.03  0.08  \n",
      "Text  7:  0.31  0.21  0.36  0.34  0.55  0.37  ---- 0.34  0.16  0.11  \n",
      "Text  8:  0.16  0.14  0.26  0.21  0.36  0.25  0.34  ---- 0.12  0.21  \n",
      "Text  9:  0.10  0.09  0.12  0.03  0.09  -0.03  0.16  0.12  ---- 0.20  \n",
      "Text 10:  0.03  0.01  0.06  0.07  0.13  0.08  0.11  0.21  0.20  ---- \n",
      "\n",
      "* = High similarity (>0.6)\n",
      "\n",
      "================================================================================\n",
      "DETAILED COMPARISONS\n",
      "================================================================================\n",
      "\n",
      "Positive review vs Positive review:\n",
      "  Text 1: 'Amazing movie! Absolutely loved it!'\n",
      "  Text 2: 'Fantastic film, highly recommend!'\n",
      "  Similarity: 0.768\n",
      "  \u2192 Very similar! These texts are closely related in meaning\n",
      "\n",
      "Negative review vs Negative review:\n",
      "  Text 4: 'Terrible waste of time'\n",
      "  Text 5: 'Very disappointing and boring'\n",
      "  Similarity: 0.545\n",
      "  \u2192 Moderately similar. Some shared concepts\n",
      "\n",
      "Positive review vs Negative review:\n",
      "  Text 1: 'Amazing movie! Absolutely loved it!'\n",
      "  Text 4: 'Terrible waste of time'\n",
      "  Similarity: 0.090\n",
      "  \u2192 Different topics or sentiments\n",
      "\n",
      "Movie review vs Off-topic text:\n",
      "  Text 1: 'Amazing movie! Absolutely loved it!'\n",
      "  Text 9: 'The weather is nice today'\n",
      "  Similarity: 0.095\n",
      "  \u2192 Different topics or sentiments\n",
      "\n",
      "================================================================================\n",
      "CLUSTERS (which texts group together?)\n",
      "================================================================================\n",
      "\n",
      "Texts similar to 'Amazing movie! Absolutely loved it!':\n",
      "  Text 2 (sim=0.768): 'Fantastic film, highly recommend!'\n",
      "  Text 3 (sim=0.516): 'Great cinematography and acting'\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Questions\n",
    "\n",
    "1. Compare similarity between Text 1 and Text 2 (both positive) vs Text 1 and Text 4 (positive vs negative). What aspects of semantic meaning do embeddings prioritize?\n",
    "\n",
    "2. Find similarity scores between two negative reviews (Text 4 and Text 5) and two positive reviews (Text 1 and Text 2). Why would averaging embeddings per class work for classification?\n",
    "\n",
    "3. After adding restaurant reviews: How similar was \"Amazing food\" to \"Amazing movie\"? What does this reveal about domain transfer?"
   ],
   "metadata": {
    "id": "TL1g6q0kEjPw"
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}