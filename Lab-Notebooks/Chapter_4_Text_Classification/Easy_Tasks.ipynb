{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlH0PB_t39yg"
      },
      "source": [
        "# Chapter 4: Text Classification - Easy Tasks\n",
        "\n",
        "This notebook covers the basic text classification concepts: zero-shot classification, classifier strategies, temperature effects, and embedding similarity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xus_y4Ga39yk"
      },
      "source": [
        "\n",
        "## Setup\n",
        "\n",
        "Run all cells in this section to set up the environment and load necessary data.\n",
        "\n",
        "Before running these cells, it is advised to first run and try to get familiar with the codes and concepts from the main Chapter 4 Notebook (`Start_Here.ipynb`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LGW2SD-c864"
      },
      "source": [
        "### [OPTIONAL] - Installing Packages on <img src=\"https://colab.google/static/images/icons/colab.png\" width=100>\n",
        "\n",
        "\n",
        "If you are viewing this notebook on Google Colab (or any other cloud vendor), you need to **uncomment and run** the following codeblock to install the dependencies for this chapter:\n",
        "\n",
        "---\n",
        "\n",
        " **NOTE**: We will want to use a GPU to run the examples in this notebook. In Google Colab, go to\n",
        "**Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4**.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "N-PxmOIhc865"
      },
      "outputs": [],
      "source": [
        " %%capture\n",
        "!pip install transformers sentence-transformers openai\n",
        "!pip install -U datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WToW_Psg39ys"
      },
      "source": [
        "### Data Loading\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the same data as in Start_Here.ipynb notebook"
      ],
      "metadata": {
        "id": "TBJUHeDD4eAy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5phRS_z2U_3T",
        "outputId": "0721c545-1189-421c-e11b-d300191281f6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 8530\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 1066\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 1066\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load our data\n",
        "data = load_dataset(\"rotten_tomatoes\")\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXl6LASB39yw"
      },
      "source": [
        "### Helper Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "X0KyKHtqyjn3"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def evaluate_performance(y_true, y_pred):\n",
        "    \"\"\"Create and print the classification report\"\"\"\n",
        "    performance = classification_report(\n",
        "        y_true, y_pred,\n",
        "        target_names=[\"Negative Review\", \"Positive Review\"]\n",
        "    )\n",
        "    print(performance)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Your Turn - Text Classification Experiments\n",
        "\n",
        "Run each task first to see the results without the changes. Follow the instructions to modify and experiment.\n",
        "\n",
        "These tasks are mostly for observation and require minimal modification to the code."
      ],
      "metadata": {
        "id": "NKYNfoaVC4hU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Easy Tasks - Hands-On Experimentation\n",
        "\n"
      ],
      "metadata": {
        "id": "gwqUbziIDNee"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD982uIW39y-"
      },
      "source": [
        "**About This Task:**\n",
        "Zero-shot classification classifies text without training examples."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Easy Task 1: Zero-Shot Classifier\n",
        "\n",
        "### Instructions\n",
        "\n",
        "1. Execute the code to see baseline predictions for 3 basic reviews\n",
        "2. Uncomment the larger `test_reviews` list and run again to test harder cases\n",
        "3. Uncomment one label option to see how wording affects predictions\n",
        "4. Compare which label style works best for ambiguous reviews"
      ],
      "metadata": {
        "id": "TnHfnLsxDQcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "\n",
        "# Test reviews - THESE WORK AS-IS\n",
        "test_reviews = [\n",
        "    \"This movie was absolutely fantastic! A masterpiece!\",\n",
        "    \"Terrible waste of time. Very disappointing.\",\n",
        "    \"An okay film, nothing special but watchable.\",\n",
        "]\n",
        "\n",
        "# TODO: Uncomment to test harder cases\n",
        "# test_reviews = [\n",
        "#     \"This movie was absolutely fantastic! A masterpiece!\",\n",
        "#     \"Terrible waste of time. Very disappointing.\",\n",
        "#     \"An okay film, nothing special but watchable.\",\n",
        "#     \"Oh great, another masterpiece... NOT!\",  # Sarcastic\n",
        "#     \"Boring.\",  # Very short\n",
        "#     \"Great acting but terrible plot.\",  # Mixed sentiment\n",
        "# ]\n",
        "\n",
        "# Label descriptions - THESE WORK AS-IS\n",
        "labels = [\n",
        "    \"A negative movie review\",\n",
        "    \"A positive movie review\"\n",
        "]\n",
        "\n",
        "# TODO: Try different label options\n",
        "# labels = [\"negative\", \"positive\"]  # Option 1: Simple\n",
        "# labels = [\"bad movie review\", \"good movie review\"]  # Option 2: Different wording\n",
        "# labels = [\"a scathing negative movie review\", \"an enthusiastic positive movie review\"]  # Option 3: Detailed\n",
        "\n",
        "# Create embeddings and calculate similarity\n",
        "label_embeddings = model.encode(labels)\n",
        "review_embeddings = model.encode(test_reviews)\n",
        "sim_matrix = cosine_similarity(review_embeddings, label_embeddings)"
      ],
      "metadata": {
        "id": "ZC6N2A3LC6JZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code cell, we simply load and use the model on the test reviews. You are encouraged to run the above cell with the different labels option to see the difference similarity."
      ],
      "metadata": {
        "id": "B-NX5qe-53ph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classification Results:\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "for i, review in enumerate(test_reviews):\n",
        "    prediction = np.argmax(sim_matrix[i])\n",
        "    confidence = sim_matrix[i][prediction]\n",
        "    margin = abs(sim_matrix[i][0] - sim_matrix[i][1])\n",
        "\n",
        "    print(f\"\\nReview {i+1}: '{review}'\")\n",
        "    print(f\"Predicted: {labels[prediction]}\")\n",
        "    print(f\"Confidence: {confidence:.3f}\")\n",
        "    print(f\"Scores -> Negative: {sim_matrix[i][0]:.3f}, Positive: {sim_matrix[i][1]:.3f}\")\n",
        "    print(f\"Margin (certainty): {margin:.3f}\")"
      ],
      "metadata": {
        "id": "DeRNmhHE513y",
        "outputId": "14cf6192-25fe-4261-d3ab-074a8de0586a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Results:\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Review 1: 'This movie was absolutely fantastic! A masterpiece!'\n",
            "Predicted: A positive movie review\n",
            "Confidence: 0.493\n",
            "Scores -> Negative: 0.382, Positive: 0.493\n",
            "Margin (certainty): 0.111\n",
            "\n",
            "Review 2: 'Terrible waste of time. Very disappointing.'\n",
            "Predicted: A negative movie review\n",
            "Confidence: 0.439\n",
            "Scores -> Negative: 0.439, Positive: 0.299\n",
            "Margin (certainty): 0.140\n",
            "\n",
            "Review 3: 'An okay film, nothing special but watchable.'\n",
            "Predicted: A positive movie review\n",
            "Confidence: 0.542\n",
            "Scores -> Negative: 0.503, Positive: 0.542\n",
            "Margin (certainty): 0.039\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test other `test_reviews` or `labels`, comment the previous one. Make sure only one of the sets is uncommented.\n",
        "\n",
        "Tip: to uncomment/comment lines, use `Ctrl + /`"
      ],
      "metadata": {
        "id": "NOucKIpW6kU1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Questions\n",
        "\n",
        "1. Why did the classifier fail on the sarcastic review (\"*Oh great, another masterpiece... NOT*\")? What semantic features did embeddings miss?\n",
        "\n",
        "2. Which reviews changed predictions when you modified label descriptions? Why are some reviews more sensitive to label wording than others?\n",
        "\n",
        "3. Which reviews have low confidence margins (<0.1)? What linguistic features make certain reviews harder to classify?\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "hAfvoaoTDidJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1V2I1IWn39zB"
      },
      "source": [
        "**About This Task:**\n",
        "Different classification strategies affect model behavior and performance."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Easy Task 2: Classifier Strategy Analysis\n",
        "\n",
        "### Instructions\n",
        "\n",
        "1. Execute code to see three pre-built classifiers (conservative, aggressive, balanced)\n",
        "2. Study each confusion matrix to identify error patterns\n",
        "3. Modify `classifier_yours` to create a very conservative classifier (precision > 0.9)\n",
        "4. Uncomment the TODO section to analyze your classifier\n",
        "5. Experiment with creating different strategy combinations"
      ],
      "metadata": {
        "id": "Rhuy6BY6DxDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# True labels: first 5 are negative (0), last 5 are positive (1)\n",
        "y_true = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])\n",
        "\n",
        "# Pre-built classifiers to analyze\n",
        "classifier_conservative = np.array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1])  # Rarely predicts positive\n",
        "classifier_aggressive = np.array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1])     # Often predicts positive\n",
        "classifier_balanced = np.array([0, 0, 0, 1, 0, 0, 1, 1, 1, 1])       # Balanced approach\n",
        "\n",
        "# TO-DO: Modify these predictions to make YOUR classifier\n",
        "# Try to achieve precision > 0.9 (be very selective about predicting 1)\n",
        "classifier_yours = np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1])\n",
        "\n"
      ],
      "metadata": {
        "id": "0Sw3mcCXDddB"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_classifier(name, y_true, y_pred):\n",
        "    \"\"\"Analyze classifier performance with detailed breakdown\"\"\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"{name}\")\n",
        "    print('='*70)\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Show confusion matrix with labels\n",
        "    print(f\"\\nConfusion Matrix:\")\n",
        "    print(f\"                    Predicted Neg | Predicted Pos\")\n",
        "    print(f\"Actual Neg (0):          {cm[0][0]}       |       {cm[0][1]}  False Positives (BAD)\")\n",
        "    print(f\"Actual Pos (1):          {cm[1][0]}       |       {cm[1][1]}  True Positives (GOOD)\")\n",
        "    print(f\"                                                     \")\n",
        "    print(f\"                   False Negatives (BAD)              \")\n",
        "\n",
        "    # Calculate metrics\n",
        "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "\n",
        "    print(f\"\\nMetrics:\")\n",
        "    print(f\"Precision: {precision:.3f} = TP/(TP+FP) = {cm[1][1]}/({cm[1][1]}+{cm[0][1]})\")\n",
        "    print(f\"            Of {cm[1][1]+cm[0][1]} positive predictions, {cm[1][1]} were correct\")\n",
        "    print(f\"\\nRecall:    {recall:.3f} = TP/(TP+FN) = {cm[1][1]}/({cm[1][1]}+{cm[1][0]})\")\n",
        "    print(f\"            Of {cm[1][1]+cm[1][0]} actual positives, found {cm[1][1]}\")\n",
        "    print(f\"\\nF1 Score:  {f1:.3f} = 2*(P*R)/(P+R)\")\n",
        "\n",
        "    # Explain strategy\n",
        "    if precision > recall + 0.1:\n",
        "        print(f\"\\n Strategy: CONSERVATIVE (careful about predicting positive)\")\n",
        "        print(f\"   Few false alarms (only {cm[0][1]} false positives)\")\n",
        "        print(f\"   Misses actual positives ({cm[1][0]} false negatives)\")\n",
        "    elif recall > precision + 0.1:\n",
        "        print(f\"\\n Strategy: AGGRESSIVE (liberal about predicting positive)\")\n",
        "        print(f\"   Finds most positives (only {cm[1][0]} false negatives)\")\n",
        "        print(f\"   Many false alarms ({cm[0][1]} false positives)\")\n",
        "    else:\n",
        "        print(f\"\\n Strategy: BALANCED\")\n",
        "\n",
        "    return precision, recall, f1\n",
        "\n",
        "# Analyze pre-built classifiers\n",
        "results = {}\n",
        "for name, classifier in [\n",
        "    (\"Conservative Classifier\", classifier_conservative),\n",
        "    (\"Aggressive Classifier\", classifier_aggressive),\n",
        "    (\"Balanced Classifier\", classifier_balanced),\n",
        "]:\n",
        "    p, r, f = analyze_classifier(name, y_true, classifier)\n",
        "    results[name] = (p, r, f)\n",
        "\n",
        "# TODO: Analyze your classifier\n",
        "# print(\"\\n\" + \"=\"*70)\n",
        "# print(\"ANALYZING YOUR CLASSIFIER\")\n",
        "# print(\"=\"*70)\n",
        "# p, r, f = analyze_classifier(\"YOUR Classifier\", y_true, classifier_yours)\n",
        "# results[\"YOUR Classifier\"] = (p, r, f)\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"{'Classifier':<25} {'Precision':<12} {'Recall':<12} {'F1':<12}\")\n",
        "print(\"-\"*70)\n",
        "for name, (p, r, f) in results.items():\n",
        "    print(f\"{name:<25} {p:.3f}        {r:.3f}       {f:.3f}\")"
      ],
      "metadata": {
        "id": "kRA9htvy8N3W",
        "outputId": "605560a1-11f3-4e5b-a73f-0b8ed5b7e918",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "Conservative Classifier\n",
            "======================================================================\n",
            "\n",
            "Confusion Matrix:\n",
            "                    Predicted Neg | Predicted Pos\n",
            "Actual Neg (0):          5       |       0  False Positives (BAD)\n",
            "Actual Pos (1):          2       |       3  True Positives (GOOD)\n",
            "                                                     \n",
            "                   False Negatives (BAD)              \n",
            "\n",
            "Metrics:\n",
            "Precision: 1.000 = TP/(TP+FP) = 3/(3+0)\n",
            "            Of 3 positive predictions, 3 were correct\n",
            "\n",
            "Recall:    0.600 = TP/(TP+FN) = 3/(3+2)\n",
            "            Of 5 actual positives, found 3\n",
            "\n",
            "F1 Score:  0.750 = 2*(P*R)/(P+R)\n",
            "\n",
            " Strategy: CONSERVATIVE (careful about predicting positive)\n",
            "   Few false alarms (only 0 false positives)\n",
            "   Misses actual positives (2 false negatives)\n",
            "\n",
            "======================================================================\n",
            "Aggressive Classifier\n",
            "======================================================================\n",
            "\n",
            "Confusion Matrix:\n",
            "                    Predicted Neg | Predicted Pos\n",
            "Actual Neg (0):          1       |       4  False Positives (BAD)\n",
            "Actual Pos (1):          0       |       5  True Positives (GOOD)\n",
            "                                                     \n",
            "                   False Negatives (BAD)              \n",
            "\n",
            "Metrics:\n",
            "Precision: 0.556 = TP/(TP+FP) = 5/(5+4)\n",
            "            Of 9 positive predictions, 5 were correct\n",
            "\n",
            "Recall:    1.000 = TP/(TP+FN) = 5/(5+0)\n",
            "            Of 5 actual positives, found 5\n",
            "\n",
            "F1 Score:  0.714 = 2*(P*R)/(P+R)\n",
            "\n",
            " Strategy: AGGRESSIVE (liberal about predicting positive)\n",
            "   Finds most positives (only 0 false negatives)\n",
            "   Many false alarms (4 false positives)\n",
            "\n",
            "======================================================================\n",
            "Balanced Classifier\n",
            "======================================================================\n",
            "\n",
            "Confusion Matrix:\n",
            "                    Predicted Neg | Predicted Pos\n",
            "Actual Neg (0):          4       |       1  False Positives (BAD)\n",
            "Actual Pos (1):          1       |       4  True Positives (GOOD)\n",
            "                                                     \n",
            "                   False Negatives (BAD)              \n",
            "\n",
            "Metrics:\n",
            "Precision: 0.800 = TP/(TP+FP) = 4/(4+1)\n",
            "            Of 5 positive predictions, 4 were correct\n",
            "\n",
            "Recall:    0.800 = TP/(TP+FN) = 4/(4+1)\n",
            "            Of 5 actual positives, found 4\n",
            "\n",
            "F1 Score:  0.800 = 2*(P*R)/(P+R)\n",
            "\n",
            " Strategy: BALANCED\n",
            "\n",
            "======================================================================\n",
            "SUMMARY\n",
            "======================================================================\n",
            "Classifier                Precision    Recall       F1          \n",
            "----------------------------------------------------------------------\n",
            "Conservative Classifier   1.000        0.600       0.750\n",
            "Aggressive Classifier     0.556        1.000       0.714\n",
            "Balanced Classifier       0.800        0.800       0.800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Questions\n",
        "\n",
        "1. The conservative classifier has 2 false negatives. What real-world mistake does this represent? Provide a movie review example.\n",
        "\n",
        "2. What strategy did you use to achieve high precision in `classifier_yours`? Why does predicting positive less frequently increase precision?\n",
        "\n",
        "3. Which classifier won on F1 score? Why doesn't the aggressive classifier win despite high recall?"
      ],
      "metadata": {
        "id": "GHv-B_ebD8CU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUmXASWa39zF"
      },
      "source": [
        "**About This Task:**\n",
        "Temperature controls randomness in language model outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Easy Task 3: Temperature Effects on Text Generation\n",
        "\n",
        "### Instructions\n",
        "\n",
        "1. Execute code to see how temperature affects token selection with a confident model\n",
        "2. Observe how probabilities and samples change across temperatures\n",
        "3. Uncomment the uncertain probability distribution and run again\n",
        "4. Compare temperature effects on confident vs uncertain models\n",
        "5. Uncomment TODO to add a new temperature value and analyze results"
      ],
      "metadata": {
        "id": "YkEaGnlqEEWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "original_probs = np.array([0.50, 0.30, 0.12, 0.05, 0.03])\n",
        "tokens = [\"positive\", \"negative\", \"neutral\", \"good\", \"bad\"]\n",
        "\n",
        "# Try uncertain distribution\n",
        "# original_probs = np.array([0.25, 0.24, 0.22, 0.18, 0.11]), much more balanced\n",
        "# Run again and compare the temperature effects\n",
        "\n",
        "def apply_temperature(probs, temperature):\n",
        "    \"\"\"Apply temperature scaling to change distribution sharpness\"\"\"\n",
        "    if temperature == 0:\n",
        "        # Deterministic: always pick the highest\n",
        "        result = np.zeros_like(probs)\n",
        "        result[np.argmax(probs)] = 1.0\n",
        "        return result\n",
        "\n",
        "    # Apply temperature scaling\n",
        "    logits = np.log(probs + 1e-10)\n",
        "    scaled_logits = logits / temperature\n",
        "    exp_logits = np.exp(scaled_logits)\n",
        "    return exp_logits / np.sum(exp_logits)\n",
        "\n",
        "def visualize_distribution(probs, tokens):\n",
        "    \"\"\"Show probability distribution as bar chart\"\"\"\n",
        "    for i, token in enumerate(tokens):\n",
        "        bar_length = int(probs[i] * 100)\n",
        "        bar = '' * bar_length\n",
        "        print(f\"  {token:10s}: {probs[i]:.3f} {bar}\")\n",
        "\n",
        "# Test different temperatures\n",
        "temperatures = [0, 0.5, 1.0, 2.0]\n",
        "\n",
        "# Add temperature=3.0\n",
        "# temperatures = [0, 0.5, 1.0, 2.0, 3.0]\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(f\"Original (temperature=1.0) probabilities:\")\n",
        "print(\"=\"*70)\n",
        "visualize_distribution(original_probs, tokens)\n",
        "\n",
        "for temp in temperatures:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Temperature = {temp}\")\n",
        "    print('='*70)\n",
        "\n",
        "    # Apply temperature\n",
        "    new_probs = apply_temperature(original_probs, temp)\n",
        "\n",
        "    # Visualize\n",
        "    visualize_distribution(new_probs, tokens)\n",
        "\n",
        "    # Sample tokens\n",
        "    print(f\"\\n  Sampling 10 tokens:\")\n",
        "    if temp == 0:\n",
        "        samples = [tokens[np.argmax(new_probs)]] * 10\n",
        "    else:\n",
        "        samples = np.random.choice(tokens, size=10, p=new_probs)\n",
        "\n",
        "    print(f\"  {samples}\")\n",
        "\n",
        "    # Show diversity metric\n",
        "    unique_tokens = len(set(samples))\n",
        "    print(f\"   Diversity: {unique_tokens}/10 unique tokens\")\n",
        "\n",
        "    # Explain what's happening\n",
        "    if temp == 0:\n",
        "        print(f\"   Effect: DETERMINISTIC - always outputs '{samples[0]}'\")\n",
        "    elif temp < 1.0:\n",
        "        print(f\"   Effect: SHARPENED - makes confident tokens more likely\")\n",
        "    elif temp == 1.0:\n",
        "        print(f\"   Effect: UNCHANGED - original distribution\")\n",
        "    else:\n",
        "        print(f\"   Effect: FLATTENED - makes all tokens more equally likely\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ls7jVLYD1qa",
        "outputId": "2b0012a9-5999-4cdf-c75b-eebb2f73e0b5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "Original (temperature=1.0) probabilities:\n",
            "======================================================================\n",
            "  positive  : 0.500 \n",
            "  negative  : 0.300 \n",
            "  neutral   : 0.120 \n",
            "  good      : 0.050 \n",
            "  bad       : 0.030 \n",
            "\n",
            "======================================================================\n",
            "Temperature = 0\n",
            "======================================================================\n",
            "  positive  : 1.000 \n",
            "  negative  : 0.000 \n",
            "  neutral   : 0.000 \n",
            "  good      : 0.000 \n",
            "  bad       : 0.000 \n",
            "\n",
            "  Sampling 10 tokens:\n",
            "  ['positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive']\n",
            "   Diversity: 1/10 unique tokens\n",
            "   Effect: DETERMINISTIC - always outputs 'positive'\n",
            "\n",
            "======================================================================\n",
            "Temperature = 0.5\n",
            "======================================================================\n",
            "  positive  : 0.699 \n",
            "  negative  : 0.252 \n",
            "  neutral   : 0.040 \n",
            "  good      : 0.007 \n",
            "  bad       : 0.003 \n",
            "\n",
            "  Sampling 10 tokens:\n",
            "  ['neutral' 'negative' 'positive' 'positive' 'positive' 'positive'\n",
            " 'neutral' 'good' 'positive' 'positive']\n",
            "   Diversity: 4/10 unique tokens\n",
            "   Effect: SHARPENED - makes confident tokens more likely\n",
            "\n",
            "======================================================================\n",
            "Temperature = 1.0\n",
            "======================================================================\n",
            "  positive  : 0.500 \n",
            "  negative  : 0.300 \n",
            "  neutral   : 0.120 \n",
            "  good      : 0.050 \n",
            "  bad       : 0.030 \n",
            "\n",
            "  Sampling 10 tokens:\n",
            "  ['negative' 'positive' 'positive' 'positive' 'negative' 'neutral'\n",
            " 'positive' 'negative' 'positive' 'negative']\n",
            "   Diversity: 3/10 unique tokens\n",
            "   Effect: UNCHANGED - original distribution\n",
            "\n",
            "======================================================================\n",
            "Temperature = 2.0\n",
            "======================================================================\n",
            "  positive  : 0.354 \n",
            "  negative  : 0.274 \n",
            "  neutral   : 0.173 \n",
            "  good      : 0.112 \n",
            "  bad       : 0.087 \n",
            "\n",
            "  Sampling 10 tokens:\n",
            "  ['bad' 'neutral' 'neutral' 'neutral' 'negative' 'positive' 'positive'\n",
            " 'positive' 'bad' 'neutral']\n",
            "   Diversity: 4/10 unique tokens\n",
            "   Effect: FLATTENED - makes all tokens more equally likely\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Questions\n",
        "\n",
        "1. Why is temperature=0 critical for classification tasks? What would go wrong with temperature=1.0?\n",
        "\n",
        "2. Compare temperature=0.5 vs 2.0. At what temperature did low-probability tokens like \"bad\" start appearing in samples?\n",
        "\n",
        "3. With the uncertain distribution ([0.25, 0.24, 0.22, 0.18, 0.11]), how did temperature effects differ from the confident model?"
      ],
      "metadata": {
        "id": "pA4FHgvIENtG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZDavrTL39zJ"
      },
      "source": [
        "**About This Task:**\n",
        "Embedding similarity measures how semantically close two texts are in vector space."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Easy Task 4: Embedding Similarity Analysis\n",
        "\n",
        "### Instructions\n",
        "\n",
        "1. Execute code to see similarity matrix for movie reviews\n",
        "2. Identify which reviews cluster together and which are distant\n",
        "3. Uncomment TODO to add reviews from different domains\n",
        "4. Analyze whether restaurant/product reviews cluster with movie reviews\n",
        "5. Add a random sentence to test similarity boundaries"
      ],
      "metadata": {
        "id": "Y2tgqir4Ecu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "\n",
        "# Movie reviews - different types\n",
        "texts = [\n",
        "    # Positive reviews\n",
        "    \"Amazing movie! Absolutely loved it!\",\n",
        "    \"Fantastic film, highly recommend!\",\n",
        "    \"Great cinematography and acting\",\n",
        "\n",
        "    # Negative reviews\n",
        "    \"Terrible waste of time\",\n",
        "    \"Very disappointing and boring\",\n",
        "    \"Poor acting and weak plot\",\n",
        "\n",
        "    # Neutral reviews\n",
        "    \"It was okay, nothing special\",\n",
        "    \"Some good parts, some bad parts\",\n",
        "\n",
        "    # Off-topic\n",
        "    \"The weather is nice today\",\n",
        "    \"I like eating pizza\"\n",
        "]\n",
        "\n",
        "# TODO: Test domain transfer\n",
        "# texts = [\n",
        "#     # Positive movie reviews\n",
        "#     \"Amazing movie! Absolutely loved it!\",\n",
        "#     \"Fantastic film, highly recommend!\",\n",
        "#     \"Great cinematography and acting\",\n",
        "#\n",
        "#     # Negative movie reviews\n",
        "#     \"Terrible waste of time\",\n",
        "#     \"Very disappointing and boring\",\n",
        "#     \"Poor acting and weak plot\",\n",
        "#\n",
        "#     # Neutral movie reviews\n",
        "#     \"It was okay, nothing special\",\n",
        "#     \"Some good parts, some bad parts\",\n",
        "#\n",
        "#     # Positive restaurant review (different domain!)\n",
        "#     \"Amazing food! Absolutely loved it!\",\n",
        "#     \"Fantastic restaurant, highly recommend!\",\n",
        "#\n",
        "#     # Off-topic\n",
        "#     \"The weather is nice today\",\n",
        "#     \"I like eating pizza\",\n",
        "# ]\n",
        "\n",
        "labels = [f\"Text {i+1}\" for i in range(len(texts))]\n",
        "\n",
        "# Generate embeddings\n",
        "embeddings = model.encode(texts)\n",
        "similarity_matrix = cosine_similarity(embeddings)\n",
        "\n",
        "print(f\"Each text converted to {embeddings.shape[1]}-dimensional vector\")\n",
        "print(f\"Comparing {embeddings.shape[0]} texts\\n\")\n",
        "\n",
        "# Show full similarity matrix\n",
        "print(\"Similarity Matrix (0=unrelated, 1=identical):\")\n",
        "print(\"=\"*80)\n",
        "print(f\"{'':10s}\", end=\"\")\n",
        "for i in range(len(texts)):\n",
        "    print(f\"T{i+1:2d} \", end=\"\")\n",
        "print()\n",
        "\n",
        "for i in range(len(texts)):\n",
        "    print(f\"Text {i+1:2d}:  \", end=\"\")\n",
        "    for j in range(len(texts)):\n",
        "        if i == j:\n",
        "            print(\"---- \", end=\"\")\n",
        "        else:\n",
        "            sim = similarity_matrix[i][j]\n",
        "            if sim > 0.6:\n",
        "                print(f\"{sim:.2f}*\", end=\"\")  # High similarity\n",
        "            else:\n",
        "                print(f\"{sim:.2f} \", end=\"\")\n",
        "            print(\" \", end=\"\")\n",
        "    print()\n",
        "\n",
        "print(\"\\n* = High similarity (>0.6)\")\n",
        "\n",
        "# Detailed comparisons\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DETAILED COMPARISONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "comparisons = [\n",
        "    (0, 1, \"Positive review vs Positive review\"),\n",
        "    (3, 4, \"Negative review vs Negative review\"),\n",
        "    (0, 3, \"Positive review vs Negative review\"),\n",
        "    (0, 8, \"Movie review vs Off-topic text\"),\n",
        "]\n",
        "\n",
        "# TODO: Compare movie vs restaurant reviews\n",
        "# comparisons.append((0, 8, \"Positive MOVIE vs Positive RESTAURANT\"))\n",
        "\n",
        "for i, j, description in comparisons:\n",
        "    if i < len(texts) and j < len(texts):\n",
        "        sim = similarity_matrix[i][j]\n",
        "        print(f\"\\n{description}:\")\n",
        "        print(f\"  Text {i+1}: '{texts[i]}'\")\n",
        "        print(f\"  Text {j+1}: '{texts[j]}'\")\n",
        "        print(f\"  Similarity: {sim:.3f}\")\n",
        "\n",
        "        if sim > 0.7:\n",
        "            print(f\"   Very similar! These texts are closely related in meaning\")\n",
        "        elif sim > 0.4:\n",
        "            print(f\"   Moderately similar. Some shared concepts\")\n",
        "        else:\n",
        "            print(f\"   Different topics or sentiments\")\n",
        "\n",
        "# Find clusters\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CLUSTERS (which texts group together?)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Find texts similar to first positive review\n",
        "positive_idx = 0\n",
        "similar_to_positive = []\n",
        "for i in range(len(texts)):\n",
        "    if i != positive_idx and similarity_matrix[positive_idx][i] > 0.5:\n",
        "        similar_to_positive.append((i, similarity_matrix[positive_idx][i]))\n",
        "\n",
        "print(f\"\\nTexts similar to '{texts[positive_idx]}':\")\n",
        "for idx, sim in sorted(similar_to_positive, key=lambda x: x[1], reverse=True):\n",
        "    print(f\"  Text {idx+1} (sim={sim:.3f}): '{texts[idx]}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mahDQbyaEHUn",
        "outputId": "ae8b87f3-48c8-47dd-826a-8e4842a3df0e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Each text converted to 768-dimensional vector\n",
            "Comparing 10 texts\n",
            "\n",
            "Similarity Matrix (0=unrelated, 1=identical):\n",
            "================================================================================\n",
            "          T 1 T 2 T 3 T 4 T 5 T 6 T 7 T 8 T 9 T10 \n",
            "Text  1:  ---- 0.77* 0.52  0.09  0.24  0.23  0.31  0.16  0.10  0.03  \n",
            "Text  2:  0.77* ---- 0.47  0.11  0.26  0.23  0.21  0.14  0.09  0.01  \n",
            "Text  3:  0.52  0.47  ---- 0.10  0.32  0.51  0.36  0.26  0.12  0.06  \n",
            "Text  4:  0.09  0.11  0.10  ---- 0.55  0.29  0.34  0.21  0.03  0.07  \n",
            "Text  5:  0.24  0.26  0.32  0.55  ---- 0.50  0.55  0.36  0.09  0.13  \n",
            "Text  6:  0.23  0.23  0.51  0.29  0.50  ---- 0.37  0.25  -0.03  0.08  \n",
            "Text  7:  0.31  0.21  0.36  0.34  0.55  0.37  ---- 0.34  0.16  0.11  \n",
            "Text  8:  0.16  0.14  0.26  0.21  0.36  0.25  0.34  ---- 0.12  0.21  \n",
            "Text  9:  0.10  0.09  0.12  0.03  0.09  -0.03  0.16  0.12  ---- 0.20  \n",
            "Text 10:  0.03  0.01  0.06  0.07  0.13  0.08  0.11  0.21  0.20  ---- \n",
            "\n",
            "* = High similarity (>0.6)\n",
            "\n",
            "================================================================================\n",
            "DETAILED COMPARISONS\n",
            "================================================================================\n",
            "\n",
            "Positive review vs Positive review:\n",
            "  Text 1: 'Amazing movie! Absolutely loved it!'\n",
            "  Text 2: 'Fantastic film, highly recommend!'\n",
            "  Similarity: 0.768\n",
            "   Very similar! These texts are closely related in meaning\n",
            "\n",
            "Negative review vs Negative review:\n",
            "  Text 4: 'Terrible waste of time'\n",
            "  Text 5: 'Very disappointing and boring'\n",
            "  Similarity: 0.545\n",
            "   Moderately similar. Some shared concepts\n",
            "\n",
            "Positive review vs Negative review:\n",
            "  Text 1: 'Amazing movie! Absolutely loved it!'\n",
            "  Text 4: 'Terrible waste of time'\n",
            "  Similarity: 0.090\n",
            "   Different topics or sentiments\n",
            "\n",
            "Movie review vs Off-topic text:\n",
            "  Text 1: 'Amazing movie! Absolutely loved it!'\n",
            "  Text 9: 'The weather is nice today'\n",
            "  Similarity: 0.095\n",
            "   Different topics or sentiments\n",
            "\n",
            "================================================================================\n",
            "CLUSTERS (which texts group together?)\n",
            "================================================================================\n",
            "\n",
            "Texts similar to 'Amazing movie! Absolutely loved it!':\n",
            "  Text 2 (sim=0.768): 'Fantastic film, highly recommend!'\n",
            "  Text 3 (sim=0.516): 'Great cinematography and acting'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Questions\n",
        "\n",
        "1. Compare similarity between Text 1 and Text 2 (both positive) vs Text 1 and Text 4 (positive vs negative). What aspects of semantic meaning do embeddings prioritize?\n",
        "\n",
        "2. Find similarity scores between two negative reviews (Text 4 and Text 5) and two positive reviews (Text 1 and Text 2). Why would averaging embeddings per class work for classification?\n",
        "\n",
        "3. After adding restaurant reviews: How similar was \"Amazing food\" to \"Amazing movie\"? What does this reveal about domain transfer?"
      ],
      "metadata": {
        "id": "TL1g6q0kEjPw"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}