{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LlH0PB_t39yg"
   },
   "source": [
    "# Chapter 4: Text Classification - Easy Tasks (Solutions)\n",
    "\n",
    "Complete working solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xus_y4Ga39yk"
   },
   "source": [
    "\n",
    "## Setup\n",
    "\n",
    "Run all cells in this section to set up the environment and load necessary data.\n",
    "\n",
    "Before running these cells, it is advised to first run and try to get familiar with the codes and concepts from the main Chapter 4 Notebook (`Start_Here.ipynb`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LGW2SD-c864"
   },
   "source": [
    "### [Optional] - Installing Packages on <img src=\"https://colab.google/static/images/icons/colab.png\" width=100>\n",
    "\n",
    "If you are viewing this notebook on Google Colab (or any other cloud vendor), you need to **uncomment and run** the following codeblock to install the dependencies for this chapter:\n",
    "\n",
    "---\n",
    "\n",
    " **Note**: We will want to use a GPU to run the examples in this notebook. In Google Colab, go to\n",
    "**Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "N-PxmOIhc865"
   },
   "outputs": [],
   "source": [
    " %%capture\n",
    "!pip install transformers sentence-transformers openai\n",
    "!pip install -U datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WToW_Psg39ys"
   },
   "source": [
    "### Data Loading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We use the same data as in Start_Here.ipynb notebook"
   ],
   "metadata": {
    "id": "TBJUHeDD4eAy"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616,
     "referenced_widgets": [
      "97950f51209a426393b8627508ee201e",
      "1778998fd9474775a0e40f0b61fc0efd",
      "d6e926285e734de18c34bacddb372cfd",
      "cb97e75f46d1426fb90bb237c859db47",
      "5e1e6105af87462786d41c932f6a7d30",
      "34c0fb8e1fc547c9a11cbd1b2e42c8fb",
      "678958f75e304a56862a118747649cfd",
      "2869282767294b12ab2b58cff1696790",
      "4c5b18b7a15b452187507c4770ddf1a1",
      "d1a05424979c401393052bb0ed283ee3",
      "c0cb726b53ae44609ad5635d72a61afc",
      "af5d4c3832d942d7a53c4c90d6499427",
      "02b88d3ef1634b42ba22f5fdd26c22ea",
      "50a5edeb392640598c989ee6816a4851",
      "691c11cea26e4e99a0423e1995a33a45",
      "11365199c6e84f9fa5c6a41ca578ae09",
      "cd4f9455278a4127b9e02264b30692f4",
      "439dcc84a7e144829d4dd08652f39064",
      "3f3eb7bb3d0c453ba236b4637bab7151",
      "c1abfde1598c401a84f64143160da234",
      "2e21cc49bc2642aa94ebe3383c3be64c",
      "1ef3771ef53d4f16a63ebac1f126e080",
      "12ba43b248194eeba9305b266dcef8c5",
      "363319d1a523486db33bb54950a07484",
      "852f62f4c30a400285c192be2120dca5",
      "6f023b17c105458b877230a845ee6480",
      "da8489b6bf154ceaadb560c2efb39df7",
      "42be4ded24164525808faf219a52f8d3",
      "966caaa38732418385592dcfdec2521f",
      "24bdb3c2a4264de2819601a920960bd2",
      "9b8cc0c245ab437da53da6ff7959aebe",
      "6c3ec74ec19f414cba2e400e74f52ff0",
      "04a8325ced3048dbbcc6d8a90c782020",
      "dcc730d91a9042928759592ba413bc09",
      "0cb438a405b14b2f9ef1462166ecad85",
      "a2829d12e64947028507884543d6a3b3",
      "519d84e0f5844ca790927926d1b4fe6a",
      "03384a8a596b47a792045d11ded29a37",
      "1805a770a5694118bd51961e0f2ea529",
      "ef503822a3db498bae24d2b6535ca3a4",
      "92108b2ebc004dd0a0f3e9bc166f6199",
      "cfa895aee56945c285a9fed76dac0607",
      "ae47d07209c24cb49bf51c5229e8fda9",
      "56ae615553744fa083e223d5ee43a1a0",
      "6a7c4ee6a78b42f9926b47ece7a9b864",
      "c9f36022cacd4721949870be02f3ea19",
      "a5e7b8e7028a4ef4b84bf67676d17ff6",
      "a56b667f9984479fadd88fca0d34ef89",
      "47b5a55625d9466d81d0e4b91b97ff37",
      "7900f5394f574571bef46b3adbf22d45",
      "898ae45d4dcf4d4ba5cda54772e80249",
      "b0db527d3f9e4a80a029ea4889ad9175",
      "2caf05c2baea4c36bcdbaa539994a4ad",
      "89ffd2cffd2e4cbe86509651e83748c0",
      "83787bf54c5e4deabbdf9ca44a78f550",
      "000471e07d034e3ea241055fbf56126f",
      "cde8782b541a49e8af4ee371e36461db",
      "fce0e1f94b4146e19d3073335ed055f8",
      "2c903b6373114915988126064b24c7ab",
      "3a03fc0e29544e718341d62371b81469",
      "c0ba455a1ca54fcb8d0280e77187f740",
      "4b4c1e46c66444f6916d829810eb8c1e",
      "85239fbb4cf946e391658e7be48f3735",
      "2ae16e2aa800423790e537b3293f3e14",
      "6b70361eccfa4248a5f8e07d6a034b17",
      "ea56b759ac3e484a8384610ec1bc0c4e",
      "f9f01fc2964e40538871012c78dfd118",
      "550f83e345f9446985a62151c992ddcd",
      "c82d5b539d9f4c2999d6b929af1c2ccd",
      "effa7de1f8cd46819d74870bf1f9887d",
      "4937047cd73b4f83814539cedf061046",
      "fa1cdadb390f464ca53552ddc7fbd727",
      "bc8b515ed0c34b75ba6e06311453df29",
      "2e0d0001e5d74996b147d57224c4e485",
      "49f1c0c2e33a4719be4574c269a78b2d",
      "548ef9287d734077a3e7864f6636be27",
      "103d404eb291428ea9babd4bf8e64ff9"
     ]
    },
    "id": "5phRS_z2U_3T",
    "outputId": "60eb226a-ac5e-44a0-8946-8498913a37bc"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "97950f51209a426393b8627508ee201e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "train.parquet:   0%|          | 0.00/699k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "af5d4c3832d942d7a53c4c90d6499427"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "validation.parquet:   0%|          | 0.00/90.0k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "12ba43b248194eeba9305b266dcef8c5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "test.parquet:   0%|          | 0.00/92.2k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dcc730d91a9042928759592ba413bc09"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6a7c4ee6a78b42f9926b47ece7a9b864"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating validation split:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "000471e07d034e3ea241055fbf56126f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating test split:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f9f01fc2964e40538871012c78dfd118"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 8530\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# Load our data\n",
    "data = load_dataset(\"rotten_tomatoes\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXl6LASB39yw"
   },
   "source": [
    "### Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "X0KyKHtqyjn3"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def evaluate_performance(y_true, y_pred):\n",
    "    \"\"\"Create and print the classification report\"\"\"\n",
    "    performance = classification_report(\n",
    "        y_true, y_pred,\n",
    "        target_names=[\"Negative Review\", \"Positive Review\"]\n",
    "    )\n",
    "    print(performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Challenges\n",
    "\n",
    "Complete the following tasks by implementing the starter code.\n",
    "\n",
    "Solutions are in `00_Solutions.ipynb`."
   ],
   "metadata": {
    "id": "NKYNfoaVC4hU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Level: Easy\n",
    "\n",
    "These challenges introduce core concepts. Implement the Try sections to practice."
   ],
   "metadata": {
    "id": "gwqUbziIDNee"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qD982uIW39y-"
   },
   "source": [
    "**About This Task:**\n",
    "Zero-shot classification classifies text without training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Easy Task 1: Zero-Shot Classifier\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Execute the code to see baseline predictions for 3 basic reviews\n",
    "2. Uncomment the larger `test_reviews` list and run again to test harder cases\n",
    "3. Uncomment one label option to see how wording affects predictions\n",
    "4. Compare which label style works best for ambiguous reviews"
   ],
   "metadata": {
    "id": "TnHfnLsxDQcA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ],
   "metadata": {
    "id": "ZC6N2A3LC6JZ"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"
   ],
   "metadata": {
    "id": "_81W_w1d7Wc3",
    "outputId": "73093b8d-f8f9-45c3-9913-e21ca6d714ad",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369,
     "referenced_widgets": [
      "1bd919805cc9484199504ddd6b8bce59",
      "853c981e1eed473db5152815192cc145",
      "d2e774a79b544009b0d34f7611f3c5d5",
      "30ac4cf4065f41d8a692867d9eb27670",
      "bf89cb858e5d44e596ae3adde915fbcd",
      "2bb1b665caa44a2c8cca35835d87d652",
      "8c1e969268964946a240b48c0befa829",
      "9d3c3b9cc6b94521a29f04dd05e03699",
      "7c2772330e22476886ae8145a9d5d623",
      "bd9aa4ea98234954bbaf635b2a851bd3",
      "3a82f0e83083464e808004c6f3a9f12c",
      "6813c3a9dfa042dcb18847268b11d161",
      "1c3ab6a6522a448db46df090c79b7cb3",
      "6f2b8ddc3fa04bbbba0dfb8f457ca595",
      "2a4aa11b8813404f86354525f321e827",
      "5089dd0313764ecf9ad0085105e5c4a1",
      "6250e656eb1a4d21997619f24f53c3c8",
      "61f811262320474b8b49a3b9423e0ba5",
      "dc0c274dafb84014a2fd40cf8d8a5fe7",
      "bb41961c0fd74fa79553fe47a1e5944d",
      "9080826acc6e41fdbf358d532ae12bd5",
      "772cb5fc9be349c59626631f887d7568",
      "53a26345a25d4e96b6b71a784fcac980",
      "c0a01a47cef249e887d10b066bc21ee2",
      "0980cd7d1cb54f5e8b29ec70d5b38ffc",
      "e59e25ac852c4362ac84807362ec7c86",
      "dd265b36f9044043a2074e04dec27604",
      "2c4292ac83fb4d49b264c4306d2160ac",
      "c474f43ec810482fa86bd43add26faf3",
      "855029a4570f441e99f2a019e032e22c",
      "05242ef5b53441ee92ddf370d53593a7",
      "e293f63b0ce54520a2caab682352f0ad",
      "8888e46b31fe48458ec2256ff3b42d80",
      "5bb6ac929f2e4ae8a5fee8419be18a90",
      "23f5bb34e6a7463e8bb999f939b79c87",
      "b7cc5d02f477452f90aa1503e729b995",
      "eb8f8995407c4591ba53380cec6f5e9d",
      "12488553142b45a2b346b127f9009ff2",
      "aa994f9b47814f0bb45be46233fbe639",
      "b84f46cad5ba43a899968109bfd01a4a",
      "97fa92855aa4483e803a38435a395333",
      "f2656f3e2fcf47368e31e6378e66e5b8",
      "5540c663b4fa4e80a8b8598e77be5812",
      "2efe3bcb7b574feea6eebb38ffe53a6f",
      "2110c75e41f841cebbbff185ab966da2",
      "62bec97f78c64fd7a217cb74e5f0209a",
      "f7a1478382834ff88d214158e601a1fd",
      "59635c21028e4cf68ec0c44654b98b09",
      "02bb8b2c40ad48f3b473f15b2c090517",
      "ec93eae9334540ceb24bcd34f0412fbe",
      "dd4bc814aad74a52a7ab6bc2a4741477",
      "460129f37a544338ad476bc788774e0d",
      "6a30f7639db143bbb553f8dd1717bbe4",
      "37c6a84ebaf04f729236ac0f1b4c12df",
      "7e09c8d0e3c447519f219e0831a1b9df",
      "b2cfd2e8ca8f4541bf72b0edfe9a7e67",
      "83ee947dcd3b4e8992f0651a0a6ec4f4",
      "7f1ec4cec8aa4073a590632e69d9881c",
      "e14e57b676584c2c99e05e9f0f261082",
      "2e0549404bc64082a91f635b86348906",
      "50d17c42962f4788872378f995284798",
      "7f1aa1624f554197becfd073c8de6085",
      "3e4553f880d444cf958b8b51391228b5",
      "b43b7ac14c6649218b11931a80c4af7c",
      "96d1f04c5de342709b6b7511bb5a9b9c",
      "d5b6caa9dc9446f4b7b0441337143aad",
      "288e2a829a8d4e5890b4759cd1098969",
      "ac54c8480d0a41f0aded058166cec709",
      "b8be56edd2a64295adb8b09de118a773",
      "ae0080afda854b859863a99381cf1037",
      "d16ec442b9934148b9b2d82bec22ff22",
      "e82e6140e6c140b68ba5079bdef3d3f4",
      "5cf078ebd0c74e68a269136601851648",
      "b3030d669c434c90b3033cf9e4af8295",
      "db271208adf6469d8840dc636a5eaca0",
      "a93a01c67b6c4b1fb3a5fb1f1c9e35b5",
      "569e76de31044233b00f6552d60a5257",
      "29b435da95844ea2b68df24b754e2d50",
      "649c983150da4100ac6f3b32fd035012",
      "d9839e97481946cead103085966d3c6c",
      "2f51abb6190742729dd0c30f49a54b65",
      "c3dd1d97ed344bd98794ca27d0e66385",
      "9ccd666e565748e790e5d7ae5b69dc59",
      "0e54b539e06d4c02a7bc507901d3c15b",
      "3857b75c16cf464aa447f5691a8ec0d2",
      "81307816b5d64a29bf0afb8f112ea36d",
      "d2a0f21992dc4358a0f4956ee40469d0",
      "af0a9e0af31f42d193ad48fa4d889ad1",
      "e6b4f500b0194990a70fd14d9c83c626",
      "9efe2195ec524475b5a003cfa70865c9",
      "173b54b4a7ac42f99c7542cae18f4823",
      "43a0baba22f84e8cb994e49c6882f9cd",
      "54f1d69f6c8240eca8de3f92a5f0e51b",
      "bc71ddbdbf6c43e4a12b9b8d73c9eef3",
      "3336f5e8cede42dba8487989e47a3429",
      "69cc59921ef341b09604a9b9cd05da5c",
      "60e10d52c5b34d9c8dc0069f212213c4",
      "52763e5a8b324609b5346f11f3bf956b",
      "f07d4df724cb4fd1aa1e9161ca355f9f",
      "88b4a9db3de2473abfe983183d166632",
      "f3c8c909bed04e6d9241bfbe490c1eac",
      "fdb97bf8457d4103940a824074971c13",
      "9af301a275ce4e1bbafa03877b6f94a5",
      "a6b30a9dd53d49beb0c9a8b5aa65a1ab",
      "4d91c01c2e7b44e2b12dfbbdb2298756",
      "32eaf7d1ed0147dfac31f0f42b85e14d",
      "135eb43a810648f3aa2c9628a8370563",
      "b2a3e74c78804664a955b479d075679e",
      "5c9475bea98f4abdb6a732354a6f35d4",
      "20df6a3df9bc445a862b9383d512254f",
      "0cf608d09eb5497580a7d54b6b3255fe",
      "3812c8efd83d47e29a9b36e70bb61ed1",
      "d3a90a060437478b9c9ea25c15297136",
      "a21597e2c7cf40898375991c7e3f6a15",
      "db189f18c6d941b48e8e84a5dca47b39",
      "8edb9c7434024c15a6bfb7774d447c98",
      "910acc6e3a99456a97101eb4b97ca90b",
      "bb2b50a5ddbb4dcd988deea48cf9abab",
      "c13c2d8a10794aa0bc2d44afeafc0d7f",
      "247830841d784bfb9289bdaa07858380",
      "f99e1fe28bc04a769618e3bddd5da5d7"
     ]
    }
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1bd919805cc9484199504ddd6b8bce59"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6813c3a9dfa042dcb18847268b11d161"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "53a26345a25d4e96b6b71a784fcac980"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5bb6ac929f2e4ae8a5fee8419be18a90"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2110c75e41f841cebbbff185ab966da2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b2cfd2e8ca8f4541bf72b0edfe9a7e67"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "288e2a829a8d4e5890b4759cd1098969"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "29b435da95844ea2b68df24b754e2d50"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e6b4f500b0194990a70fd14d9c83c626"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "88b4a9db3de2473abfe983183d166632"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0cf608d09eb5497580a7d54b6b3255fe"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Test reviews - These work as-is\n",
    "test_reviews = [\n",
    "    \"This movie was absolutely fantastic! A masterpiece!\",\n",
    "    \"Terrible waste of time. Very disappointing.\",\n",
    "    \"An okay film, nothing special but watchable.\",\n",
    "]\n",
    "# Your task: Uncomment to test harder cases\n",
    "# test_reviews = [\n",
    "#     \"This movie was absolutely fantastic! A masterpiece!\",\n",
    "#     \"Terrible waste of time. Very disappointing.\",\n",
    "#     \"An okay film, nothing special but watchable.\",\n",
    "#     \"Oh great, another masterpiece... not!\",  # Sarcastic\n",
    "#     \"Boring.\",  # Very short\n",
    "#     \"Great acting but terrible plot.\",  # Mixed sentiment\n",
    "# ]"
   ],
   "metadata": {
    "id": "8d4ZihTT7Wc4"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "f5aFZ79f7Wc5"
   },
   "source": [
    "# Label descriptions - These work as-is\n",
    "labels = [\n",
    "    \"A negative movie review\",\n",
    "    \"A positive movie review\"\n",
    "]\n",
    "# Implement: Try different label options\n",
    "# labels = [\"negative\", \"positive\"]  # Option 1: Simple\n",
    "# labels = [\"bad movie review\", \"good movie review\"]  # Option 2: Different wording\n",
    "# labels = [\"a scathing negative movie review\", \"an enthusiastic positive movie review\"]  # Option 3: Detailed"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dYlHWbyP7Wc7"
   },
   "source": [
    "# Complete this: Encode the labels and reviews, then calculate cosine similarity\n",
    "# Hint: Use model.encode() for embeddings and cosine_similarity() for the matrix\n",
    "label_embeddings = model.encode(labels)\n",
    "review_embeddings = model.encode(test_reviews)\n",
    "sim_matrix = cosine_similarity(review_embeddings, label_embeddings)"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the above code cell, we simply load and use the model on the test reviews. You are encouraged to run the above cell with the different labels option to see the difference similarity."
   ],
   "metadata": {
    "id": "B-NX5qe-53ph"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Fill in: For each review, find the predicted label and print results\n# Hint: Use np.argmax() to find the highest similarity score\n\nprint(\"Classification Results:\")\n\nfor i, review in enumerate(test_reviews):\n    # prediction: which label (0 or 1) has highest similarity\n    prediction = np.argmax(sim_matrix[i])\n    \n    # confidence: how similar is the review to predicted label (0.0 to 1.0)\n    # Higher confidence = model is more sure about its prediction\n    confidence = sim_matrix[i][prediction]\n    \n    # margin: difference between top 2 predictions (how clear-cut the decision is)\n    # Large margin (>0.15) = confident, small margin (<0.05) = uncertain/ambiguous\n    top_two = sorted(sim_matrix[i], reverse=True)[:2]\n    margin = top_two[0] - top_two[1]\n    \n    print(f\"\\nReview {i+1}: '{review}'\")\n    print(f\"Predicted: {labels[prediction]}\")\n    print(f\"Confidence: {confidence:.3f}\")\n    print(f\"Margin: {margin:.3f}\")"
   ],
   "metadata": {
    "id": "DeRNmhHE513y",
    "outputId": "607d2509-9c8e-4be2-ed78-cc9202c458f7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classification Results:\n",
      "\n",
      "Review 1: 'This movie was absolutely fantastic! A masterpiece!'\n",
      "Predicted: A positive movie review\n",
      "Confidence: 0.493\n",
      "Margin: 0.111\n",
      "\n",
      "Review 2: 'Terrible waste of time. Very disappointing.'\n",
      "Predicted: A negative movie review\n",
      "Confidence: 0.439\n",
      "Margin: 0.140\n",
      "\n",
      "Review 3: 'An okay film, nothing special but watchable.'\n",
      "Predicted: A positive movie review\n",
      "Confidence: 0.542\n",
      "Margin: 0.039\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "To test other `test_reviews` or `labels`, comment the previous one. Make sure only one of the sets is uncommented.\n",
    "\n",
    "Tip: to uncomment/comment lines, use `Ctrl + /`"
   ],
   "metadata": {
    "id": "NOucKIpW6kU1"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "### Questions\n",
    "\n",
    "1. Why did the classifier fail on the sarcastic review (\"*Oh great, another masterpiece... NOT*\")? What semantic features did embeddings miss?\n",
    "\n",
    "2. Which reviews changed predictions when you modified label descriptions? Why are some reviews more sensitive to label wording than others?\n",
    "\n",
    "3. Which reviews have low confidence margins (<0.1)? What linguistic features make certain reviews harder to classify?\n",
    "\n"
   ],
   "metadata": {
    "id": "hAfvoaoTDidJ"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1V2I1IWn39zB"
   },
   "source": [
    "**About This Task:**\n",
    "Different classification strategies affect model behavior and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Easy Task 2: Classifier Strategy Analysis\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Execute code to see three pre-built classifiers (conservative, aggressive, balanced)\n",
    "2. Study each confusion matrix to identify error patterns\n",
    "3. Modify `classifier_yours` to create a very conservative classifier (precision > 0.9)\n",
    "4. Uncomment the Try section to analyze your classifier\n",
    "5. Experiment with creating different strategy combinations"
   ],
   "metadata": {
    "id": "Rhuy6BY6DxDr"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hq2Klrbc7WdA"
   },
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "import numpy as np"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6rp-T8r17WdB"
   },
   "source": [
    "y_true = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PsisRIOY7WdD"
   },
   "source": [
    "classifier_conservative = np.array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1])  # Rarely predicts positive\n",
    "classifier_aggressive = np.array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1])     # Often predicts positive\n",
    "classifier_balanced = np.array([0, 0, 0, 1, 0, 0, 1, 1, 1, 1])       # Balanced approach"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5W09dFnS7WdE"
   },
   "source": [
    "# Your task: Modify these predictions to make your classifier\n",
    "# Try to achieve precision > 0.9 (be very selective about predicting 1)\n",
    "classifier_yours = np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1])"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VGiWgoFw7WdE"
   },
   "source": [
    "def analyze_classifier(name, y_true, y_pred):\n",
    "    \"\"\"Analyze classifier performance with detailed breakdown\"\"\"\n",
    "    print(f\"\\n{name}\")\n",
    "    # Calculate the confusion matrix\n",
    "    # Hint: Use confusion_matrix(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"                    Predicted Neg | Predicted Pos\")\n",
    "    print(f\"Actual Neg (0):          {cm[0][0]}       |       {cm[0][1]}  (False Positives)\")\n",
    "    print(f\"Actual Pos (1):          {cm[1][0]}       |       {cm[1][1]}  (True Positives)\")\n",
    "    print(f\"                   (False Negatives)\")\n",
    "\n",
    "    # Calculate precision, recall, and f1\n",
    "    # Hint: Use precision_score, recall_score, f1_score from sklearn\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    if precision is not None and recall is not None and f1 is not None:\n",
    "        print(f\"\\nMetrics:\")\n",
    "        print(f\"Precision: {precision:.3f} = TP/(TP+FP) = {cm[1][1]}/({cm[1][1]}+{cm[0][1]})\")\n",
    "        print(f\"Recall:    {recall:.3f} = TP/(TP+FN) = {cm[1][1]}/({cm[1][1]}+{cm[1][0]})\")\n",
    "        print(f\"F1 Score:  {f1:.3f} = 2*(P*R)/(P+R)\")\n",
    "        # Explain strategy\n",
    "        if precision > recall + 0.1:\n",
    "            print(f\"\\nStrategy: Conservative (few false alarms, misses some positives)\")\n",
    "        elif recall > precision + 0.1:\n",
    "            print(f\"\\nStrategy: Aggressive (finds most positives, many false alarms)\")\n",
    "        else:\n",
    "            print(f\"\\nStrategy: Balanced\")\n",
    "    return precision, recall, f1"
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jhx0wVzS7WdG",
    "outputId": "ef897520-b4cd-4abc-cb79-90df1868f247",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "results = {}\n",
    "for name, classifier in [\n",
    "    (\"Conservative Classifier\", classifier_conservative),\n",
    "    (\"Aggressive Classifier\", classifier_aggressive),\n",
    "    (\"Balanced Classifier\", classifier_balanced),\n",
    "]:\n",
    "    p, r, f = analyze_classifier(name, y_true, classifier)\n",
    "    results[name] = (p, r, f)"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Conservative Classifier\n",
      "\n",
      "Confusion Matrix:\n",
      "                    Predicted Neg | Predicted Pos\n",
      "Actual Neg (0):          5       |       0  (False Positives)\n",
      "Actual Pos (1):          2       |       3  (True Positives)\n",
      "                   (False Negatives)\n",
      "\n",
      "Metrics:\n",
      "Precision: 1.000 = TP/(TP+FP) = 3/(3+0)\n",
      "Recall:    0.600 = TP/(TP+FN) = 3/(3+2)\n",
      "F1 Score:  0.750 = 2*(P*R)/(P+R)\n",
      "\n",
      "Strategy: Conservative (few false alarms, misses some positives)\n",
      "\n",
      "Aggressive Classifier\n",
      "\n",
      "Confusion Matrix:\n",
      "                    Predicted Neg | Predicted Pos\n",
      "Actual Neg (0):          1       |       4  (False Positives)\n",
      "Actual Pos (1):          0       |       5  (True Positives)\n",
      "                   (False Negatives)\n",
      "\n",
      "Metrics:\n",
      "Precision: 0.556 = TP/(TP+FP) = 5/(5+4)\n",
      "Recall:    1.000 = TP/(TP+FN) = 5/(5+0)\n",
      "F1 Score:  0.714 = 2*(P*R)/(P+R)\n",
      "\n",
      "Strategy: Aggressive (finds most positives, many false alarms)\n",
      "\n",
      "Balanced Classifier\n",
      "\n",
      "Confusion Matrix:\n",
      "                    Predicted Neg | Predicted Pos\n",
      "Actual Neg (0):          4       |       1  (False Positives)\n",
      "Actual Pos (1):          1       |       4  (True Positives)\n",
      "                   (False Negatives)\n",
      "\n",
      "Metrics:\n",
      "Precision: 0.800 = TP/(TP+FP) = 4/(4+1)\n",
      "Recall:    0.800 = TP/(TP+FN) = 4/(4+1)\n",
      "F1 Score:  0.800 = 2*(P*R)/(P+R)\n",
      "\n",
      "Strategy: Balanced\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LnHUyo877WdH"
   },
   "source": [
    "# Analyze your classifier\n",
    "# # print(\"Analyzing Your Classifier\")\n",
    "# # p, r, f = analyze_classifier(\"YOUR Classifier\", y_true, classifier_yours)\n",
    "# results[\"your Classifier\"] = (p, r, f)"
   ],
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LfgyvCyN7WdI",
    "outputId": "a72c9068-cc75-4dc9-be25-c96e4095e9af",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "print(f\"{'Classifier':<25} {'Precision':<12} {'Recall':<12} {'F1':<12}\")\n",
    "for name, (p, r, f) in results.items():\n",
    "    print(f\"{name:<25} {p:.3f}        {r:.3f}       {f:.3f}\")"
   ],
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classifier                Precision    Recall       F1          \n",
      "Conservative Classifier   1.000        0.600       0.750\n",
      "Aggressive Classifier     0.556        1.000       0.714\n",
      "Balanced Classifier       0.800        0.800       0.800\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Questions\n",
    "\n",
    "1. The conservative classifier has 2 false negatives. What real-world mistake does this represent? Provide a movie review example.\n",
    "\n",
    "2. What strategy did you use to achieve high precision in `classifier_yours`? Why does predicting positive less frequently increase precision?\n",
    "\n",
    "3. Which classifier won on F1 score? Why doesn't the aggressive classifier win despite high recall?"
   ],
   "metadata": {
    "id": "GHv-B_ebD8CU"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUmXASWa39zF"
   },
   "source": [
    "**About This Task:**\n",
    "Temperature controls randomness in language model outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Easy Task 3: Temperature Effects on Text Generation\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Execute code to see how temperature affects token selection with a confident model\n",
    "2. Observe how probabilities and samples change across temperatures\n",
    "3. Uncomment the uncertain probability distribution and run again\n",
    "4. Compare temperature effects on confident vs uncertain models\n",
    "5. Uncomment Try to add a new temperature value and analyze results"
   ],
   "metadata": {
    "id": "YkEaGnlqEEWU"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VEUCj_0a7WdM"
   },
   "source": [
    "import numpy as np"
   ],
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GwZZIm5B7WdO"
   },
   "source": [
    "original_probs = np.array([0.50, 0.30, 0.12, 0.05, 0.03])\n",
    "tokens = [\"positive\", \"negative\", \"neutral\", \"good\", \"bad\"]\n",
    "\n",
    "# Try uncertain distribution\n",
    "# original_probs = np.array([0.25, 0.24, 0.22, 0.18, 0.11]), much more balanced\n",
    "# Run again and compare the temperature effects"
   ],
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Uj98unj67WdO"
   },
   "source": [
    "def apply_temperature(probs, temperature):\n",
    "    \"\"\"Apply temperature scaling to change distribution sharpness\"\"\"\n",
    "    if temperature == 0:\n",
    "        # For temperature=0, return deterministic distribution\n",
    "        result = np.zeros_like(probs)\n",
    "        result[np.argmax(probs)] = 1.0\n",
    "        return result\n",
    "\n",
    "    # Apply temperature scaling\n",
    "    # Steps: log -> divide by temp -> exp -> normalize\n",
    "    logits = np.log(probs + 1e-10)  # Add small value to avoid log(0)\n",
    "    scaled_logits = logits / temperature  # Scale by temperature\n",
    "    exp_logits = np.exp(scaled_logits)  # Exponentiate\n",
    "    normalized = exp_logits / np.sum(exp_logits)  # Normalize to sum to 1.0\n",
    "    return normalized"
   ],
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Q3EKkaa37WdP"
   },
   "source": [
    "def visualize_distribution(probs, tokens):\n",
    "    \"\"\"Show probability distribution as bar chart\"\"\"\n",
    "    for i, token in enumerate(tokens):\n",
    "        bar_length = int(probs[i] * 100)\n",
    "        bar = '' * bar_length\n",
    "        print(f\"  {token:10s}: {probs[i]:.3f} {bar}\")"
   ],
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cRgtzbGm7WdQ"
   },
   "source": [
    "# Test different temperatures\n",
    "temperatures = [0, 0.5, 1.0, 2.0]\n",
    "# Add temperature=3.0\n",
    "# temperatures = [0, 0.5, 1.0, 2.0, 3.0]"
   ],
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GvnXZI2T7WdR",
    "outputId": "4a937da1-f1a8-49f6-dfe0-4462d23c1dfd",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "print(f\"Original (temperature=1.0) probabilities:\")\n",
    "visualize_distribution(original_probs, tokens)"
   ],
   "execution_count": 23,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original (temperature=1.0) probabilities:\n",
      "  positive  : 0.500 \n",
      "  negative  : 0.300 \n",
      "  neutral   : 0.120 \n",
      "  good      : 0.050 \n",
      "  bad       : 0.030 \n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PHqoPBHi7WdS",
    "outputId": "dfbb372c-d29b-4c57-e7dc-11bcaa579bf7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "for temp in temperatures:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Temperature = {temp}\")\n",
    "    print('='*70)\n",
    "\n",
    "    # Apply temperature\n",
    "    new_probs = apply_temperature(original_probs, temp)\n",
    "    # Visualize\n",
    "    visualize_distribution(new_probs, tokens)\n",
    "    # Sample tokens\n",
    "    print(f\"\\n  Sampling 10 tokens:\")\n",
    "    if temp == 0:\n",
    "        samples = [tokens[np.argmax(new_probs)]] * 10\n",
    "    else:\n",
    "        samples = np.random.choice(tokens, size=10, p=new_probs)\n",
    "    print(f\"  {samples}\")\n",
    "    # Show diversity metric\n",
    "    unique_tokens = len(set(samples))\n",
    "    print(f\"   Diversity: {unique_tokens}/10 unique tokens\")\n",
    "    # Explain what's happening\n",
    "    if temp == 0:\n",
    "        print(f\"   Effect: Deterministic - always outputs '{samples[0]}'\")\n",
    "    elif temp < 1.0:\n",
    "        print(f\"   Effect: Sharpened - makes confident tokens more likely\")\n",
    "    elif temp == 1.0:\n",
    "        print(f\"   Effect: Unchanged - original distribution\")\n",
    "    else:\n",
    "        print(f\"   Effect: Flattened - makes all tokens more equally likely\")"
   ],
   "execution_count": 24,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "======================================================================\n",
      "Temperature = 0\n",
      "======================================================================\n",
      "  positive  : 1.000 \n",
      "  negative  : 0.000 \n",
      "  neutral   : 0.000 \n",
      "  good      : 0.000 \n",
      "  bad       : 0.000 \n",
      "\n",
      "  Sampling 10 tokens:\n",
      "  ['positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive']\n",
      "   Diversity: 1/10 unique tokens\n",
      "   Effect: Deterministic - always outputs 'positive'\n",
      "\n",
      "======================================================================\n",
      "Temperature = 0.5\n",
      "======================================================================\n",
      "  positive  : 0.699 \n",
      "  negative  : 0.252 \n",
      "  neutral   : 0.040 \n",
      "  good      : 0.007 \n",
      "  bad       : 0.003 \n",
      "\n",
      "  Sampling 10 tokens:\n",
      "  ['positive' 'positive' 'negative' 'positive' 'positive' 'positive'\n",
      " 'negative' 'negative' 'positive' 'positive']\n",
      "   Diversity: 2/10 unique tokens\n",
      "   Effect: Sharpened - makes confident tokens more likely\n",
      "\n",
      "======================================================================\n",
      "Temperature = 1.0\n",
      "======================================================================\n",
      "  positive  : 0.500 \n",
      "  negative  : 0.300 \n",
      "  neutral   : 0.120 \n",
      "  good      : 0.050 \n",
      "  bad       : 0.030 \n",
      "\n",
      "  Sampling 10 tokens:\n",
      "  ['good' 'positive' 'negative' 'negative' 'positive' 'neutral' 'positive'\n",
      " 'positive' 'positive' 'positive']\n",
      "   Diversity: 4/10 unique tokens\n",
      "   Effect: Unchanged - original distribution\n",
      "\n",
      "======================================================================\n",
      "Temperature = 2.0\n",
      "======================================================================\n",
      "  positive  : 0.354 \n",
      "  negative  : 0.274 \n",
      "  neutral   : 0.173 \n",
      "  good      : 0.112 \n",
      "  bad       : 0.087 \n",
      "\n",
      "  Sampling 10 tokens:\n",
      "  ['neutral' 'bad' 'bad' 'negative' 'positive' 'negative' 'good' 'positive'\n",
      " 'neutral' 'positive']\n",
      "   Diversity: 5/10 unique tokens\n",
      "   Effect: Flattened - makes all tokens more equally likely\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Questions\n",
    "\n",
    "1. Why is temperature=0 critical for classification tasks? What would go wrong with temperature=1.0?\n",
    "\n",
    "2. Compare temperature=0.5 vs 2.0. At what temperature did low-probability tokens like \"bad\" start appearing in samples?\n",
    "\n",
    "3. With the uncertain distribution ([0.25, 0.24, 0.22, 0.18, 0.11]), how did temperature effects differ from the confident model?"
   ],
   "metadata": {
    "id": "pA4FHgvIENtG"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZDavrTL39zJ"
   },
   "source": [
    "**About This Task:**\n",
    "Embedding similarity measures how semantically close two texts are in vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Easy Task 4: Embedding Similarity Analysis\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Execute code to see similarity matrix for movie reviews\n",
    "2. Identify which reviews cluster together and which are distant\n",
    "3. Uncomment Try to add reviews from different domains\n",
    "4. Analyze whether restaurant/product reviews cluster with movie reviews\n",
    "5. Add a random sentence to test similarity boundaries"
   ],
   "metadata": {
    "id": "Y2tgqir4Ecu0"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TcCuGi6N7WdU"
   },
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ],
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EQP5idM27WdW"
   },
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"
   ],
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZpR2GweA7WdW"
   },
   "source": [
    "# Movie reviews - different types\n",
    "texts = [\n",
    "    # Positive reviews\n",
    "    \"Amazing movie! Absolutely loved it!\",\n",
    "    \"Fantastic film, highly recommend!\",\n",
    "    \"Great cinematography and acting\",\n",
    "\n",
    "    # Negative reviews\n",
    "    \"Terrible waste of time\",\n",
    "    \"Very disappointing and boring\",\n",
    "    \"Poor acting and weak plot\",\n",
    "\n",
    "    # Neutral reviews\n",
    "    \"It was okay, nothing special\",\n",
    "    \"Some good parts, some bad parts\",\n",
    "\n",
    "    # Off-topic\n",
    "    \"The weather is nice today\",\n",
    "    \"I like eating pizza\"\n",
    "]"
   ],
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Implement: Test domain transfer\n",
    "# texts = [\n",
    "#     # Positive movie reviews\n",
    "#     \"Amazing movie! Absolutely loved it!\",\n",
    "#     \"Fantastic film, highly recommend!\",\n",
    "#     \"Great cinematography and acting\",\n",
    "#\n",
    "#     # Negative movie reviews\n",
    "#     \"Terrible waste of time\",\n",
    "#     \"Very disappointing and boring\",\n",
    "#     \"Poor acting and weak plot\",\n",
    "#\n",
    "#     # Neutral movie reviews\n",
    "#     \"It was okay, nothing special\",\n",
    "#     \"Some good parts, some bad parts\",\n",
    "#\n",
    "#     # Positive restaurant review (different domain!)\n",
    "#     \"Amazing food! Absolutely loved it!\",\n",
    "#     \"Fantastic restaurant, highly recommend!\",\n",
    "#\n",
    "#     # Off-topic\n",
    "#     \"The weather is nice today\",\n",
    "#     \"I like eating pizza\",\n",
    "# ]"
   ],
   "metadata": {
    "id": "VQXM_eggeUas"
   },
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZJTM2D7Y7WdX"
   },
   "source": [
    "labels = [f\"Text {i+1}\" for i in range(len(texts))]"
   ],
   "execution_count": 29,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "P_KegI7i7Wda"
   },
   "source": [
    "# Generate embeddings\n",
    "embeddings = model.encode(texts)\n",
    "similarity_matrix = cosine_similarity(embeddings)"
   ],
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RcM9HaB_7Wdb",
    "outputId": "72d4b99a-8f19-4345-e9b9-2300d754d814",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "print(f\"Each text converted to {embeddings.shape[1]}-dimensional vector\")\n",
    "print(f\"Comparing {embeddings.shape[0]} texts\\n\")"
   ],
   "execution_count": 31,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Each text converted to 768-dimensional vector\n",
      "Comparing 10 texts\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "A0YJHXQe7Wdc",
    "outputId": "1719ea3b-586f-4191-f68f-a24d122ee722",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "# Show full similarity matrix\n",
    "print(\"Similarity Matrix (0=unrelated, 1=identical):\")\n",
    "print(f\"{'':10s}\", end=\"\")\n",
    "for i in range(len(texts)):\n",
    "    print(f\"T{i+1:2d} \", end=\"\")\n",
    "print()\n",
    "for i in range(len(texts)):\n",
    "    print(f\"Text {i+1:2d}:  \", end=\"\")\n",
    "    for j in range(len(texts)):\n",
    "        if i == j:\n",
    "            print(\"---- \", end=\"\")\n",
    "        else:\n",
    "            sim = similarity_matrix[i][j]\n",
    "            if sim > 0.6:\n",
    "                print(f\"{sim:.2f}*\", end=\"\")  # High similarity\n",
    "            else:\n",
    "                print(f\"{sim:.2f} \", end=\"\")\n",
    "            print(\" \", end=\"\")\n",
    "    print()\n",
    "    print(\"\\n* = High similarity (>0.6)\")"
   ],
   "execution_count": 32,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Similarity Matrix (0=unrelated, 1=identical):\n",
      "          T 1 T 2 T 3 T 4 T 5 T 6 T 7 T 8 T 9 T10 \n",
      "Text  1:  ---- 0.77* 0.52  0.09  0.24  0.23  0.31  0.16  0.10  0.03  \n",
      "\n",
      "* = High similarity (>0.6)\n",
      "Text  2:  0.77* ---- 0.47  0.11  0.26  0.23  0.21  0.14  0.09  0.01  \n",
      "\n",
      "* = High similarity (>0.6)\n",
      "Text  3:  0.52  0.47  ---- 0.10  0.32  0.51  0.36  0.26  0.12  0.06  \n",
      "\n",
      "* = High similarity (>0.6)\n",
      "Text  4:  0.09  0.11  0.10  ---- 0.55  0.29  0.34  0.21  0.03  0.07  \n",
      "\n",
      "* = High similarity (>0.6)\n",
      "Text  5:  0.24  0.26  0.32  0.55  ---- 0.50  0.55  0.36  0.09  0.13  \n",
      "\n",
      "* = High similarity (>0.6)\n",
      "Text  6:  0.23  0.23  0.51  0.29  0.50  ---- 0.37  0.25  -0.03  0.08  \n",
      "\n",
      "* = High similarity (>0.6)\n",
      "Text  7:  0.31  0.21  0.36  0.34  0.55  0.37  ---- 0.34  0.16  0.11  \n",
      "\n",
      "* = High similarity (>0.6)\n",
      "Text  8:  0.16  0.14  0.26  0.21  0.36  0.25  0.34  ---- 0.12  0.21  \n",
      "\n",
      "* = High similarity (>0.6)\n",
      "Text  9:  0.10  0.09  0.12  0.03  0.09  -0.03  0.16  0.12  ---- 0.20  \n",
      "\n",
      "* = High similarity (>0.6)\n",
      "Text 10:  0.03  0.01  0.06  0.07  0.13  0.08  0.11  0.21  0.20  ---- \n",
      "\n",
      "* = High similarity (>0.6)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lRahl5n-7Wdd",
    "outputId": "9253502e-5d23-4361-fcca-776cad2697af",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "# Detailed comparisons\n",
    "print(\"Detailed Comparisons\")\n",
    "comparisons = [\n",
    "    (0, 1, \"Positive review vs Positive review\"),\n",
    "    (3, 4, \"Negative review vs Negative review\"),\n",
    "    (0, 3, \"Positive review vs Negative review\"),\n",
    "    (0, 8, \"Movie review vs Off-topic text\"),\n",
    "]\n",
    "# Complete this: Compare movie vs restaurant reviews\n",
    "comparisons.append((0, 8, \"Positive MOVIE vs Positive RESTAURANT\"))\n",
    "for i, j, description in comparisons:\n",
    "    if i < len(texts) and j < len(texts):\n",
    "        sim = similarity_matrix[i][j]\n",
    "        print(f\"\\n{description}:\")\n",
    "        print(f\"  Text {i+1}: '{texts[i]}'\")\n",
    "        print(f\"  Text {j+1}: '{texts[j]}'\")\n",
    "        print(f\"  Similarity: {sim:.3f}\")\n",
    "        if sim > 0.7:\n",
    "            print(f\"   Very similar. These texts are closely related in meaning\")\n",
    "        elif sim > 0.4:\n",
    "            print(f\"   Moderately similar. Some shared concepts\")\n",
    "        else:\n",
    "            print(f\"   Different topics or sentiments\")"
   ],
   "execution_count": 33,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Detailed Comparisons\n",
      "\n",
      "Positive review vs Positive review:\n",
      "  Text 1: 'Amazing movie! Absolutely loved it!'\n",
      "  Text 2: 'Fantastic film, highly recommend!'\n",
      "  Similarity: 0.768\n",
      "   Very similar. These texts are closely related in meaning\n",
      "\n",
      "Negative review vs Negative review:\n",
      "  Text 4: 'Terrible waste of time'\n",
      "  Text 5: 'Very disappointing and boring'\n",
      "  Similarity: 0.545\n",
      "   Moderately similar. Some shared concepts\n",
      "\n",
      "Positive review vs Negative review:\n",
      "  Text 1: 'Amazing movie! Absolutely loved it!'\n",
      "  Text 4: 'Terrible waste of time'\n",
      "  Similarity: 0.090\n",
      "   Different topics or sentiments\n",
      "\n",
      "Movie review vs Off-topic text:\n",
      "  Text 1: 'Amazing movie! Absolutely loved it!'\n",
      "  Text 9: 'The weather is nice today'\n",
      "  Similarity: 0.095\n",
      "   Different topics or sentiments\n",
      "\n",
      "Positive MOVIE vs Positive RESTAURANT:\n",
      "  Text 1: 'Amazing movie! Absolutely loved it!'\n",
      "  Text 9: 'The weather is nice today'\n",
      "  Similarity: 0.095\n",
      "   Different topics or sentiments\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Questions\n",
    "\n",
    "1. Compare similarity between Text 1 and Text 2 (both positive) vs Text 1 and Text 4 (positive vs negative). What aspects of semantic meaning do embeddings prioritize?\n",
    "\n",
    "2. Find similarity scores between two negative reviews (Text 4 and Text 5) and two positive reviews (Text 1 and Text 2). Why would averaging embeddings per class work for classification?\n",
    "\n",
    "3. After adding restaurant reviews: How similar was \"Amazing food\" to \"Amazing movie\"? What does this reveal about domain transfer?"
   ],
   "metadata": {
    "id": "TL1g6q0kEjPw"
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}