{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Prompt Engineering - Hard Tasks (Solutions)\n\nComplete solutions matching task structure. Hard tasks use function-based implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run all cells in this section to set up the environment and load the model.\n",
    "\n",
    "Before running these cells, review the concepts from the main Chapter 6 notebook (00_Start_Here.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional] - Installing Packages on Google Colab",
    "",
    "If you are viewing this notebook on Google Colab, uncomment and run the following code to install dependencies.",
    "",
    "**Note**: Use a GPU for this notebook. In Google Colab, go to Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# %%capture\n",
    "# !pip install --upgrade transformers>=4.40.0 torch accelerate"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model_path = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=False,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def generate_text(prompt, temperature=0.7, max_tokens=500):\n",
    "    \"\"\"Generate text with specified parameters\"\"\"\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        return_full_text=False,\n",
    "        max_new_tokens=max_tokens,\n",
    "        do_sample=True if temperature > 0 else False,\n",
    "        temperature=temperature if temperature > 0 else None,\n",
    "    )\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    output = pipe(messages)\n",
    "    return output[0]['generated_text']"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "\n",
    "Complete the following tasks by implementing the starter code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Level: Hard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About This Task:**\n",
    "Tree-of-Thought generates multiple options at each step and evaluates which paths look most promising. We break this into modular functions instead of one big script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hard Task 1: Tree-of-Thought\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Run baseline CoT to see linear reasoning\n",
    "2. Study how ToT functions work together (generate \u2192 evaluate \u2192 explore)\n",
    "3. Improve `generate_next_steps` prompt to get better options\n",
    "4. Modify `evaluate_option` to use different criteria (safety vs progress)\n",
    "5. Test on a different strategic problem"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "problem = \"\"\"You need to transport a fox, a chicken, and grain across a river.\n",
    "Your boat can only carry you and one item.\n",
    "If left alone: fox eats chicken, chicken eats grain.\n",
    "How do you get everything across safely?\"\"\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: Linear CoT\n",
    "\n",
    "This is the simple approach - ask the model to think step-by-step in one shot."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def linear_cot_solve(problem):\n",
    "    \"\"\"Simple linear chain-of-thought - one path, no exploration\"\"\"\n",
    "    prompt = f\"{problem}\\n\\nLet's think step-by-step:\"\n",
    "    return generate_text(prompt, temperature=0, max_tokens=500)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Linear CoT:\")\n",
    "solution_cot = linear_cot_solve(problem)\n",
    "print(solution_cot)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how linear CoT might miss better solutions or get stuck. Now we'll use Tree-of-Thought to explore multiple paths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree-of-Thought Implementation\n",
    "\n",
    "We break ToT into separate functions. Each function has a single responsibility - this is how real systems are built."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import re\n",
    "from collections import deque"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function 1: Generate Options**\n",
    "\n",
    "Your task: Improve this prompt to generate more specific, actionable next steps."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def generate_next_steps(current_state, problem, num_options=3):\n",
    "    \"\"\"\n",
    "    Generate possible next moves from current state.\n",
    "    \n",
    "    This function uses a prompt to brainstorm options. In real systems,\n",
    "    this allows you to:\n",
    "    - Call it repeatedly at each decision point\n",
    "    - Adjust num_options based on problem complexity\n",
    "    - Cache and reuse results for similar states\n",
    "    \n",
    "    Args:\n",
    "        current_state: Description of where we are now\n",
    "        problem: The full problem description for context\n",
    "        num_options: How many alternatives to generate\n",
    "    \n",
    "    Returns:\n",
    "        List of possible next actions\n",
    "    \"\"\"\n",
    "    # Improve this prompt to get better, more specific options\n",
    "    prompt = f\"\"\"{problem}\n",
    "\n",
    "Current situation: {current_state}\n",
    "\n",
    "What are {num_options} possible next moves? List them:\n",
    "1.\"\"\"\n",
    "    \n",
    "    output = generate_text(prompt, temperature=0.7, max_tokens=200)\n",
    "    \n",
    "    # Parse the output to extract individual options\n",
    "    # This is why functions are useful - parsing logic is isolated here\n",
    "    lines = output.strip().split('\\n')\n",
    "    options = []\n",
    "    \n",
    "    for line in lines[:num_options]:\n",
    "        clean = line.strip()\n",
    "        # Remove common prefixes like \"1.\", \"2.\", \"-\", etc.\n",
    "        for prefix in ['1.', '2.', '3.', '4.', '-', '\u2022']:\n",
    "            if clean.startswith(prefix):\n",
    "                clean = clean[len(prefix):].strip()\n",
    "        if clean:\n",
    "            options.append(clean)\n",
    "    \n",
    "    return options[:num_options]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function 2: Evaluate Options**\n",
    "\n",
    "This function scores how good each option is. Notice how we can easily swap evaluation criteria by modifying just this function."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def evaluate_option(option, problem, criterion=\"progress\"):\n",
    "    \"\"\"\n",
    "    Score how promising this option is (0-10).\n",
    "    \n",
    "    Using a function here means we can:\n",
    "    - Change evaluation criteria without touching tree exploration code\n",
    "    - Test different scoring strategies independently  \n",
    "    - Add logging or caching if needed\n",
    "    \n",
    "    Args:\n",
    "        option: The action to evaluate\n",
    "        problem: Full problem for context\n",
    "        criterion: What to optimize for (\"progress\" or \"safety\")\n",
    "    \n",
    "    Returns:\n",
    "        Score from 0-10\n",
    "    \"\"\"\n",
    "    if criterion == \"safety\":\n",
    "        # Your task: Write a prompt that focuses on safety\n",
    "        prompt = f\"\"\"{problem}\n",
    "\n",
    "Proposed move: {option}\n",
    "\n",
    "On a scale of 0-10, how SAFE is this move?\n",
    "Consider: Does it prevent anything from being eaten?\n",
    "\n",
    "Score (0-10):\"\"\"\n",
    "    else:\n",
    "        # Default: Focus on progress\n",
    "        prompt = f\"\"\"{problem}\n",
    "\n",
    "Proposed move: {option}\n",
    "\n",
    "On a scale of 0-10, how promising is this move?\n",
    "Consider: Does it make progress? Does it violate constraints?\n",
    "\n",
    "Score (0-10):\"\"\"\n",
    "    \n",
    "    output = generate_text(prompt, temperature=0, max_tokens=50)\n",
    "    \n",
    "    # Extract score from text - regex makes this robust to different formats\n",
    "    match = re.search(r'(\\d+)', output)\n",
    "    if match:\n",
    "        score = int(match.group(1))\n",
    "        return min(max(score, 0), 10)  # Clamp to 0-10 range\n",
    "    return 5  # Default middle score if parsing fails"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Structure: Tree Node**\n",
    "\n",
    "We use a class to represent each decision point. This keeps state organized."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class TreeNode:\n",
    "    \"\"\"\n",
    "    Represents one state in the decision tree.\n",
    "    \n",
    "    Using a class instead of just variables makes it easy to:\n",
    "    - Track parent-child relationships\n",
    "    - Reconstruct the path to any node\n",
    "    - Store metadata like scores\n",
    "    \"\"\"\n",
    "    def __init__(self, state, action=None, parent=None):\n",
    "        self.state = state          # Current situation description\n",
    "        self.action = action        # What action led here\n",
    "        self.parent = parent        # Previous node (for path reconstruction)\n",
    "        self.children = []          # Possible next nodes\n",
    "        self.score = 0              # Evaluation score\n",
    "    \n",
    "    def get_path(self):\n",
    "        \"\"\"Reconstruct the sequence of actions from root to this node\"\"\"\n",
    "        path = []\n",
    "        node = self\n",
    "        # Walk backwards through parent links\n",
    "        while node.parent:\n",
    "            path.append(node.action)\n",
    "            node = node.parent\n",
    "        return list(reversed(path))  # Reverse to get root-to-leaf order"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main Algorithm: Tree Search**\n",
    "\n",
    "This orchestrates the generate \u2192 evaluate \u2192 explore loop. Notice how clean it is because we separated concerns into functions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def tree_of_thought_solve(problem, max_depth=3, branch_factor=2, criterion=\"progress\"):\n",
    "    \"\"\"\n",
    "    Explore multiple reasoning paths to find the best solution.\n",
    "    \n",
    "    This function shows why modular design matters:\n",
    "    - We call generate_next_steps() for brainstorming\n",
    "    - We call evaluate_option() for scoring\n",
    "    - We use TreeNode to manage state\n",
    "    - The search logic is separate from prompt engineering\n",
    "    \n",
    "    Args:\n",
    "        problem: Problem to solve\n",
    "        max_depth: How many steps ahead to plan\n",
    "        branch_factor: How many options to consider at each step\n",
    "        criterion: What to optimize (\"progress\" or \"safety\")\n",
    "    \n",
    "    Returns:\n",
    "        Best solution node found\n",
    "    \"\"\"\n",
    "    # Start with initial state\n",
    "    initial_state = \"Everything is on the starting side\"\n",
    "    root = TreeNode(initial_state)\n",
    "    \n",
    "    print(\"Tree-of-Thought exploration:\")\n",
    "    print(f\"Criterion: {criterion}\")\n",
    "    \n",
    "    # Breadth-first search using a queue\n",
    "    queue = deque([(root, 0)])\n",
    "    best_solution = None\n",
    "    best_score = -1\n",
    "    explored_nodes = 0\n",
    "    \n",
    "    # Explore tree level by level\n",
    "    while queue and explored_nodes < 10:  # Limit exploration for demo\n",
    "        node, depth = queue.popleft()\n",
    "        explored_nodes += 1\n",
    "        \n",
    "        print(f\"\\nDepth {depth}, Node {explored_nodes}\")\n",
    "        print(f\"State: {node.state}\")\n",
    "        \n",
    "        # At max depth, evaluate final state\n",
    "        if depth >= max_depth:\n",
    "            score = evaluate_option(node.state, problem, criterion)\n",
    "            node.score = score\n",
    "            print(f\"Final score: {score}/10\")\n",
    "            \n",
    "            # Track best solution\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_solution = node\n",
    "            continue\n",
    "        \n",
    "        # Generate possible next moves - using our function!\n",
    "        next_steps = generate_next_steps(node.state, problem, num_options=branch_factor)\n",
    "        \n",
    "        if next_steps:\n",
    "            print(f\"\\nExploring {len(next_steps)} options:\")\n",
    "            \n",
    "            for i, step in enumerate(next_steps, 1):\n",
    "                # Evaluate each option - using our function!\n",
    "                score = evaluate_option(step, problem, criterion)\n",
    "                print(f\"  {i}. {step} (score: {score}/10)\")\n",
    "                \n",
    "                # Create child node\n",
    "                new_state = f\"{node.state}, then {step}\"\n",
    "                child = TreeNode(new_state, step, node)\n",
    "                child.score = score\n",
    "                node.children.append(child)\n",
    "                \n",
    "                # Pruning: Only explore promising branches (score >= 5)\n",
    "                if score >= 5:\n",
    "                    queue.append((child, depth + 1))\n",
    "                else:\n",
    "                    print(f\"       \u2192 Pruned (low score)\")\n",
    "    \n",
    "    # Display best solution found\n",
    "    if best_solution:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"Best solution path:\")\n",
    "        for i, action in enumerate(best_solution.get_path(), 1):\n",
    "            print(f\"{i}. {action}\")\n",
    "        print(f\"\\nFinal score: {best_score}/10\")\n",
    "        return best_solution\n",
    "    \n",
    "    return None"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the tree search. Watch how it explores multiple paths."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "solution_tot = tree_of_thought_solve(problem, max_depth=3, branch_factor=2, criterion=\"progress\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Different Criteria\n",
    "\n",
    "See how easy it is to swap evaluation criteria? Just change one parameter!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Your task: Run with safety criterion and compare results\n",
    "# solution_safety = tree_of_thought_solve(problem, max_depth=3, branch_factor=2, criterion=\"safety\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1. How many branches were pruned (score < 5)? What does pruning save in terms of LLM calls?\n",
    "\n",
    "2. Compare the solution paths from \"progress\" vs \"safety\" criteria. Did they differ?\n",
    "\n",
    "3. Why is using separate functions better than one big script with all prompts and logic mixed together?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About This Task:**\n",
    "Chain prompting breaks complex tasks into sequential stages. Each stage is a separate function call - this is standard practice in production systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hard Task 2: Chain Prompting\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Run single-prompt baseline to see limitations\n",
    "2. Study the 4-stage chain - notice how each function builds on previous outputs\n",
    "3. Improve Stage 3 prompt to generate better response strategies\n",
    "4. Test what happens if you skip a stage\n",
    "5. Try a different customer review"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "customer_review = \"\"\"I've been using your product for 3 months. Setup was confusing\n",
    "and took 2 hours with no clear instructions. Once I figured it out, the features\n",
    "are quite powerful. Performance is good, but the mobile app crashes occasionally.\n",
    "Support was helpful. For $49/month, I expected better documentation.\n",
    "I'm considering upgrading to premium.\"\"\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: Single Prompt\n",
    "\n",
    "The naive approach - ask the model to do everything at once."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def single_prompt_analysis(review):\n",
    "    \"\"\"One-shot analysis - tries to do everything in one call\"\"\"\n",
    "    prompt = f\"\"\"Analyze this customer review and generate a response:\n",
    "\n",
    "Review: {review}\n",
    "\n",
    "Provide:\n",
    "1. Sentiment analysis\n",
    "2. Key issues\n",
    "3. A professional response\n",
    "\"\"\"\n",
    "    return generate_text(prompt, temperature=0, max_tokens=600)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Single prompt approach:\")\n",
    "single_result = single_prompt_analysis(customer_review)\n",
    "print(single_result)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the single prompt might miss nuances or produce a generic response. Now we'll use a 4-stage chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain Prompting: 4 Specialized Functions\n",
    "\n",
    "Each stage is a focused function. This is how real systems work - specialized components that you can test and improve independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stage 1: Extract Information**\n",
    "\n",
    "First, we pull out facts. This function only does extraction - nothing else."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def extract_information(review):\n",
    "    \"\"\"\n",
    "    Extract structured facts from unstructured review text.\n",
    "    \n",
    "    Why separate this?\n",
    "    - Easy to test extraction accuracy independently\n",
    "    - Can reuse extracted facts for multiple analyses\n",
    "    - Can cache results to avoid re-extraction\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Extract key information from this review:\n",
    "\n",
    "Review: {review}\n",
    "\n",
    "List:\n",
    "- Issues mentioned:\n",
    "- Positive aspects:\n",
    "- Duration of usage:\n",
    "- Price mentioned:\n",
    "\"\"\"\n",
    "    return generate_text(prompt, temperature=0, max_tokens=300)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Stage 1: Extraction\")\n",
    "stage1_output = extract_information(customer_review)\n",
    "print(stage1_output)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stage 2: Analyze Sentiment**\n",
    "\n",
    "Now we analyze the extracted facts. See how we pass stage1_output as input?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def analyze_sentiment(extracted_info):\n",
    "    \"\"\"\n",
    "    Determine overall sentiment from extracted facts.\n",
    "    \n",
    "    Benefits of separating:\n",
    "    - Can swap sentiment analysis methods without changing extraction\n",
    "    - Can test sentiment accuracy on known examples\n",
    "    - Clearer what this function is responsible for\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Based on this information:\n",
    "\n",
    "{extracted_info}\n",
    "\n",
    "Determine:\n",
    "1. Overall sentiment (positive/negative/mixed)\n",
    "2. Satisfaction level (1-10)\n",
    "3. Main concerns\n",
    "\"\"\"\n",
    "    return generate_text(prompt, temperature=0, max_tokens=300)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\\nStage 2: Sentiment Analysis\")\n",
    "stage2_output = analyze_sentiment(stage1_output)\n",
    "print(stage2_output)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stage 3: Plan Response Strategy**\n",
    "\n",
    "Your task: Improve this prompt to create a more detailed response plan."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def plan_response_strategy(sentiment_analysis, extracted_info):\n",
    "    \"\"\"\n",
    "    Decide what to say in the response.\n",
    "    \n",
    "    This function takes both sentiment and facts to create a strategy.\n",
    "    Separating strategy planning from actual writing allows:\n",
    "    - Human review of strategy before writing\n",
    "    - A/B testing different strategies\n",
    "    - Consistent application of business rules\n",
    "    \"\"\"\n",
    "    # Improve this prompt to be more specific about strategy\n",
    "    prompt = f\"\"\"Given this sentiment:\n",
    "\n",
    "{sentiment_analysis}\n",
    "\n",
    "And these facts:\n",
    "\n",
    "{extracted_info}\n",
    "\n",
    "Plan the response:\n",
    "- What to acknowledge\n",
    "- What to apologize for\n",
    "- What actions to offer\n",
    "\"\"\"\n",
    "    return generate_text(prompt, temperature=0, max_tokens=300)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\\nStage 3: Response Strategy\")\n",
    "stage3_output = plan_response_strategy(stage2_output, stage1_output)\n",
    "print(stage3_output)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stage 4: Write Final Response**\n",
    "\n",
    "Finally, we execute the strategy to produce the actual response text."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def write_response(strategy):\n",
    "    \"\"\"\n",
    "    Generate the actual response text following the strategy.\n",
    "    \n",
    "    Why separate this?\n",
    "    - Can change tone/style without changing strategy logic\n",
    "    - Can generate multiple versions for A/B testing\n",
    "    - Can add templates or brand voice guidelines here\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Write a professional customer service response following this strategy:\n",
    "\n",
    "{strategy}\n",
    "\n",
    "Tone: Professional, empathetic, solution-focused\n",
    "Length: 2-3 paragraphs\n",
    "\n",
    "Response:\n",
    "\"\"\"\n",
    "    return generate_text(prompt, temperature=0, max_tokens=400)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\\nStage 4: Final Response\")\n",
    "stage4_output = write_response(stage3_output)\n",
    "print(stage4_output)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how each stage built on the previous one? This is modular design - each function has one job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1. What information from Stage 1 was used in Stage 3? What would happen if you skipped Stage 1?\n",
    "\n",
    "2. Which stage added the most value compared to the single-prompt approach?\n",
    "\n",
    "3. Why is it better to have 4 functions instead of copying the prompts 4 times in a row?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About This Task:**\n",
    "Output verification uses one prompt to generate, another to check, and a third to fix. Functions make this generate-verify-correct loop clean and testable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hard Task 3: Output Verification Loop\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Study the 3-function pattern: generate \u2192 verify \u2192 correct\n",
    "2. Run to see which requirements the model might miss\n",
    "3. Improve the verification prompt to catch more issues\n",
    "4. Test what happens if you run verify \u2192 correct twice\n",
    "5. Try different code generation tasks"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "requirements = {\n",
    "    'task': 'Write a function that calculates a discount',\n",
    "    'name': 'calculate_discount',\n",
    "    'parameters': ['price', 'discount_percent'],\n",
    "    'must_have': ['docstring', 'input validation', 'return statement']\n",
    "}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function 1: Generate Code**\n",
    "\n",
    "First attempt at generating code from requirements."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def generate_function(requirements):\n",
    "    \"\"\"\n",
    "    Generate code from requirements.\n",
    "    \n",
    "    Why use a function?\n",
    "    - Can be called multiple times for different requirements\n",
    "    - Easy to test with various requirement sets\n",
    "    - Can log all generated code for analysis\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Write a Python function:\n",
    "\n",
    "Task: {requirements['task']}\n",
    "Function name: {requirements['name']}\n",
    "Parameters: {', '.join(requirements['parameters'])}\n",
    "Must include: {', '.join(requirements['must_have'])}\n",
    "\n",
    "Write the complete function:\n",
    "\"\"\"\n",
    "    return generate_text(prompt, temperature=0, max_tokens=400)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Generated code:\")\n",
    "code = generate_function(requirements)\n",
    "print(code)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function 2: Verify Code**\n",
    "\n",
    "Check if the generated code meets all requirements. Your task: Improve this to catch more issues."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def verify_code(code, requirements):\n",
    "    \"\"\"\n",
    "    Use a prompt to verify if code meets requirements.\n",
    "    \n",
    "    Separating verification into its own function:\n",
    "    - Allows testing verification logic independently\n",
    "    - Can swap between prompt-based and code-based verification\n",
    "    - Makes it easy to add human-in-the-loop approval\n",
    "    \"\"\"\n",
    "    # Improve this prompt to be more thorough in checking\n",
    "    prompt = f\"\"\"Check if this code meets the requirements:\n",
    "\n",
    "Code:\n",
    "{code}\n",
    "\n",
    "Requirements:\n",
    "- Function name: {requirements['name']}\n",
    "- Must have: {', '.join(requirements['must_have'])}\n",
    "\n",
    "List any issues found (or write 'No issues'):\n",
    "\"\"\"\n",
    "    return generate_text(prompt, temperature=0, max_tokens=200)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\\nVerification:\")\n",
    "verification = verify_code(code, requirements)\n",
    "print(verification)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function 3: Request Corrections**\n",
    "\n",
    "If verification found issues, ask the model to fix them."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def request_correction(code, verification_feedback):\n",
    "    \"\"\"\n",
    "    Request fixes based on verification results.\n",
    "    \n",
    "    Using a function here:\n",
    "    - Can iterate the verify-correct loop multiple times\n",
    "    - Can add retry logic with exponential backoff\n",
    "    - Can track how many correction rounds were needed\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"This code has issues:\n",
    "\n",
    "Code:\n",
    "{code}\n",
    "\n",
    "Issues found:\n",
    "{verification_feedback}\n",
    "\n",
    "Fix these issues. Provide the complete corrected function:\n",
    "\"\"\"\n",
    "    return generate_text(prompt, temperature=0, max_tokens=400)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the verify-correct loop. Watch how the functions work together."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if 'no issues' not in verification.lower():\n",
    "    print(\"\\nIssues found - requesting corrections:\")\n",
    "    corrected_code = request_correction(code, verification)\n",
    "    print(corrected_code)\n",
    "    \n",
    "    # Your task: Try verifying again to see if issues were fixed\n",
    "    # second_check = verify_code(corrected_code, requirements)\n",
    "    # print(\"\\nSecond verification:\")\n",
    "    # print(second_check)\n",
    "else:\n",
    "    print(\"\\nNo issues found!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1. What issues did verification catch? How would you catch these with code-based checks instead of prompts?\n",
    "\n",
    "2. If you run verify \u2192 correct twice, does it improve quality further? When should you stop?\n",
    "\n",
    "3. Why is it useful to have separate verify and correct functions instead of one \"fix_code\" function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About This Task:**\n",
    "Multi-stage reasoning uses a 4-function pipeline: reason \u2192 reflect \u2192 revise \u2192 finalize. Each stage improves on the previous one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hard Task 4: Self-Reflection Pipeline\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Study the 4-stage pipeline and how each function builds on previous outputs\n",
    "2. Run to see how self-reflection catches initial mistakes\n",
    "3. Improve the reflection prompt to ask better critical questions\n",
    "4. Test what happens if you skip the reflection stage\n",
    "5. Try a different decision-making problem"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "problem = \"\"\"A company is choosing between two pricing strategies:\n",
    "\n",
    "Strategy A: $10/month, expect 10,000 customers\n",
    "Strategy B: $25/month, expect 5,000 customers\n",
    "\n",
    "Customer support costs $2 per customer per month.\n",
    "Which strategy maximizes profit?\"\"\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stage 1: Initial Reasoning**\n",
    "\n",
    "First pass at solving the problem."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def initial_reasoning(problem):\n",
    "    \"\"\"\n",
    "    First attempt at solving the problem.\n",
    "    \n",
    "    Why separate this?\n",
    "    - Can compare initial vs final reasoning\n",
    "    - Can measure how often initial reasoning is wrong\n",
    "    - Can use initial reasoning as a baseline for testing\n",
    "    \"\"\"\n",
    "    prompt = f\"{problem}\\n\\nLet's think step-by-step:\"\n",
    "    return generate_text(prompt, temperature=0, max_tokens=400)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Stage 1: Initial Reasoning\")\n",
    "initial = initial_reasoning(problem)\n",
    "print(initial)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stage 2: Self-Reflection**\n",
    "\n",
    "Your task: Improve this prompt to ask more critical questions about the reasoning."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def self_reflect(problem, initial_reasoning):\n",
    "    \"\"\"\n",
    "    Critically examine the initial reasoning.\n",
    "    \n",
    "    This function demonstrates meta-cognition - thinking about thinking.\n",
    "    Benefits:\n",
    "    - Can test if reflection improves accuracy\n",
    "    - Can tune reflection questions for different problem types\n",
    "    - Can add domain-specific reflection guidelines\n",
    "    \"\"\"\n",
    "    # Improve these critical questions\n",
    "    prompt = f\"\"\"{problem}\n",
    "\n",
    "Here was my initial reasoning:\n",
    "{initial_reasoning}\n",
    "\n",
    "Now critically examine this:\n",
    "- Did I consider all factors?\n",
    "- Are calculations correct?\n",
    "- Did I miss anything?\n",
    "\n",
    "Critical reflection:\n",
    "\"\"\"\n",
    "    return generate_text(prompt, temperature=0, max_tokens=300)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\\nStage 2: Self-Reflection\")\n",
    "reflection = self_reflect(problem, initial)\n",
    "print(reflection)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what the model caught when reviewing its own reasoning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stage 3: Revised Reasoning**\n",
    "\n",
    "Incorporate insights from reflection to improve the answer."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def revised_reasoning(problem, initial, reflection):\n",
    "    \"\"\"\n",
    "    Generate improved reasoning based on reflection.\n",
    "    \n",
    "    This function shows the value of iteration:\n",
    "    - Initial attempt \u2192 reflection \u2192 revision is a powerful pattern\n",
    "    - Keeping them separate means we can inspect each stage\n",
    "    - Can measure improvement from initial to revised\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"{problem}\n",
    "\n",
    "My initial reasoning:\n",
    "{initial}\n",
    "\n",
    "After reflection:\n",
    "{reflection}\n",
    "\n",
    "Now provide improved reasoning that addresses the concerns:\n",
    "\"\"\"\n",
    "    return generate_text(prompt, temperature=0, max_tokens=400)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\\nStage 3: Revised Reasoning\")\n",
    "revised = revised_reasoning(problem, initial, reflection)\n",
    "print(revised)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stage 4: Final Answer with Confidence**\n",
    "\n",
    "Produce a clean final answer with confidence assessment."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def final_answer_with_confidence(problem, revised_reasoning):\n",
    "    \"\"\"\n",
    "    Extract final answer and assess confidence.\n",
    "    \n",
    "    Separating this allows:\n",
    "    - Consistent answer formatting across problems\n",
    "    - Tracking confidence scores over time\n",
    "    - Filtering low-confidence answers for human review\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"{problem}\n",
    "\n",
    "After careful analysis:\n",
    "{revised_reasoning}\n",
    "\n",
    "Provide:\n",
    "1. Final answer (which strategy?)\n",
    "2. Key reasoning (2-3 sentences)\n",
    "3. Confidence level (0-100%)\n",
    "4. Main uncertainty\n",
    "\"\"\"\n",
    "    return generate_text(prompt, temperature=0, max_tokens=300)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\\nStage 4: Final Answer\")\n",
    "final = final_answer_with_confidence(problem, revised)\n",
    "print(final)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the 4-stage pipeline produced a well-reasoned, confident answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare: With vs Without Reflection"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Your task: Run without reflection stage to see the difference\n",
    "# skipped_reflection = final_answer_with_confidence(problem, initial)\n",
    "# print(\"\\nWithout reflection stage:\")\n",
    "# print(skipped_reflection)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1. What mistake did self-reflection catch that initial reasoning missed?\n",
    "\n",
    "2. Compare the final answer with vs without the reflection stage. Was reflection worth the extra LLM call?\n",
    "\n",
    "3. Why is having 4 separate functions better than one big function that does all stages? List 3 specific advantages."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}