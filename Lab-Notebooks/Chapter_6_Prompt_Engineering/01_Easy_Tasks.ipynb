{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Prompt Engineering - Easy Tasks\n",
    "\n",
    "This notebook covers basic prompt engineering concepts: temperature effects, prompt components, in-context learning, and chain-of-thought reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run all cells in this section to set up the environment and load the model.\n",
    "\n",
    "Before running these cells, review the concepts from the main Chapter 6 notebook (00_Start_Here.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional] - Installing Packages on <img src=\"https://colab.google/static/images/icons/colab.png\" width=100>\n",
    "\n",
    "If you are viewing this notebook on Google Colab, uncomment and run the following code to install dependencies.\n",
    "\n",
    "**Note**: Use a GPU for this notebook. In Google Colab, go to Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install transformers>=4.40.0 torch accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=False,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, temperature=0.7, max_tokens=200):\n",
    "    \"\"\"Generate text with specified parameters\"\"\"\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        return_full_text=False,\n",
    "        max_new_tokens=max_tokens,\n",
    "        do_sample=True if temperature > 0 else False,\n",
    "        temperature=temperature if temperature > 0 else None,\n",
    "    )\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    output = pipe(messages)\n",
    "    return output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "\n",
    "Complete the following tasks by implementing the starter code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Level: Easy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About This Task:**\n",
    "Temperature controls randomness in generation. Lower values give consistent outputs, higher values give varied outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Easy Task 1: Finding the Right Temperature\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Execute code to compare temperature effects on three use cases\n",
    "2. Fill in missing temperature values based on your observations\n",
    "3. Run determinism test to verify temperature=0 consistency\n",
    "4. Test with your own prompts\n",
    "5. Analyze which temperatures work best for different tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we test three different use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompts = [\n",
    "    \"What is the capital of France?\",  # Factual\n",
    "    \"Write the first sentence of a mystery novel.\",  # Creative\n",
    "    \"Write a Python function to calculate factorial.\",  # Code\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = [0.0, 0.3, 0.7, 1.0, 1.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how different temperatures affect each use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt in test_prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for temp in temperatures:\n",
    "        output = generate_text(prompt, temperature=temp, max_tokens=50)\n",
    "        print(f\"\\nTemp={temp}: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1a: Select Best Temperature\n",
    "\n",
    "Based on the outputs above, fill in the best temperature for each use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in: What temperature works best for each task?\n",
    "best_temp_factual = None  # For \"What is the capital of France?\"\n",
    "best_temp_creative = None  # For \"Write the first sentence...\"\n",
    "best_temp_code = None  # For \"Write a Python function...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your selections here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing your temperature selections:\")\n",
    "\n",
    "if best_temp_factual is not None:\n",
    "    output = generate_text(\"What is the capital of France?\", temperature=best_temp_factual, max_tokens=30)\n",
    "    print(f\"\\nFactual (temp={best_temp_factual}): {output}\")\n",
    "\n",
    "if best_temp_creative is not None:\n",
    "    output = generate_text(\"Write the first sentence of a mystery novel.\", temperature=best_temp_creative, max_tokens=50)\n",
    "    print(f\"\\nCreative (temp={best_temp_creative}): {output}\")\n",
    "\n",
    "if best_temp_code is not None:\n",
    "    output = generate_text(\"Write a Python function to calculate factorial.\", temperature=best_temp_code, max_tokens=100)\n",
    "    print(f\"\\nCode (temp={best_temp_code}): {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1b: Determinism Test\n",
    "\n",
    "Run this cell multiple times to verify temperature=0 gives identical outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = generate_text(\"What is 2+2?\", temperature=0, max_tokens=20)\n",
    "print(f\"Output: {output}\")\n",
    "print(\"\\nRun this cell again - you should get the EXACT same output.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1. At temperature=1.5, did the factual question give wrong answers? Why is determinism critical for factual tasks?\n",
    "\n",
    "2. For creative writing, compare outputs at temperature=0.3 vs 1.0. Which produced more interesting variations?\n",
    "\n",
    "3. Did code generation at temperature=1.5 produce valid Python? What's the risk of high temperature for code?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About This Task:**\n",
    "Prompts have seven components: Persona, Instruction, Context, Format, Audience, Tone, Data. Adding more components improves output quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Easy Task 2: Building a Complete Prompt\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Run pre-built prompt versions to see incremental improvements\n",
    "2. Complete `prompt_v5` by adding the missing 3 components\n",
    "3. Test removing Format to see its impact\n",
    "4. Create your own scenario\n",
    "5. Compare output quality as components are added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with just an instruction and gradually add components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 1: Instruction only\n",
    "prompt_v1 = \"Explain how to make coffee.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"V1: Instruction only\")\n",
    "output = generate_text(prompt_v1, temperature=0, max_tokens=150)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 2: + Audience\n",
    "prompt_v2 = \"\"\"Explain how to make coffee.\n",
    "\n",
    "Audience: Someone who has never made coffee before.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"V2: + Audience\")\n",
    "output = generate_text(prompt_v2, temperature=0, max_tokens=150)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how adding Audience changes the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 3: + Format\n",
    "prompt_v3 = \"\"\"Explain how to make coffee.\n",
    "\n",
    "Audience: Someone who has never made coffee before.\n",
    "\n",
    "Format:\n",
    "1. Equipment needed\n",
    "2. Step-by-step instructions\n",
    "3. Common mistakes\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"V3: + Format\")\n",
    "output = generate_text(prompt_v3, temperature=0, max_tokens=200)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how Format structures the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 4: + Tone\n",
    "prompt_v4 = \"\"\"Explain how to make coffee.\n",
    "\n",
    "Audience: Someone who has never made coffee before.\n",
    "\n",
    "Format:\n",
    "1. Equipment needed\n",
    "2. Step-by-step instructions\n",
    "3. Common mistakes\n",
    "\n",
    "Tone: Friendly and encouraging.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"V4: + Tone\")\n",
    "output = generate_text(prompt_v4, temperature=0, max_tokens=200)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2a: Complete Version 5\n",
    "\n",
    "Your task: Add Persona, Context, and Data to create a complete prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in: Add the 3 missing components\n",
    "prompt_v5 = \"\"\"Persona: [Fill in - who is giving this explanation?]\n",
    "\n",
    "Explain how to make coffee.\n",
    "\n",
    "Context: [Fill in - why does the person need to learn this?]\n",
    "\n",
    "Audience: Someone who has never made coffee before.\n",
    "\n",
    "Format:\n",
    "1. Equipment needed\n",
    "2. Step-by-step instructions\n",
    "3. Common mistakes\n",
    "\n",
    "Tone: Friendly and encouraging.\n",
    "\n",
    "Data: [Fill in - specific details like coffee-to-water ratio]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"V5: All 7 components\")\n",
    "output = generate_text(prompt_v5, temperature=0, max_tokens=250)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1. Compare V1 and V2 outputs. How did specifying Audience change the language complexity?\n",
    "\n",
    "2. Which component made the biggest single improvement to output quality?\n",
    "\n",
    "3. When might you intentionally use fewer components? Give a specific scenario where V1 would be better than V5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About This Task:**\n",
    "In-context learning uses examples to guide the model. Zero-shot has no examples, one-shot has one, few-shot has multiple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Easy Task 3: Improving Few-Shot Examples\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Run zero-shot, one-shot, and few-shot on test greetings\n",
    "2. Identify which greetings cause disagreement\n",
    "3. Improve the few-shot prompt by adding better examples\n",
    "4. Test edge cases\n",
    "5. Analyze why certain examples improve accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_greetings = [\n",
    "    \"Good morning, how may I assist you?\",\n",
    "    \"Hey, what's up?\",\n",
    "    \"Hello, nice to meet you.\",\n",
    "    \"Hi there.\",\n",
    "    \"Dear valued customer,\",  # Very formal\n",
    "    \"Yo!\",  # Very casual\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-Shot\n",
    "\n",
    "Here we ask the model to classify without any examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Zero-shot classification:\")\n",
    "zero_results = {}\n",
    "\n",
    "for greeting in test_greetings:\n",
    "    prompt = f\"\"\"Classify formality: formal, neutral, or casual.\n",
    "\n",
    "Greeting: {greeting}\n",
    "Formality:\"\"\"\n",
    "    \n",
    "    result = generate_text(prompt, temperature=0, max_tokens=10).strip()\n",
    "    zero_results[greeting] = result\n",
    "    print(f\"{greeting} -> {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Shot\n",
    "\n",
    "See how a single example helps guide the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"One-shot classification:\")\n",
    "one_results = {}\n",
    "\n",
    "for greeting in test_greetings:\n",
    "    prompt = f\"\"\"Classify formality: formal, neutral, or casual.\n",
    "\n",
    "Example:\n",
    "Greeting: Dear Sir or Madam\n",
    "Formality: formal\n",
    "\n",
    "Greeting: {greeting}\n",
    "Formality:\"\"\"\n",
    "    \n",
    "    result = generate_text(prompt, temperature=0, max_tokens=10).strip()\n",
    "    one_results[greeting] = result\n",
    "    print(f\"{greeting} -> {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-Shot\n",
    "\n",
    "Your task: Improve this prompt by adding 1-2 more examples to handle edge cases better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Few-shot classification:\")\n",
    "few_results = {}\n",
    "\n",
    "for greeting in test_greetings:\n",
    "    # Fill in: Add 1-2 more examples after \"Hello, how are you\"\n",
    "    prompt = f\"\"\"Classify formality: formal, neutral, or casual.\n",
    "\n",
    "Examples:\n",
    "\n",
    "Greeting: Dear Sir or Madam\n",
    "Formality: formal\n",
    "\n",
    "Greeting: Yo dude\n",
    "Formality: casual\n",
    "\n",
    "Greeting: Hello, how are you\n",
    "Formality: neutral\n",
    "\n",
    "[Add 1-2 more examples here]\n",
    "\n",
    "Greeting: {greeting}\n",
    "Formality:\"\"\"\n",
    "    \n",
    "    result = generate_text(prompt, temperature=0, max_tokens=10).strip()\n",
    "    few_results[greeting] = result\n",
    "    print(f\"{greeting} -> {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison\n",
    "\n",
    "Here we identify disagreements to see where examples help most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disagreements = []\n",
    "\n",
    "for greeting in test_greetings:\n",
    "    zero = zero_results[greeting]\n",
    "    one = one_results[greeting]\n",
    "    few = few_results[greeting]\n",
    "    \n",
    "    print(f\"\\n{greeting}\")\n",
    "    print(f\"  Zero-shot: {zero}\")\n",
    "    print(f\"  One-shot:  {one}\")\n",
    "    print(f\"  Few-shot:  {few}\")\n",
    "    \n",
    "    if zero == one == few:\n",
    "        print(f\"  All agree\")\n",
    "    else:\n",
    "        print(f\"  DISAGREEMENT\")\n",
    "        disagreements.append(greeting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice which greetings benefit most from examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{len(disagreements)} greetings showed disagreement:\")\n",
    "for g in disagreements:\n",
    "    print(f\"  - {g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1. Which greeting showed the biggest difference between zero-shot and few-shot? Why was it ambiguous?\n",
    "\n",
    "2. Did adding more examples improve accuracy on edge cases like \"Yo!\" or \"Dear valued customer\"?\n",
    "\n",
    "3. What makes a good few-shot example? Should you show edge cases or clear typical examples?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About This Task:**\n",
    "Chain-of-Thought prompting asks the model to show its reasoning step-by-step, improving accuracy on complex problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Easy Task 4: Testing Chain-of-Thought\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Run direct prompting on simple and tricky problems\n",
    "2. Compare with few-shot CoT to see reasoning improvements\n",
    "3. Test zero-shot CoT on hard problems\n",
    "4. Improve CoT examples to fix errors\n",
    "5. Analyze when step-by-step reasoning prevents mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test on both simple problems and counter-intuitive ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = [\n",
    "    (\"If John has 5 apples and gives 2 to Mary, how many does he have?\", 3, \"easy\"),\n",
    "    (\"A ticket costs $15. I buy 3 tickets with a $50 bill. How much change?\", 5, \"easy\"),\n",
    "    (\"A bat and ball cost $1.10 total. The bat costs $1 more than the ball. How much is the ball?\", 0.05, \"tricky\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Prompting\n",
    "\n",
    "Here we ask for answers directly without reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Direct prompting (no reasoning):\")\n",
    "\n",
    "for question, correct, difficulty in problems:\n",
    "    prompt = f\"{question}\\nAnswer:\"\n",
    "    answer = generate_text(prompt, temperature=0, max_tokens=30)\n",
    "    \n",
    "    print(f\"\\n[{difficulty.upper()}] {question}\")\n",
    "    print(f\"Model: {answer.strip()}\")\n",
    "    print(f\"Correct: {correct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how direct prompting might fail on the tricky problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-Shot CoT\n",
    "\n",
    "Your task: Improve the prompt by adding a third example showing careful algebra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Few-shot CoT:\")\n",
    "\n",
    "for question, correct, difficulty in problems:\n",
    "    # Fill in: Add a third example to help with the tricky problem\n",
    "    prompt = f\"\"\"Solve step-by-step.\n",
    "\n",
    "Q: Roger has 5 balls. He buys 2 cans with 3 balls each. How many balls does he have?\n",
    "A: Roger starts with 5 balls.\n",
    "He buys 2 cans, each has 3 balls.\n",
    "New balls: 2 Ã— 3 = 6\n",
    "Total: 5 + 6 = 11\n",
    "Answer: 11\n",
    "\n",
    "Q: A cafe had 23 apples. They used 20 for lunch and bought 6 more. How many now?\n",
    "A: Start with 23 apples.\n",
    "After using 20: 23 - 20 = 3\n",
    "After buying 6: 3 + 6 = 9\n",
    "Answer: 9\n",
    "\n",
    "[Add another example showing careful math]\n",
    "\n",
    "Q: {question}\n",
    "A:\"\"\"\n",
    "    \n",
    "    answer = generate_text(prompt, temperature=0, max_tokens=150)\n",
    "    \n",
    "    print(f\"\\n[{difficulty.upper()}] {question}\")\n",
    "    print(f\"Reasoning: {answer}\")\n",
    "    print(f\"Correct: {correct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how showing reasoning steps helps catch mistakes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-Shot CoT\n",
    "\n",
    "Here we use the phrase \"Let's think step-by-step\" to trigger reasoning without examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Zero-shot CoT:\")\n",
    "\n",
    "for question, correct, difficulty in problems:\n",
    "    prompt = f\"{question}\\n\\nLet's think step-by-step:\"\n",
    "    answer = generate_text(prompt, temperature=0, max_tokens=150)\n",
    "    \n",
    "    print(f\"\\n[{difficulty.upper()}] {question}\")\n",
    "    print(f\"Reasoning: {answer}\")\n",
    "    print(f\"Correct: {correct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how a simple phrase triggers step-by-step reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1. Did direct prompting get the bat-and-ball problem wrong? What's the common wrong answer ($0.10)?\n",
    "\n",
    "2. Compare few-shot CoT vs zero-shot CoT on the tricky problem. Which caught the mistake better?\n",
    "\n",
    "3. What type of problems benefit most from CoT? When is direct prompting good enough?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
