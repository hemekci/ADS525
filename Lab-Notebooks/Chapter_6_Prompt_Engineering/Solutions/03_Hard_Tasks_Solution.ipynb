{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Prompt Engineering - Hard Tasks (Solutions)\n",
    "\n",
    "Complete solutions for all Hard Tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions Summary\n",
    "\n",
    "### Task 1: Tree-of-Thought\n",
    "\n",
    "**How it works:**\n",
    "1. Generate multiple possible next steps\n",
    "2. Evaluate each option with a prompt\n",
    "3. Explore promising branches (score >= 5)\n",
    "4. Prune low-scoring branches\n",
    "\n",
    "**Key prompts:**\n",
    "\n",
    "Generate options:\n",
    "```\n",
    "{problem}\n",
    "\n",
    "Current situation: {current_state}\n",
    "\n",
    "What are {num_options} possible next moves? List them:\n",
    "1.\n",
    "```\n",
    "\n",
    "Evaluate options:\n",
    "```\n",
    "{problem}\n",
    "\n",
    "Proposed move: {option}\n",
    "\n",
    "On a scale of 0-10, how promising is this move?\n",
    "Consider: Does it make progress? Does it violate constraints?\n",
    "\n",
    "Score (0-10):\n",
    "```\n",
    "\n",
    "**Alternative evaluation criteria:**\n",
    "- Safety-focused: \"How safe is this move?\"\n",
    "- Goal-oriented: \"How close does this get us to the goal?\"\n",
    "- Risk-aware: \"What's the risk of this move failing?\"\n",
    "\n",
    "**When to use:**\n",
    "- Strategic problems with multiple valid approaches\n",
    "- When exploring alternatives is valuable\n",
    "- Problems where linear reasoning often fails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Chain Prompting\n",
    "\n",
    "**4-stage chain:**\n",
    "\n",
    "**Stage 1 - Extraction:**\n",
    "```\n",
    "Extract key information from this review:\n",
    "\n",
    "Review: {review}\n",
    "\n",
    "List:\n",
    "- Issues mentioned:\n",
    "- Positive aspects:\n",
    "- Duration of usage:\n",
    "- Price mentioned:\n",
    "```\n",
    "\n",
    "**Stage 2 - Sentiment:**\n",
    "```\n",
    "Based on this information:\n",
    "\n",
    "{stage1_output}\n",
    "\n",
    "Determine:\n",
    "1. Overall sentiment (positive/negative/mixed)\n",
    "2. Satisfaction level (1-10)\n",
    "3. Main concerns\n",
    "```\n",
    "\n",
    "**Stage 3 - Strategy:**\n",
    "```\n",
    "Given this sentiment:\n",
    "\n",
    "{stage2_output}\n",
    "\n",
    "And these facts:\n",
    "\n",
    "{stage1_output}\n",
    "\n",
    "Plan the response:\n",
    "- What to acknowledge\n",
    "- What to apologize for\n",
    "- What actions to offer\n",
    "```\n",
    "\n",
    "**Stage 4 - Final Response:**\n",
    "```\n",
    "Write a professional response following this strategy:\n",
    "\n",
    "{stage3_output}\n",
    "\n",
    "Tone: Professional, empathetic, solution-focused\n",
    "Length: 2-3 paragraphs\n",
    "\n",
    "Response:\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- Each stage is specialized\n",
    "- Easier to debug and improve\n",
    "- More thorough analysis\n",
    "- Can inspect intermediate results\n",
    "\n",
    "**Trade-offs:**\n",
    "- More API calls (more expensive)\n",
    "- Slower than single prompt\n",
    "- More complex to implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Output Verification\n",
    "\n",
    "**Generate code:**\n",
    "```\n",
    "Write a Python function:\n",
    "\n",
    "Task: {task}\n",
    "Function name: {name}\n",
    "Parameters: {parameters}\n",
    "Must include: {must_have}\n",
    "\n",
    "Write the complete function:\n",
    "```\n",
    "\n",
    "**Verification prompt:**\n",
    "```\n",
    "Check if this code meets the requirements:\n",
    "\n",
    "Code:\n",
    "{code}\n",
    "\n",
    "Requirements:\n",
    "- Function name: {name}\n",
    "- Must have: {must_have}\n",
    "\n",
    "List any issues found (or write 'No issues'):\n",
    "```\n",
    "\n",
    "**Correction prompt:**\n",
    "```\n",
    "This code has issues:\n",
    "\n",
    "Code:\n",
    "{code}\n",
    "\n",
    "Issues found:\n",
    "{verification_feedback}\n",
    "\n",
    "Fix these issues. Provide the complete corrected function:\n",
    "```\n",
    "\n",
    "**When to use prompt-based verification:**\n",
    "- Semantic requirements (clarity, style)\n",
    "- High-level structure\n",
    "- Documentation quality\n",
    "\n",
    "**When to use code-based verification:**\n",
    "- Syntax errors\n",
    "- Specific patterns (regex)\n",
    "- Performance critical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Multi-Stage Reasoning with Self-Reflection\n",
    "\n",
    "**4-stage approach:**\n",
    "\n",
    "**Stage 1 - Initial reasoning:**\n",
    "```\n",
    "{problem}\n",
    "\n",
    "Let's think step-by-step:\n",
    "```\n",
    "\n",
    "**Stage 2 - Self-reflection:**\n",
    "```\n",
    "{problem}\n",
    "\n",
    "Here was my initial reasoning:\n",
    "{initial_reasoning}\n",
    "\n",
    "Now, critically examine this:\n",
    "- Did I consider all factors?\n",
    "- Are calculations correct?\n",
    "- Did I miss anything?\n",
    "\n",
    "Critical reflection:\n",
    "```\n",
    "\n",
    "**Stage 3 - Revised reasoning:**\n",
    "```\n",
    "{problem}\n",
    "\n",
    "My initial reasoning:\n",
    "{initial}\n",
    "\n",
    "After reflection:\n",
    "{reflection}\n",
    "\n",
    "Now provide improved reasoning:\n",
    "```\n",
    "\n",
    "**Stage 4 - Final answer:**\n",
    "```\n",
    "{problem}\n",
    "\n",
    "After analysis:\n",
    "{revised_reasoning}\n",
    "\n",
    "Provide:\n",
    "1. Final answer (which strategy?)\n",
    "2. Key reasoning (2-3 sentences)\n",
    "3. Confidence level (0-100%)\n",
    "4. Main uncertainty\n",
    "```\n",
    "\n",
    "**What reflection typically catches:**\n",
    "- Calculation errors\n",
    "- Missed factors\n",
    "- Hidden assumptions\n",
    "- Alternative perspectives\n",
    "\n",
    "**When to use:**\n",
    "- High-stakes decisions\n",
    "- Complex reasoning\n",
    "- When initial answers often have errors\n",
    "- When you need confidence assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions Answered\n",
    "\n",
    "### Task 1\n",
    "\n",
    "1. **How did ToT compare to linear CoT?**\n",
    "   - ToT explored multiple approaches\n",
    "   - Found better solutions on strategic problems\n",
    "   - More expensive (multiple evaluations)\n",
    "\n",
    "2. **Which branches were pruned (scored below 5)?**\n",
    "   - Moves that violate constraints\n",
    "   - Dead-end approaches\n",
    "   - Moves that don't make progress\n",
    "\n",
    "3. **How did changing the evaluation criteria affect the solution?**\n",
    "   - Safety-focused: Prioritizes avoiding violations\n",
    "   - Progress-focused: Prioritizes moving forward\n",
    "   - Different criteria can find different solutions\n",
    "\n",
    "### Task 2\n",
    "\n",
    "1. **Did the chain approach produce a better response?**\n",
    "   - Usually yes, more thorough\n",
    "   - Better structure\n",
    "   - Addresses all aspects systematically\n",
    "\n",
    "2. **Which stage added the most value?**\n",
    "   - Varies by use case\n",
    "   - Often Stage 3 (strategy) is most valuable\n",
    "   - Stage 1 (extraction) prevents information loss\n",
    "\n",
    "3. **Could you combine stages 1 and 2? What would you lose?**\n",
    "   - Yes, but lose specialization\n",
    "   - Harder to debug\n",
    "   - May miss nuances\n",
    "\n",
    "### Task 3\n",
    "\n",
    "1. **What issues did verification catch?**\n",
    "   - Missing docstrings\n",
    "   - Missing input validation\n",
    "   - Incomplete error handling\n",
    "   - Style issues\n",
    "\n",
    "2. **Did the correction prompt successfully fix the issues?**\n",
    "   - Usually yes on first try\n",
    "   - May need iteration for complex issues\n",
    "   - Clear feedback leads to better corrections\n",
    "\n",
    "3. **When would you use prompt-based verification vs code-based checks?**\n",
    "   - Prompt-based: Semantic requirements, style, documentation\n",
    "   - Code-based: Syntax, specific patterns, performance\n",
    "   - Often use both together\n",
    "\n",
    "### Task 4\n",
    "\n",
    "1. **What did self-reflection catch that initial reasoning missed?**\n",
    "   - Hidden assumptions\n",
    "   - Calculation errors\n",
    "   - Missed factors\n",
    "   - Alternative approaches\n",
    "\n",
    "2. **How did the revised reasoning differ from the initial attempt?**\n",
    "   - More complete\n",
    "   - Considered edge cases\n",
    "   - More careful calculations\n",
    "   - Better structured\n",
    "\n",
    "3. **Was the confidence assessment reasonable?**\n",
    "   - Usually aligns with problem difficulty\n",
    "   - Lower confidence on ambiguous problems\n",
    "   - Uncertainty identification is valuable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
