{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Prompt Engineering - Hard Tasks (Solutions)\n",
    "\n",
    "Complete solutions showing improved prompts for all advanced techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Tree-of-Thought - SOLUTION\n",
    "\n",
    "The key is improving the prompts to generate better options and evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution: Improved `generate_next_steps` Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_steps(current_state, problem, num_options=3):\n",
    "    \"\"\"\n",
    "    SOLUTION: Improved prompt with more specific guidance.\n",
    "    \n",
    "    Changes from original:\n",
    "    - Added reminder about constraints\n",
    "    - Requested specific format (who/what crosses)\n",
    "    - Asked for actionable next moves\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"{problem}\n",
    "\n",
    "Current situation: {current_state}\n",
    "\n",
    "Remember the constraints:\n",
    "- Boat holds you + one item only\n",
    "- Fox eats chicken if left alone\n",
    "- Chicken eats grain if left alone\n",
    "\n",
    "What are {num_options} possible next moves? \n",
    "For each move, specify exactly who/what crosses the river.\n",
    "\n",
    "List them:\n",
    "1.\"\"\"\n",
    "    \n",
    "    output = generate_text(prompt, temperature=0.7, max_tokens=200)\n",
    "    \n",
    "    lines = output.strip().split('\\n')\n",
    "    options = []\n",
    "    \n",
    "    for line in lines[:num_options]:\n",
    "        clean = line.strip()\n",
    "        for prefix in ['1.', '2.', '3.', '4.', '-', 'â€¢']:\n",
    "            if clean.startswith(prefix):\n",
    "                clean = clean[len(prefix):].strip()\n",
    "        if clean:\n",
    "            options.append(clean)\n",
    "    \n",
    "    return options[:num_options]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution: Safety-Focused Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_option(option, problem, criterion=\"progress\"):\n",
    "    \"\"\"\n",
    "    SOLUTION: Improved safety criterion prompt.\n",
    "    \"\"\"\n",
    "    if criterion == \"safety\":\n",
    "        # Improved prompt focusing on constraint violations\n",
    "        prompt = f\"\"\"{problem}\n",
    "\n",
    "Proposed move: {option}\n",
    "\n",
    "On a scale of 0-10, how SAFE is this move?\n",
    "\n",
    "Consider:\n",
    "- Will the fox be left alone with the chicken? (unsafe)\n",
    "- Will the chicken be left alone with the grain? (unsafe)\n",
    "- Does this respect the boat capacity constraint?\n",
    "\n",
    "Score: 10 = completely safe, 0 = violates constraints\n",
    "\n",
    "Score (0-10):\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"{problem}\n",
    "\n",
    "Proposed move: {option}\n",
    "\n",
    "On a scale of 0-10, how promising is this move?\n",
    "Consider: Does it make progress? Does it violate constraints?\n",
    "\n",
    "Score (0-10):\"\"\"\n",
    "    \n",
    "    output = generate_text(prompt, temperature=0, max_tokens=50)\n",
    "    \n",
    "    match = re.search(r'(\\d+)', output)\n",
    "    if match:\n",
    "        score = int(match.group(1))\n",
    "        return min(max(score, 0), 10)\n",
    "    return 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions Answered\n",
    "\n",
    "**1. How many branches were pruned (score < 5)?**\n",
    "\n",
    "Typically 30-50% of branches. Each pruned branch saves:\n",
    "- 1 LLM call to generate next steps\n",
    "- N LLM calls to evaluate those steps (N = branch_factor)\n",
    "- All recursive exploration from that branch\n",
    "\n",
    "Pruning is essential for computational efficiency.\n",
    "\n",
    "**2. Compare \"progress\" vs \"safety\" criteria solutions.**\n",
    "\n",
    "- **Progress criterion**: May try risky moves that advance toward goal\n",
    "- **Safety criterion**: Prioritizes avoiding constraint violations\n",
    "- For this problem: Safety criterion often finds correct solution faster because it immediately rejects dangerous moves\n",
    "\n",
    "**3. Why use separate functions instead of one big script?**\n",
    "\n",
    "Benefits:\n",
    "- **Testing**: Can test `generate_next_steps()` and `evaluate_option()` independently\n",
    "- **Swapping**: Easy to change evaluation criteria by modifying one function\n",
    "- **Reusability**: Functions work for different problems with similar structure\n",
    "- **Debugging**: If evaluation is wrong, you know where to look\n",
    "- **Collaboration**: Different people can work on different functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Chain Prompting - SOLUTION\n",
    "\n",
    "Improving Stage 3 to create more detailed response strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution: Improved Stage 3 Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_response_strategy(sentiment_analysis, extracted_info):\n",
    "    \"\"\"\n",
    "    SOLUTION: More detailed strategy planning prompt.\n",
    "    \n",
    "    Improvements:\n",
    "    - Structured into specific sections\n",
    "    - Asks for concrete actions\n",
    "    - Considers customer retention\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Given this sentiment analysis:\n",
    "\n",
    "{sentiment_analysis}\n",
    "\n",
    "And these extracted facts:\n",
    "\n",
    "{extracted_info}\n",
    "\n",
    "Plan a detailed response strategy:\n",
    "\n",
    "1. ACKNOWLEDGMENT:\n",
    "   - Which specific points from the review should be acknowledged?\n",
    "   - What positive aspects should be reinforced?\n",
    "\n",
    "2. APOLOGY (if needed):\n",
    "   - What issues require an apology?\n",
    "   - Specific or general apology?\n",
    "\n",
    "3. SOLUTIONS:\n",
    "   - What concrete actions should be offered?\n",
    "   - Immediate fixes vs long-term improvements?\n",
    "   - Compensation needed?\n",
    "\n",
    "4. NEXT STEPS:\n",
    "   - What should the customer do next?\n",
    "   - Who will follow up and when?\n",
    "\n",
    "5. TONE:\n",
    "   - Based on sentiment, what tone is appropriate?\n",
    "   - How formal/casual should the response be?\n",
    "\n",
    "Strategy:\"\"\"\n",
    "    \n",
    "    return generate_text(prompt, temperature=0, max_tokens=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions Answered\n",
    "\n",
    "**1. What information from Stage 1 was used in Stage 3?**\n",
    "\n",
    "Stage 3 uses:\n",
    "- Issues mentioned (to know what to apologize for)\n",
    "- Positive aspects (to know what to reinforce)\n",
    "- Duration and price (to contextualize offers)\n",
    "\n",
    "Without Stage 1, Stage 3 would miss important details from the original review.\n",
    "\n",
    "**2. Which stage added the most value?**\n",
    "\n",
    "Depends on use case:\n",
    "- **Stage 1**: Prevents information loss from long reviews\n",
    "- **Stage 2**: Ensures response matches customer mood\n",
    "- **Stage 3**: Creates structured, actionable plan (often most valuable)\n",
    "- **Stage 4**: Executes plan with appropriate language\n",
    "\n",
    "**3. Why 4 functions instead of copying prompts 4 times?**\n",
    "\n",
    "With functions:\n",
    "- Change extraction logic once, affects all reviews\n",
    "- Can cache Stage 1 results and try different Stage 3 strategies\n",
    "- Can A/B test Stage 4 without re-running analysis\n",
    "- Can monitor which stage fails and needs improvement\n",
    "- DRY principle: Don't Repeat Yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Output Verification - SOLUTION\n",
    "\n",
    "Improving verification to catch more issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution: More Thorough Verification Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_code(code, requirements):\n",
    "    \"\"\"\n",
    "    SOLUTION: More thorough verification prompt.\n",
    "    \n",
    "    Improvements:\n",
    "    - Specific checklist format\n",
    "    - Checks multiple aspects\n",
    "    - Asks for examples of missing items\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Check if this code meets ALL requirements:\n",
    "\n",
    "CODE:\n",
    "```python\n",
    "{code}\n",
    "```\n",
    "\n",
    "REQUIREMENTS:\n",
    "- Function name must be: {requirements['name']}\n",
    "- Must have parameters: {', '.join(requirements['parameters'])}\n",
    "- Must include: {', '.join(requirements['must_have'])}\n",
    "\n",
    "CHECK EACH ITEM:\n",
    "\n",
    "1. Function name:\n",
    "   - Is it exactly '{requirements['name']}'? (Yes/No)\n",
    "\n",
    "2. Docstring:\n",
    "   - Is there a docstring explaining what the function does? (Yes/No)\n",
    "   - Does it explain parameters and return value? (Yes/No)\n",
    "\n",
    "3. Input validation:\n",
    "   - Does it check if inputs are valid? (Yes/No)\n",
    "   - What happens with negative numbers or invalid types? (Describe)\n",
    "\n",
    "4. Return statement:\n",
    "   - Is there a return statement? (Yes/No)\n",
    "   - Does it return the correct type? (Yes/No)\n",
    "\n",
    "5. Edge cases:\n",
    "   - What happens if discount is 0? If it's 100? If it's over 100?\n",
    "\n",
    "SUMMARY:\n",
    "List all issues found (or write 'No issues'):\"\"\"\n",
    "    \n",
    "    return generate_text(prompt, temperature=0, max_tokens=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions Answered\n",
    "\n",
    "**1. What issues did verification catch?**\n",
    "\n",
    "Common issues:\n",
    "- Missing or incomplete docstrings\n",
    "- No input validation (accepting negative prices)\n",
    "- No handling of edge cases (discount > 100%)\n",
    "- Unclear variable names\n",
    "- Missing type hints\n",
    "\n",
    "**2. How would you catch these with code-based checks?**\n",
    "\n",
    "```python\n",
    "# Code-based checks\n",
    "import ast\n",
    "\n",
    "def verify_code_programmatically(code, requirements):\n",
    "    tree = ast.parse(code)\n",
    "    \n",
    "    # Check function exists\n",
    "    functions = [n for n in tree.body if isinstance(n, ast.FunctionDef)]\n",
    "    assert functions[0].name == requirements['name']\n",
    "    \n",
    "    # Check docstring\n",
    "    assert ast.get_docstring(functions[0]) is not None\n",
    "    \n",
    "    # Check return statement exists\n",
    "    has_return = any(isinstance(n, ast.Return) \n",
    "                     for n in ast.walk(functions[0]))\n",
    "    assert has_return\n",
    "```\n",
    "\n",
    "**3. Why separate verify and correct functions?**\n",
    "\n",
    "Separation allows:\n",
    "- **Iteration**: Can run verify â†’ correct â†’ verify â†’ correct multiple times\n",
    "- **Logging**: Track what issues were found and how many iterations needed\n",
    "- **Human-in-loop**: Show verification results to human before correcting\n",
    "- **Different correctors**: Could use different models or strategies for correction\n",
    "- **Exit criteria**: Stop when verification passes, regardless of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Self-Reflection - SOLUTION\n",
    "\n",
    "Better reflection prompts ask more critical questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution: Improved Self-Reflection Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_reflect(problem, initial_reasoning):\n",
    "    \"\"\"\n",
    "    SOLUTION: More critical reflection questions.\n",
    "    \n",
    "    Improvements:\n",
    "    - Specific calculation checks\n",
    "    - Alternative perspective questions\n",
    "    - Assumption challenges\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"{problem}\n",
    "\n",
    "Here was my initial reasoning:\n",
    "{initial_reasoning}\n",
    "\n",
    "Now critically examine this reasoning:\n",
    "\n",
    "1. CALCULATION CHECK:\n",
    "   - Are all arithmetic operations correct?\n",
    "   - Did I account for all costs (revenue - costs)?\n",
    "   - Double-check: For Strategy A: 10,000 customers Ã— $10 = ?\n",
    "   - Double-check: For Strategy B: 5,000 customers Ã— $25 = ?\n",
    "\n",
    "2. COSTS:\n",
    "   - Did I subtract support costs?\n",
    "   - Support cost = customers Ã— $2/month\n",
    "   - Strategy A support: 10,000 Ã— $2 = ?\n",
    "   - Strategy B support: 5,000 Ã— $2 = ?\n",
    "\n",
    "3. ASSUMPTIONS:\n",
    "   - Am I assuming customer counts are guaranteed?\n",
    "   - What if projections are wrong?\n",
    "   - What other factors matter (churn, acquisition cost, lifetime value)?\n",
    "\n",
    "4. ALTERNATIVE VIEWS:\n",
    "   - Would I reach a different conclusion if I focused on profit margin?\n",
    "   - What about long-term vs short-term?\n",
    "\n",
    "5. MISSED FACTORS:\n",
    "   - What did I not consider at all?\n",
    "\n",
    "Critical reflection:\"\"\"\n",
    "    \n",
    "    return generate_text(prompt, temperature=0, max_tokens=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions Answered\n",
    "\n",
    "**1. What did reflection catch that initial reasoning missed?**\n",
    "\n",
    "Common mistakes caught:\n",
    "- Forgot to subtract support costs\n",
    "- Calculated revenue but not profit\n",
    "- Arithmetic errors in multiplication\n",
    "- Missing considerations (churn rate, scalability)\n",
    "\n",
    "**2. Compare with vs without reflection stage.**\n",
    "\n",
    "**Without reflection**:\n",
    "- Often makes calculation errors\n",
    "- May recommend wrong strategy\n",
    "- Less confident in uncertain areas\n",
    "\n",
    "**With reflection**:\n",
    "- Catches mistakes before finalizing\n",
    "- More thorough analysis\n",
    "- Explicitly states assumptions\n",
    "- Higher quality final answer\n",
    "\n",
    "**3. Why have 4 separate functions? List 3 advantages.**\n",
    "\n",
    "1. **Debugging**: If final answer is wrong, inspect each stage to find where reasoning failed\n",
    "\n",
    "2. **Iteration**: Can improve reflection questions without changing initial reasoning or final answer formatting\n",
    "\n",
    "3. **Measurement**: Can quantify how often reflection changes the answer (initial vs revised reasoning)\n",
    "\n",
    "4. **Reusability**: Same pipeline works for different decision problems\n",
    "\n",
    "5. **Caching**: Can cache initial reasoning and try different reflection strategies\n",
    "\n",
    "6. **Testing**: Each function can be unit tested with known inputs/outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Tree-of-Thought**: Specific prompts generate better options; pruning saves costs\n",
    "2. **Chain Prompting**: Structured stages prevent information loss and enable debugging\n",
    "3. **Verification**: Detailed checklists catch more issues than general requests\n",
    "4. **Self-Reflection**: Specific critical questions catch mistakes better than general \"check your work\"\n",
    "5. **Modular Design**: Functions enable testing, reuse, iteration, and measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to Use Each Technique\n",
    "\n",
    "| Technique | Best For | Cost | Complexity |\n",
    "|-----------|----------|------|------------|\n",
    "| **Tree-of-Thought** | Strategic problems with multiple valid paths | High (many LLM calls) | High |\n",
    "| **Chain Prompting** | Complex multi-step tasks requiring different expertise | Medium (N sequential calls) | Medium |\n",
    "| **Output Verification** | When correctness is critical (code, calculations) | Medium (generate + verify + correct) | Low |\n",
    "| **Self-Reflection** | High-stakes decisions, counter-intuitive problems | Medium (2-4x single call) | Low |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
