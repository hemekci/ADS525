{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Prompt Engineering - Medium Tasks (Solutions)\n",
    "\n",
    "Complete solutions for all Medium Tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions Summary\n",
    "\n",
    "### Task 1: Prompt Builder\n",
    "\n",
    "**Key insights:**\n",
    "- All code provided is working\n",
    "- Students experiment with different component combinations\n",
    "- Format and Audience typically have the biggest impact\n",
    "- Different scenarios benefit from different components\n",
    "\n",
    "**Example prompts created:**\n",
    "- Customer service: All 7 components for structured, empathetic responses\n",
    "- Technical documentation: Persona, Instruction, Audience, Format, Tone\n",
    "- Creative tasks: Fewer components for flexibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Self-Consistency\n",
    "\n",
    "**How it works:**\n",
    "1. Sample multiple reasoning paths (temperature > 0)\n",
    "2. Extract answer from each path\n",
    "3. Take majority vote\n",
    "\n",
    "**Results:**\n",
    "- Simple problems: All samples typically agree\n",
    "- Tricky problems (bat and ball): Self-consistency catches common mistakes\n",
    "- More samples = more reliable, but more expensive\n",
    "\n",
    "**Trade-offs:**\n",
    "- Pro: Higher accuracy on complex problems\n",
    "- Con: N times more expensive (N samples)\n",
    "- Con: Slower response time\n",
    "\n",
    "**When to use:**\n",
    "- High-stakes decisions\n",
    "- Counter-intuitive problems\n",
    "- When single-path accuracy is insufficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Constrained JSON Output\n",
    "\n",
    "**Key technique:**\n",
    "```python\n",
    "output = llm.create_chat_completion(\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    response_format={\"type\": \"json_object\"},\n",
    "    temperature=0,\n",
    "    max_tokens=300\n",
    ")\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- Guaranteed valid JSON (no parsing errors)\n",
    "- Reliable integration with applications\n",
    "- Consistent structure\n",
    "\n",
    "**Schema validation:**\n",
    "```python\n",
    "def validate_schema(data, required_fields):\n",
    "    missing = []\n",
    "    for field in required_fields:\n",
    "        if field not in data:\n",
    "            missing.append(field)\n",
    "    if missing:\n",
    "        return False, f\"Missing fields: {', '.join(missing)}\"\n",
    "    return True, \"Valid schema\"\n",
    "```\n",
    "\n",
    "**When to use:**\n",
    "- Building applications that consume LLM output\n",
    "- When structure is critical\n",
    "- API integrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Prompt Optimization\n",
    "\n",
    "**Systematic approach:**\n",
    "1. Start with baseline\n",
    "2. Add definitions\n",
    "3. Add examples\n",
    "4. Measure accuracy on consistent test set\n",
    "\n",
    "**Results:**\n",
    "- V1 (basic): Baseline accuracy\n",
    "- V2 (definitions): Usually 10-20% improvement\n",
    "- V3 (examples): Another 5-15% improvement\n",
    "\n",
    "**What helps most:**\n",
    "- Definitions: Clarify boundaries between categories\n",
    "- Examples: Show the desired format and edge cases\n",
    "- Both together: Best results\n",
    "\n",
    "**Iterative improvement:**\n",
    "- Identify failing cases\n",
    "- Add examples that cover those cases\n",
    "- Refine definitions\n",
    "- Test again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions Answered\n",
    "\n",
    "### Task 1\n",
    "\n",
    "1. **Which component had the biggest impact on output quality?**\n",
    "   - Format and Audience typically have the most impact\n",
    "   - Format: Controls structure\n",
    "   - Audience: Affects language level and complexity\n",
    "\n",
    "2. **How did adding audience change the language used?**\n",
    "   - Simpler vocabulary for younger audiences\n",
    "   - More technical language for expert audiences\n",
    "   - Different tone and formality levels\n",
    "\n",
    "3. **When would you intentionally omit certain components?**\n",
    "   - Simple tasks: Instruction may be enough\n",
    "   - Creative tasks: Too much structure limits creativity\n",
    "   - Token budget: Remove least impactful components\n",
    "\n",
    "### Task 2\n",
    "\n",
    "1. **Which problem benefited most from self-consistency?**\n",
    "   - Counter-intuitive problems (bat and ball)\n",
    "   - Problems where single reasoning often makes errors\n",
    "   - Complex multi-step calculations\n",
    "\n",
    "2. **Did all samples agree? What does disagreement tell you?**\n",
    "   - Simple problems: High agreement\n",
    "   - Complex problems: More disagreement\n",
    "   - Disagreement indicates problem difficulty or ambiguity\n",
    "\n",
    "3. **What is the trade-off of using more samples?**\n",
    "   - More reliable results\n",
    "   - Higher cost (N times more expensive)\n",
    "   - Slower response time\n",
    "\n",
    "### Task 3\n",
    "\n",
    "1. **Why is guaranteed JSON output important for applications?**\n",
    "   - Eliminates parsing errors\n",
    "   - Reliable integration\n",
    "   - Predictable structure\n",
    "   - Production-ready\n",
    "\n",
    "2. **What happens if you request fields the model cannot infer from the prompt?**\n",
    "   - May generate placeholder values\n",
    "   - May omit the field\n",
    "   - May hallucinate reasonable-sounding values\n",
    "\n",
    "3. **How would you handle optional vs required fields?**\n",
    "   - Validate required fields with schema checker\n",
    "   - Check optional fields separately\n",
    "   - Provide defaults for missing optional fields\n",
    "\n",
    "### Task 4\n",
    "\n",
    "1. **Which improvement (definitions or examples) had a bigger impact?**\n",
    "   - Usually definitions have the biggest single impact\n",
    "   - Examples provide additional improvement\n",
    "   - Both together are best\n",
    "\n",
    "2. **Are there tickets that all versions got wrong? What makes them difficult?**\n",
    "   - Borderline cases between categories\n",
    "   - Context-dependent urgency\n",
    "   - Ambiguous wording\n",
    "\n",
    "3. **What would you try next to improve further?**\n",
    "   - Add more diverse examples\n",
    "   - Refine definitions based on failures\n",
    "   - Add Chain-of-Thought reasoning\n",
    "   - Use self-consistency for difficult cases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
