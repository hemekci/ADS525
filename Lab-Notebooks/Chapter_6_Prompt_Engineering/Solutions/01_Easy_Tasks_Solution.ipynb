{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Prompt Engineering - Easy Tasks (Solutions)\n",
    "\n",
    "Complete solutions for all Easy Tasks with filled-in answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run all cells in this section to set up the environment and load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=False,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, temperature=0.7, max_tokens=200):\n",
    "    \"\"\"Generate text with specified parameters\"\"\"\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        return_full_text=False,\n",
    "        max_new_tokens=max_tokens,\n",
    "        do_sample=True if temperature > 0 else False,\n",
    "        temperature=temperature if temperature > 0 else None,\n",
    "    )\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    output = pipe(messages)\n",
    "    return output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Finding the Right Temperature - SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1a Solution: Best Temperature Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Filled in based on experimentation\n",
    "best_temp_factual = 0.0  # For \"What is the capital of France?\" - need deterministic answer\n",
    "best_temp_creative = 1.0  # For \"Write the first sentence...\" - want variety\n",
    "best_temp_code = 0.0  # For \"Write a Python function...\" - need correct syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing selections:\")\n",
    "\n",
    "if best_temp_factual is not None:\n",
    "    output = generate_text(\"What is the capital of France?\", temperature=best_temp_factual, max_tokens=30)\n",
    "    print(f\"\\nFactual (temp={best_temp_factual}): {output}\")\n",
    "\n",
    "if best_temp_creative is not None:\n",
    "    output = generate_text(\"Write the first sentence of a mystery novel.\", temperature=best_temp_creative, max_tokens=50)\n",
    "    print(f\"\\nCreative (temp={best_temp_creative}): {output}\")\n",
    "\n",
    "if best_temp_code is not None:\n",
    "    output = generate_text(\"Write a Python function to calculate factorial.\", temperature=best_temp_code, max_tokens=100)\n",
    "    print(f\"\\nCode (temp={best_temp_code}): {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions Answered\n",
    "\n",
    "**1. At temperature=1.5, did the factual question give wrong answers?**\n",
    "\n",
    "Sometimes yes. High temperature can produce creative but incorrect answers. Determinism is critical for factual tasks because we want the same correct answer every time.\n",
    "\n",
    "**2. For creative writing, compare outputs at temperature=0.3 vs 1.0.**\n",
    "\n",
    "Temperature 0.3 produces safer, more predictable sentences. Temperature 1.0 produces more interesting variations with unexpected word choices.\n",
    "\n",
    "**3. Did code generation at temperature=1.5 produce valid Python?**\n",
    "\n",
    "Often no. High temperature can produce syntax errors or logical mistakes. The risk is broken code that won't run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Building a Complete Prompt - SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2a Solution: Complete Version 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: All 7 components filled in\n",
    "prompt_v5 = \"\"\"Persona: You are a patient barista trainer with 10 years of coffee-making experience.\n",
    "\n",
    "Explain how to make coffee.\n",
    "\n",
    "Context: This person just got their first coffee machine and wants to make their first cup at home.\n",
    "\n",
    "Audience: Someone who has never made coffee before.\n",
    "\n",
    "Format:\n",
    "1. Equipment needed\n",
    "2. Step-by-step instructions\n",
    "3. Common mistakes\n",
    "\n",
    "Tone: Friendly and encouraging.\n",
    "\n",
    "Data: Use a 1:16 coffee-to-water ratio. For one cup, use 15g coffee and 240ml water. Water temperature should be 195-205°F.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"V5: All 7 components\")\n",
    "output = generate_text(prompt_v5, temperature=0, max_tokens=250)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions Answered\n",
    "\n",
    "**1. Compare V1 and V2 outputs. How did specifying Audience change the language complexity?**\n",
    "\n",
    "V2 used simpler vocabulary, shorter sentences, and avoided jargon. The explanation became more accessible to beginners.\n",
    "\n",
    "**2. Which component made the biggest single improvement to output quality?**\n",
    "\n",
    "Format typically has the biggest impact because it structures the entire response. Audience is second because it controls language level.\n",
    "\n",
    "**3. When might you intentionally use fewer components?**\n",
    "\n",
    "- Quick exploratory tasks where V1 is sufficient\n",
    "- Creative tasks where too much structure limits creativity\n",
    "- Token budget constraints in production systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Improving Few-Shot Examples - SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 Solution: Better Few-Shot Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_greetings = [\n",
    "    \"Good morning, how may I assist you?\",\n",
    "    \"Hey, what's up?\",\n",
    "    \"Hello, nice to meet you.\",\n",
    "    \"Hi there.\",\n",
    "    \"Dear valued customer,\",\n",
    "    \"Yo!\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Few-shot classification:\")\n",
    "few_results = {}\n",
    "\n",
    "for greeting in test_greetings:\n",
    "    # SOLUTION: Added two more examples to cover edge cases\n",
    "    prompt = f\"\"\"Classify formality: formal, neutral, or casual.\n",
    "\n",
    "Examples:\n",
    "\n",
    "Greeting: Dear Sir or Madam\n",
    "Formality: formal\n",
    "\n",
    "Greeting: Yo dude\n",
    "Formality: casual\n",
    "\n",
    "Greeting: Hello, how are you\n",
    "Formality: neutral\n",
    "\n",
    "Greeting: Good evening, I hope you are well\n",
    "Formality: formal\n",
    "\n",
    "Greeting: Hey there\n",
    "Formality: casual\n",
    "\n",
    "Greeting: {greeting}\n",
    "Formality:\"\"\"\n",
    "    \n",
    "    result = generate_text(prompt, temperature=0, max_tokens=10).strip()\n",
    "    few_results[greeting] = result\n",
    "    print(f\"{greeting} -> {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions Answered\n",
    "\n",
    "**1. Which greeting showed the biggest difference between zero-shot and few-shot?**\n",
    "\n",
    "\"Hi there\" and \"Dear valued customer\" are ambiguous. Few-shot examples help clarify boundaries by showing similar examples.\n",
    "\n",
    "**2. Did adding more examples improve accuracy on edge cases?**\n",
    "\n",
    "Yes. Adding \"Hey there\" (casual) and \"Good evening, I hope you are well\" (formal) helped with borderline cases.\n",
    "\n",
    "**3. What makes a good few-shot example?**\n",
    "\n",
    "- Clear, unambiguous instances\n",
    "- Cover the full spectrum (formal, neutral, casual)\n",
    "- Include edge cases similar to what you expect\n",
    "- Consistent formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Testing Chain-of-Thought - SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 Solution: Better CoT Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = [\n",
    "    (\"If John has 5 apples and gives 2 to Mary, how many does he have?\", 3, \"easy\"),\n",
    "    (\"A ticket costs $15. I buy 3 tickets with a $50 bill. How much change?\", 5, \"easy\"),\n",
    "    (\"A bat and ball cost $1.10 total. The bat costs $1 more than the ball. How much is the ball?\", 0.05, \"tricky\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Few-shot CoT:\")\n",
    "\n",
    "for question, correct, difficulty in problems:\n",
    "    # SOLUTION: Added a third example showing careful algebra\n",
    "    prompt = f\"\"\"Solve step-by-step.\n",
    "\n",
    "Q: Roger has 5 balls. He buys 2 cans with 3 balls each. How many balls does he have?\n",
    "A: Roger starts with 5 balls.\n",
    "He buys 2 cans, each has 3 balls.\n",
    "New balls: 2 × 3 = 6\n",
    "Total: 5 + 6 = 11\n",
    "Answer: 11\n",
    "\n",
    "Q: A cafe had 23 apples. They used 20 for lunch and bought 6 more. How many now?\n",
    "A: Start with 23 apples.\n",
    "After using 20: 23 - 20 = 3\n",
    "After buying 6: 3 + 6 = 9\n",
    "Answer: 9\n",
    "\n",
    "Q: A pen and a notebook together cost $3. The notebook costs $2 more than the pen. How much is the pen?\n",
    "A: Let pen cost = x\n",
    "Then notebook cost = x + 2\n",
    "Together: x + (x + 2) = 3\n",
    "Simplify: 2x + 2 = 3\n",
    "Subtract 2: 2x = 1\n",
    "Divide by 2: x = 0.50\n",
    "Answer: The pen costs $0.50\n",
    "\n",
    "Q: {question}\n",
    "A:\"\"\"\n",
    "    \n",
    "    answer = generate_text(prompt, temperature=0, max_tokens=150)\n",
    "    \n",
    "    print(f\"\\n[{difficulty.upper()}] {question}\")\n",
    "    print(f\"Reasoning: {answer}\")\n",
    "    print(f\"Correct: {correct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions Answered\n",
    "\n",
    "**1. Did direct prompting get the bat-and-ball problem wrong?**\n",
    "\n",
    "Often yes. The common wrong answer is $0.10. People intuitively think bat=$1.00, ball=$0.10, but that makes the bat only $0.90 more, not $1.00 more.\n",
    "\n",
    "**2. Compare few-shot CoT vs zero-shot CoT on the tricky problem.**\n",
    "\n",
    "Few-shot CoT with the algebra example catches the mistake better because it shows how to set up equations carefully. The third example demonstrates the exact pattern needed.\n",
    "\n",
    "**3. What type of problems benefit most from CoT?**\n",
    "\n",
    "- Multi-step calculations\n",
    "- Counter-intuitive problems\n",
    "- Problems where the obvious answer is wrong\n",
    "- When you need to verify reasoning\n",
    "\n",
    "Direct prompting is fine for simple lookups or single-step problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Temperature**: Use 0 for factual/code, 0.7-1.0 for creative tasks\n",
    "2. **Prompt Components**: Format and Audience have the biggest impact\n",
    "3. **Few-Shot Learning**: Good examples cover the spectrum and include edge cases\n",
    "4. **Chain-of-Thought**: Essential for multi-step and counter-intuitive problems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
