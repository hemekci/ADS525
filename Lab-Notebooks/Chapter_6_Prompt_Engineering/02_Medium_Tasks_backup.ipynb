{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Prompt Engineering - Medium Tasks\n",
    "\n",
    "This notebook covers advanced prompt engineering: structured prompt building, self-consistency, constrained output, and systematic optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run all cells in this section to set up the environment and load the model.\n",
    "\n",
    "Before running these cells, review the concepts from the main Chapter 6 notebook (00_Start_Here.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional] - Installing Packages on Google Colab",
    "",
    "If you are viewing this notebook on Google Colab, uncomment and run the following code to install dependencies.",
    "",
    "**Note**: Use a GPU for this notebook. In Google Colab, go to Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# %%capture\n# !pip install --upgrade transformers>=4.40.0 torch accelerate\n# !pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "source": "import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "model_path = \"microsoft/Phi-3-mini-4k-instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    device_map=\"cuda\",\n    torch_dtype=\"auto\",\n    trust_remote_code=False,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, temperature=0.7, max_tokens=300):\n",
    "    \"\"\"Generate text with specified parameters\"\"\"\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        return_full_text=False,\n",
    "        max_new_tokens=max_tokens,\n",
    "        do_sample=True if temperature > 0 else False,\n",
    "        temperature=temperature if temperature > 0 else None,\n",
    "    )\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    output = pipe(messages)\n",
    "    return output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "\n",
    "Complete the following tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Level: Medium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About This Task:**\n",
    "Building production prompts requires assembling components dynamically based on the use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Medium Task 1: Prompt Builder\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Run the PromptBuilder to see how components are assembled\n",
    "2. Create different prompts by setting different components\n",
    "3. Test removing individual components to see their impact\n",
    "4. Build prompts for different scenarios (instructions, explanations, translations)\n",
    "5. Compare outputs with different component combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptBuilder:\n",
    "    \"\"\"Build structured prompts with all 7 components\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.persona = None\n",
    "        self.instruction = None\n",
    "        self.context = None\n",
    "        self.format_spec = None\n",
    "        self.audience = None\n",
    "        self.tone = None\n",
    "        self.data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def set_persona(self, persona):\n",
    "        self.persona = persona\n",
    "        return self\n",
    "    \n",
    "    def set_instruction(self, instruction):\n",
    "        self.instruction = instruction\n",
    "        return self\n",
    "    \n",
    "    def set_context(self, context):\n",
    "        self.context = context\n",
    "        return self\n",
    "    \n",
    "    def set_format(self, format_spec):\n",
    "        self.format_spec = format_spec\n",
    "        return self\n",
    "    \n",
    "    def set_audience(self, audience):\n",
    "        self.audience = audience\n",
    "        return self\n",
    "    \n",
    "    def set_tone(self, tone):\n",
    "        self.tone = tone\n",
    "        return self\n",
    "    \n",
    "    def set_data(self, data):\n",
    "        self.data = data\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def build(self):\n",
    "        \"\"\"Assemble the complete prompt\"\"\"\n",
    "        parts = []\n",
    "        \n",
    "        if self.persona:\n",
    "            parts.append(f\"You are {self.persona}.\")\n",
    "        \n",
    "        if self.instruction:\n",
    "            parts.append(f\"\\nYour task: {self.instruction}\")\n",
    "        \n",
    "        if self.context:\n",
    "            parts.append(f\"\\nContext: {self.context}\")\n",
    "        \n",
    "        if self.format_spec:\n",
    "            parts.append(f\"\\nFormat:\\n{self.format_spec}\")\n",
    "        \n",
    "        if self.audience:\n",
    "            parts.append(f\"\\nAudience: {self.audience}\")\n",
    "        \n",
    "        if self.tone:\n",
    "            parts.append(f\"\\nTone: {self.tone}\")\n",
    "        \n",
    "        if self.data:\n",
    "            parts.append(f\"\\nData to work with:\\n{self.data}\")\n",
    "        \n",
    "        return \"\".join(parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to add the build method to the class. Run this to fix that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PromptBuilder.build = build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 1: Responding to a complaint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaint_prompt = PromptBuilder() \\\n",
    "    .set_persona(\"a helpful customer service representative\") \\\n",
    "    .set_instruction(\"Respond to this customer complaint\") \\\n",
    "    .set_context(\"The customer has been waiting 2 weeks for a refund\") \\\n",
    "    .set_format(\"1. Acknowledge\\n2. Apologize\\n3. Solution\") \\\n",
    "    .set_audience(\"A frustrated customer\") \\\n",
    "    .set_tone(\"Professional and empathetic\") \\\n",
    "    .set_data(\"Customer: Sarah\\nOrder: 12345\\nAmount: $89.99\") \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(complaint_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = generate_text(complaint_prompt, temperature=0, max_tokens=200)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2: Explaining a concept\n",
    "\n",
    "Your task: Build a prompt for explaining how email works to someone unfamiliar with technology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_prompt = PromptBuilder() \\\n",
    "    .set_persona(\"a patient technology teacher\") \\\n",
    "    .set_instruction(\"Explain how email works\") \\\n",
    "    .set_context(\"The person has never used email before\") \\\n",
    "    .set_format(\"1. What it is\\n2. How to use it\\n3. Simple analogy\") \\\n",
    "    .set_audience(\"Someone unfamiliar with technology\") \\\n",
    "    .set_tone(\"Simple and encouraging\") \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(explanation_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = generate_text(explanation_prompt, temperature=0, max_tokens=200)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1b: Component Impact Test\n",
    "\n",
    "Test what happens when you remove different components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_task = \"Explain what a computer virus is\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 1: Instruction only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = PromptBuilder().set_instruction(base_task).build()\n",
    "print(\"Version 1 (instruction only):\")\n",
    "print(generate_text(v1, temperature=0, max_tokens=150))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 2: Add audience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2 = PromptBuilder() \\\n",
    "    .set_instruction(base_task) \\\n",
    "    .set_audience(\"A 12-year-old student\") \\\n",
    "    .build()\n",
    "print(\"\\nVersion 2 (with audience):\")\n",
    "print(generate_text(v2, temperature=0, max_tokens=150))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 3: Add format\n",
    "\n",
    "Your task: Add a format specification to structure the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v3 = PromptBuilder() \\\n",
    "    .set_instruction(base_task) \\\n",
    "    .set_audience(\"A 12-year-old student\") \\\n",
    "    .set_format(\"1. What it is\\n2. How it spreads\\n3. How to stay safe\") \\\n",
    "    .build()\n",
    "print(\"\\nVersion 3 (with format):\")\n",
    "print(generate_text(v3, temperature=0, max_tokens=200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1. Which component had the biggest impact on output quality?\n",
    "\n",
    "2. How did adding audience change the language used?\n",
    "\n",
    "3. When would you intentionally omit certain components?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About This Task:**\n",
    "Self-consistency improves reliability by sampling multiple reasoning paths and taking the majority answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Medium Task 2: Self-Consistency\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Run single CoT to see baseline performance\n",
    "2. Run self-consistency with 5 samples to see multiple reasoning paths\n",
    "3. Test with different sample counts (3, 5, 10)\n",
    "4. Try different temperature values\n",
    "5. Identify which problems benefit most from multiple samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_problems = [\n",
    "    {\n",
    "        \"problem\": \"A bill is $80. You want to leave a 20% tip. What is the total?\",\n",
    "        \"answer\": 96\n",
    "    },\n",
    "    {\n",
    "        \"problem\": \"If 5 machines make 5 widgets in 5 minutes, how long for 100 machines to make 100 widgets?\",\n",
    "        \"answer\": 5\n",
    "    },\n",
    "    {\n",
    "        \"problem\": \"A bat and ball cost $1.10 total. The bat costs $1 more than the ball. How much is the ball?\",\n",
    "        \"answer\": 0.05\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single CoT Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_cot_solve(problem):\n",
    "    prompt = f\"{problem}\\n\\nLet's think step-by-step:\"\n",
    "    return generate_text(prompt, temperature=0, max_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Single CoT Reasoning:\")\n",
    "for item in test_problems:\n",
    "    problem = item[\"problem\"]\n",
    "    correct = item[\"answer\"]\n",
    "    output = single_cot_solve(problem)\n",
    "    print(f\"\\nProblem: {problem}\")\n",
    "    print(f\"Correct: {correct}\")\n",
    "    print(f\"Reasoning: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(text):\n",
    "    \"\"\"Extract numerical answer from reasoning text\"\"\"\n",
    "    match = re.search(r'answer is\\s+\\$?([0-9.]+)', text.lower())\n",
    "    if match:\n",
    "        return float(match.group(1))\n",
    "    \n",
    "    match = re.search(r'=\\s+\\$?([0-9.]+)', text)\n",
    "    if match:\n",
    "        return float(match.group(1))\n",
    "    \n",
    "    numbers = re.findall(r'\\$?([0-9.]+)', text)\n",
    "    if numbers:\n",
    "        return float(numbers[-1])\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_consistency_solve(problem, num_samples=5, temperature=0.7):\n",
    "    \"\"\"Solve using self-consistency\"\"\"\n",
    "    prompt = f\"{problem}\\n\\nLet's think step-by-step:\"\n",
    "    \n",
    "    reasoning_paths = []\n",
    "    answers = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        output = generate_text(prompt, temperature=temperature, max_tokens=200)\n",
    "        reasoning_paths.append(output)\n",
    "        \n",
    "        answer = extract_answer(output)\n",
    "        if answer is not None:\n",
    "            answers.append(answer)\n",
    "    \n",
    "    if not answers:\n",
    "        return None, reasoning_paths, []\n",
    "    \n",
    "    answer_counts = Counter(answers)\n",
    "    majority_answer = answer_counts.most_common(1)[0][0]\n",
    "    \n",
    "    return majority_answer, reasoning_paths, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Self-Consistency (5 samples):\")\n",
    "for item in test_problems:\n",
    "    problem = item[\"problem\"]\n",
    "    correct = item[\"answer\"]\n",
    "    \n",
    "    majority, paths, all_answers = self_consistency_solve(problem, num_samples=5)\n",
    "    \n",
    "    print(f\"\\nProblem: {problem}\")\n",
    "    print(f\"Correct: {correct}\")\n",
    "    print(f\"All answers: {all_answers}\")\n",
    "    print(f\"Majority: {majority}\")\n",
    "    \n",
    "    if paths:\n",
    "        print(f\"Example reasoning: {paths[0][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2b: Test Different Sample Counts\n",
    "\n",
    "Your task: Try different numbers of samples and see how it affects reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tricky_problem = test_problems[2]\n",
    "sample_counts = [3, 5, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing different sample counts:\")\n",
    "print(f\"Problem: {tricky_problem['problem']}\")\n",
    "print(f\"Correct answer: {tricky_problem['answer']}\")\n",
    "\n",
    "for n in sample_counts:\n",
    "    majority, _, all_answers = self_consistency_solve(tricky_problem[\"problem\"], num_samples=n)\n",
    "    print(f\"\\n{n} samples: {all_answers}\")\n",
    "    print(f\"Majority: {majority}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1. Which problem benefited most from self-consistency?\n",
    "\n",
    "2. Did all samples agree? What does disagreement tell you?\n",
    "\n",
    "3. What is the trade-off of using more samples?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About This Task:**\n",
    "Constrained generation forces the model to output valid JSON, which is critical for applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Medium Task 3: Constrained JSON Output\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Run without constraints to see free-form output\n",
    "2. Apply JSON constraints to guarantee valid output\n",
    "3. Test different data structures (user profiles, products, events)\n",
    "4. Create prompts that request specific JSON schemas\n",
    "5. Handle validation of required fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Llama.from_pretrained(\n",
    "    repo_id=\"microsoft/Phi-3-mini-4k-instruct-gguf\",\n",
    "    filename=\"*fp16.gguf\",\n",
    "    n_gpu_layers=-1,\n",
    "    n_ctx=2048,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Create a user profile for a software engineer:\n",
    "- Name: Alex Johnson\n",
    "- Age: 28\n",
    "- Skills: Python, Machine Learning\n",
    "- Experience: 5 years\n",
    "\n",
    "Return as JSON.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm.create_chat_completion(\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0,\n",
    "    max_tokens=300\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = output['choices'][0]['message']['content']\n",
    "print(\"Without constraints:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    parsed = json.loads(response)\n",
    "    print(\"\\nValid JSON\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"\\nInvalid JSON: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With JSON Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm.create_chat_completion(\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    response_format={\"type\": \"json_object\"},\n",
    "    temperature=0,\n",
    "    max_tokens=300\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = output['choices'][0]['message']['content']\n",
    "print(\"With constraints:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    parsed = json.loads(response)\n",
    "    print(\"\\nValid JSON\")\n",
    "    print(\"\\nFields:\")\n",
    "    for key, value in parsed.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"\\nInvalid JSON: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_schema(data, required_fields):\n",
    "    \"\"\"Check if JSON contains required fields\"\"\"\n",
    "    missing = []\n",
    "    \n",
    "    for field in required_fields:\n",
    "        if field not in data:\n",
    "            missing.append(field)\n",
    "    \n",
    "    if missing:\n",
    "        return False, f\"Missing fields: {', '.join(missing)}\"\n",
    "    return True, \"Valid schema\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_schema = [\"name\", \"age\", \"skills\", \"years_of_experience\"]\n",
    "valid, message = validate_schema(parsed, user_schema)\n",
    "print(f\"\\nSchema validation: {message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3b: Different Data Structures\n",
    "\n",
    "Your task: Create a prompt that generates a product catalog entry as JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_prompt = \"\"\"Create a product catalog entry for a laptop:\n",
    "- Brand: TechPro\n",
    "- Model: UltraBook X1\n",
    "- Price: $1299.99\n",
    "- RAM: 16GB\n",
    "- Storage: 512GB SSD\n",
    "- In stock: Yes\n",
    "\n",
    "Return as JSON.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm.create_chat_completion(\n",
    "    messages=[{\"role\": \"user\", \"content\": product_prompt}],\n",
    "    response_format={\"type\": \"json_object\"},\n",
    "    temperature=0,\n",
    "    max_tokens=300\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = output['choices'][0]['message']['content']\n",
    "parsed = json.loads(response)\n",
    "print(json.dumps(parsed, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1. Why is guaranteed JSON output important for applications?\n",
    "\n",
    "2. What happens if you request fields the model cannot infer from the prompt?\n",
    "\n",
    "3. How would you handle optional vs required fields?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About This Task:**\n",
    "Systematic optimization means testing variations, measuring performance, and iterating to improve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Medium Task 4: Prompt Optimization\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Run baseline prompt to see initial performance\n",
    "2. Create improved versions with definitions and examples\n",
    "3. Test each version on the same evaluation set\n",
    "4. Measure accuracy to quantify improvements\n",
    "5. Analyze which improvements had the biggest impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tickets = [\n",
    "    {\"text\": \"My account has been hacked! Someone is making purchases\", \"urgency\": \"high\"},\n",
    "    {\"text\": \"How do I change my password?\", \"urgency\": \"low\"},\n",
    "    {\"text\": \"I've been trying to log in for 2 hours, site is down\", \"urgency\": \"high\"},\n",
    "    {\"text\": \"What are your business hours?\", \"urgency\": \"low\"},\n",
    "    {\"text\": \"I was charged twice for my order\", \"urgency\": \"medium\"},\n",
    "    {\"text\": \"Can you recommend a product?\", \"urgency\": \"low\"},\n",
    "    {\"text\": \"Payment failing and order deadline is today\", \"urgency\": \"high\"},\n",
    "    {\"text\": \"Update my shipping address\", \"urgency\": \"medium\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 1: Basic Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_urgency_v1(ticket_text):\n",
    "    prompt = f\"\"\"Classify this support ticket urgency as high, medium, or low.\n",
    "\n",
    "Ticket: {ticket_text}\n",
    "Urgency:\"\"\"\n",
    "    \n",
    "    output = generate_text(prompt, temperature=0, max_tokens=10)\n",
    "    return output.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Version 1: Basic prompt\")\n",
    "correct_v1 = 0\n",
    "\n",
    "for ticket in test_tickets:\n",
    "    prediction = classify_urgency_v1(ticket[\"text\"])\n",
    "    correct = ticket[\"urgency\"]\n",
    "    match = prediction == correct\n",
    "    \n",
    "    if match:\n",
    "        correct_v1 += 1\n",
    "    \n",
    "    print(f\"Predicted: {prediction} | Actual: {correct} | {match}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_v1 = correct_v1 / len(test_tickets)\n",
    "print(f\"\\nAccuracy: {accuracy_v1:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 2: Add Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_urgency_v2(ticket_text):\n",
    "    prompt = f\"\"\"Classify this support ticket urgency.\n",
    "\n",
    "Definitions:\n",
    "- high: Security issues, service outages, urgent deadlines\n",
    "- medium: Billing issues, order changes, time-sensitive requests\n",
    "- low: General questions, information requests, non-urgent help\n",
    "\n",
    "Ticket: {ticket_text}\n",
    "Urgency:\"\"\"\n",
    "    \n",
    "    output = generate_text(prompt, temperature=0, max_tokens=10)\n",
    "    return output.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Version 2: With definitions\")\n",
    "correct_v2 = 0\n",
    "\n",
    "for ticket in test_tickets:\n",
    "    prediction = classify_urgency_v2(ticket[\"text\"])\n",
    "    correct = ticket[\"urgency\"]\n",
    "    match = prediction == correct\n",
    "    \n",
    "    if match:\n",
    "        correct_v2 += 1\n",
    "    \n",
    "    print(f\"Predicted: {prediction} | Actual: {correct} | {match}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_v2 = correct_v2 / len(test_tickets)\n",
    "print(f\"\\nAccuracy: {accuracy_v2:.1%}\")\n",
    "print(f\"Improvement: {accuracy_v2 - accuracy_v1:+.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 3: Add Examples\n",
    "\n",
    "Your task: Add few-shot examples to further improve accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_urgency_v3(ticket_text):\n",
    "    prompt = f\"\"\"Classify support ticket urgency.\n",
    "\n",
    "Definitions:\n",
    "- high: Security issues, service outages, urgent deadlines\n",
    "- medium: Billing issues, order changes, time-sensitive requests\n",
    "- low: General questions, information requests, non-urgent help\n",
    "\n",
    "Examples:\n",
    "\n",
    "Ticket: I can't access my account and think it's been compromised\n",
    "Urgency: high\n",
    "\n",
    "Ticket: I was charged for a subscription I cancelled\n",
    "Urgency: medium\n",
    "\n",
    "Ticket: Do you ship internationally?\n",
    "Urgency: low\n",
    "\n",
    "Now classify:\n",
    "Ticket: {ticket_text}\n",
    "Urgency:\"\"\"\n",
    "    \n",
    "    output = generate_text(prompt, temperature=0, max_tokens=10)\n",
    "    return output.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Version 3: With examples\")\n",
    "correct_v3 = 0\n",
    "\n",
    "for ticket in test_tickets:\n",
    "    prediction = classify_urgency_v3(ticket[\"text\"])\n",
    "    correct = ticket[\"urgency\"]\n",
    "    match = prediction == correct\n",
    "    \n",
    "    if match:\n",
    "        correct_v3 += 1\n",
    "    \n",
    "    print(f\"Predicted: {prediction} | Actual: {correct} | {match}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_v3 = correct_v3 / len(test_tickets)\n",
    "print(f\"\\nAccuracy: {accuracy_v3:.1%}\")\n",
    "print(f\"Improvement over v2: {accuracy_v3 - accuracy_v2:+.1%}\")\n",
    "print(f\"Total improvement: {accuracy_v3 - accuracy_v1:+.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final comparison:\")\n",
    "print(f\"Version 1 (basic): {accuracy_v1:.1%}\")\n",
    "print(f\"Version 2 (definitions): {accuracy_v2:.1%}\")\n",
    "print(f\"Version 3 (examples): {accuracy_v3:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1. Which improvement (definitions or examples) had a bigger impact?\n",
    "\n",
    "2. Are there tickets that all versions got wrong? What makes them difficult?\n",
    "\n",
    "3. What would you try next to improve further?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}