{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ETtu9CvVMDR"
   },
   "source": [
    "<h1>Chapter 7 - Easy Tasks</h1>\n",
    "<i>Going beyond prompt engineering.</i>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: llama-cpp-python==0.2.69 in /danielcastillo/Desktop/ADS525/ads525/lib/python3.10/site-packages (0.2.69)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /danielcastillo/Desktop/ADS525/ads525/lib/python3.10/site-packages (from llama-cpp-python==0.2.69) (4.15.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /danielcastillo/Desktop/ADS525/ads525/lib/python3.10/site-packages (from llama-cpp-python==0.2.69) (1.26.4)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /danielcastillo/Desktop/ADS525/ads525/lib/python3.10/site-packages (from llama-cpp-python==0.2.69) (3.1.6)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /danielcastillo/Desktop/ADS525/ads525/lib/python3.10/site-packages (from llama-cpp-python==0.2.69) (5.6.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /danielcastillo/Desktop/ADS525/ads525/lib/python3.10/site-packages (from jinja2>=2.11.3->llama-cpp-python==0.2.69) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain>=0.1.17 openai>=1.13.3 langchain_openai>=0.1.6 transformers>=4.40.1 datasets>=2.18.0 accelerate>=0.27.2 sentence-transformers>=2.5.1 duckduckgo-search>=5.2.2 langchain_community\n",
    "%pip install llama-cpp-python==0.2.69"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rerbJgwAigbK"
   },
   "source": [
    "# Loading the LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the 16-bit variant model from Hugging Face\n",
    "!wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf\n",
    "\n",
    "# If this command does not work for you, you can use the link directly to download the model\n",
    "# https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-cpp-python in /danielcastillo/Desktop/ADS525/ads525/lib/python3.10/site-packages (0.2.69)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /danielcastillo/Desktop/ADS525/ads525/lib/python3.10/site-packages (from llama-cpp-python) (3.1.6)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /danielcastillo/Desktop/ADS525/ads525/lib/python3.10/site-packages (from llama-cpp-python) (1.26.4)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /danielcastillo/Desktop/ADS525/ads525/lib/python3.10/site-packages (from llama-cpp-python) (4.15.0)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /danielcastillo/Desktop/ADS525/ads525/lib/python3.10/site-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /danielcastillo/Desktop/ADS525/ads525/lib/python3.10/site-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp8.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "LQcht_ZFijW7"
   },
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "model_16_bit = LlamaCpp(\n",
    "    model_path=\"Phi-3-mini-4k-instruct-fp16.gguf\",\n",
    "    n_gpu_layers=-1,\n",
    "    max_tokens=500,\n",
    "    n_ctx=2048,\n",
    "    seed=42,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the 4-bit Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's download the 4-bit quantized model using llama.cpp's quantization tool\n",
    "# Make sure the model path is correct for your system!\n",
    "model_4_bit = LlamaCpp(\n",
    "    model_path=\"Phi-3-mini-4k-instruct-q4.gguf\",\n",
    "    n_gpu_layers=-1,\n",
    "    max_tokens=500,\n",
    "    n_ctx=2048,\n",
    "    seed=42,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Performance of Each Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompts = [\n",
    "    \"What is 2 + 2?\",\n",
    "    \"Write a haiku about winter.\",\n",
    "    \"Explain what AI is in one sentence.\",\n",
    "    \"Name three colors.\",\n",
    "    \"What's the capital of France?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on the First Test Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: What is 2 + 2?\n",
      "16-bit Response: \n",
      "<|assistant|> The sum of 2 and 2 is 4.\n",
      "\n",
      "This question falls into the category of basic arithmetic, which has a clear-cut answer that doesn't require opinion or subjective interpretation. It's essential in mathematics to understand such fundamental operations because they form the building blocks for more complex calculations.\n",
      "Time taken: 13.55 seconds\n"
     ]
    }
   ],
   "source": [
    "# Test 16-bit model on first prompt\n",
    "prompt = test_prompts[0]  # \"What is 2 + 2?\"\n",
    "print(f\"Testing: {prompt}\")\n",
    "\n",
    "start_time = time.time()\n",
    "response_16bit_1 = model_16_bit.invoke(prompt)\n",
    "end_time = time.time()\n",
    "time_16bit_1 = end_time - start_time\n",
    "\n",
    "print(f\"16-bit Response: {response_16bit_1}\")\n",
    "print(f\"Time taken: {time_16bit_1:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: What is 2 + 2?\n",
      "4-bit Response: \n",
      "<|assistant|> The sum of 2 and 2 is 4.\n",
      "\n",
      "Here's a brief explanation: When you add the number 2 to another number 2, you get 4 as the result because addition combines quantities together. In mathematical terms:\n",
      "\n",
      "2 + 2 = 4\n",
      "\n",
      "This operation follows the basic principles of arithmetic and applies universally across mathematics.\n",
      "Time taken: 6.99 seconds\n"
     ]
    }
   ],
   "source": [
    "# Test 4-bit model on first prompt\n",
    "prompt = test_prompts[0]  # \"What is 2 + 2?\"\n",
    "print(f\"Testing: {prompt}\")\n",
    "\n",
    "start_time = time.time()\n",
    "response_4bit_1 = model_4_bit.invoke(prompt)\n",
    "end_time = time.time()\n",
    "time_4bit_1 = end_time - start_time\n",
    "\n",
    "print(f\"4-bit Response: {response_4bit_1}\")\n",
    "print(f\"Time taken: {time_4bit_1:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on the Second Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: Write a haiku about winter.\n",
      "16-bit Response: \n",
      "<|assistant|> Snowflakes gently fall,  \n",
      "White blanket covers the earth,  \n",
      "Winter's peace enwraps.\n",
      "Time taken: 6.29 seconds\n"
     ]
    }
   ],
   "source": [
    "# Test 16-bit model on second prompt\n",
    "prompt = test_prompts[1]  # \"Write a haiku about winter.\"\n",
    "print(f\"Testing: {prompt}\")\n",
    "\n",
    "start_time = time.time()\n",
    "response_16bit_2 = model_16_bit.invoke(prompt)\n",
    "end_time = time.time()\n",
    "time_16bit_2 = end_time - start_time\n",
    "\n",
    "print(f\"16-bit Response: {response_16bit_2}\")\n",
    "print(f\"Time taken: {time_16bit_2:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: Write a haiku about winter.\n",
      "4-bit Response: \n",
      "<|assistant|> Snowflakes gently fall,  \n",
      "\n",
      "Winter's silent whispers call,  \n",
      "\n",
      "Blanketing the world in white hush.\n",
      "Time taken: 3.26 seconds\n"
     ]
    }
   ],
   "source": [
    "# Test 4-bit model on second prompt\n",
    "prompt = test_prompts[1]  # \"Write a haiku about winter.\"\n",
    "print(f\"Testing: {prompt}\")\n",
    "\n",
    "start_time = time.time()\n",
    "response_4bit_2 = model_4_bit.invoke(prompt)\n",
    "end_time = time.time()\n",
    "time_4bit_2 = end_time - start_time\n",
    "\n",
    "print(f\"4-bit Response: {response_4bit_2}\")\n",
    "print(f\"Time taken: {time_4bit_2:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on the Third Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: Explain what AI is in one sentence.\n",
      "16-bit Response:  \n",
      "Time taken: 1.36 seconds\n"
     ]
    }
   ],
   "source": [
    "# Test 16-bit model on third prompt\n",
    "prompt = test_prompts[2]  # \"Explain what AI is in one sentence.\"\n",
    "print(f\"Testing: {prompt}\")\n",
    "\n",
    "start_time = time.time()\n",
    "response_16bit_3 = model_16_bit.invoke(prompt)\n",
    "end_time = time.time()\n",
    "time_16bit_3 = end_time - start_time\n",
    "\n",
    "print(f\"16-bit Response: {response_16bit_3}\")\n",
    "print(f\"Time taken: {time_16bit_3:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: Explain what AI is in one sentence.\n",
      "4-bit Response:  <|assistant|> AI, or artificial intelligence, refers to the development of computer systems that can perform tasks and make decisions autonomously, mimicking human cognition and problem-solving abilities.\n",
      "\n",
      "A more detailed explanation: Artificial Intelligence (AI) is a branch of computer science focused on creating machines and software programs capable of performing tasks that typically require human intelligence, such as visual perception, speech recognition, decision-making, and natural language understanding, while exhibiting learning, reasoning, and adaptive behavior. AI encompasses various subfields like machine learning, deep learning, computer vision, natural language processing, robotics, and more, which are designed to enable machines to improve their performance over time through data analysis and experience.\n",
      "Time taken: 13.58 seconds\n"
     ]
    }
   ],
   "source": [
    "# Test 4-bit model on third prompt\n",
    "prompt = test_prompts[2]  # \"Explain what AI is in one sentence.\"\n",
    "print(f\"Testing: {prompt}\")\n",
    "\n",
    "start_time = time.time()\n",
    "response_4bit_3 = model_4_bit.invoke(prompt)\n",
    "end_time = time.time()\n",
    "time_4bit_3 = end_time - start_time\n",
    "\n",
    "print(f\"4-bit Response: {response_4bit_3}\")\n",
    "print(f\"Time taken: {time_4bit_3:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on the Fourth Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: Name three colors.\n",
      "16-bit Response: \n",
      "<|assistant|> 1. Blue 2. Red 3. Yellow\n",
      "<|assistant|> Certainly! Here are three colors:\n",
      "1. Azure - a bright, clear blue similar to the color of a cloudless sky on a sunny day.\n",
      "2. Ruby red - a deep, rich shade of red that is reminiscent of the precious gemstone.\n",
      "3. Lemon yellow - a vivid and energetic shade of yellow, resembling the color of ripe lemons.\n",
      "Time taken: 20.99 seconds\n"
     ]
    }
   ],
   "source": [
    "# Test 16-bit model on fourth prompt\n",
    "\n",
    "prompt = test_prompts[3]  # \"Name three colors.\"\n",
    "print(f\"Testing: {prompt}\")\n",
    "\n",
    "start_time = time.time()\n",
    "response_16bit_4 = model_16_bit.invoke(prompt)\n",
    "end_time = time.time()\n",
    "time_16bit_4 = end_time - start_time\n",
    "\n",
    "print(f\"16-bit Response: {response_16bit_4}\")\n",
    "print(f\"Time taken: {time_16bit_4:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: Name three colors.\n",
      "4-bit Response: \n",
      "<|assistant|> 1. Red: A vibrant and intense color often associated with energy, passion, and love. It is the primary color on many digital devices' screens.\n",
      "\n",
      "2. Blue: Often symbolizing calmness, trust, and stability. It is a popular choice for corporate branding due to its association with reliability.\n",
      "\n",
      "3. Yellow: A cheerful and optimistic hue that can represent happiness, creativity, or caution depending on the shade used (e.g., bright yellow vs. darker tones).\n",
      "Time taken: 10.02 seconds\n"
     ]
    }
   ],
   "source": [
    "# Test 4-bit model on fourth prompt\n",
    "\n",
    "prompt = test_prompts[3]  # \"Name three colors.\"\n",
    "print(f\"Testing: {prompt}\")\n",
    "\n",
    "start_time = time.time()\n",
    "response_4bit_4 = model_4_bit.invoke(prompt)\n",
    "end_time = time.time()\n",
    "time_4bit_4 = end_time - start_time\n",
    "\n",
    "print(f\"4-bit Response: {response_4bit_4}\")\n",
    "print(f\"Time taken: {time_4bit_4:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the Average Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average time taken by 16-bit model: 10.55 seconds\n",
      "Average time taken by 4-bit model: 8.46 seconds\n"
     ]
    }
   ],
   "source": [
    "total_time_16bit = time_16bit_1 + time_16bit_2 + time_16bit_3 + time_16bit_4\n",
    "total_time_4bit = time_4bit_1 + time_4bit_2 + time_4bit_3 + time_4bit_4\n",
    "\n",
    "average_time_16bit = total_time_16bit / 4\n",
    "average_time_4bit = total_time_4bit / 4\n",
    "\n",
    "print(f\"Average time taken by 16-bit model: {average_time_16bit:.2f} seconds\")\n",
    "print(f\"Average time taken by 4-bit model: {average_time_4bit:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "As you can see, while the responses from the 16-bit model are better quality, they take longer to be generated. When choosing the right version, speed and quality are the tradeoffs."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ads525",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}