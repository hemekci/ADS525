{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDe7DsPWmEBV"
      },
      "source": [
        "<h1>Chapter 1 - Introduction to Language Models</h1>\n",
        "<i>Exploring the exciting field of Language AI</i>\n",
        "\n",
        "\n",
        "<a href=\"https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961\"><img src=\"https://img.shields.io/badge/Buy%20the%20Book!-grey?logo=amazon\"></a>\n",
        "<a href=\"https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/\"><img src=\"https://img.shields.io/badge/O'Reilly-white.svg?logo=data:image/svg%2bxml;base64,PHN2ZyB3aWR0aD0iMzQiIGhlaWdodD0iMjciIHZpZXdCb3g9IjAgMCAzNCAyNyIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGNpcmNsZSBjeD0iMTMiIGN5PSIxNCIgcj0iMTEiIHN0cm9rZT0iI0Q0MDEwMSIgc3Ryb2tlLXdpZHRoPSI0Ii8+CjxjaXJjbGUgY3g9IjMwLjUiIGN5PSIzLjUiIHI9IjMuNSIgZmlsbD0iI0Q0MDEwMSIvPgo8L3N2Zz4K\"></a>\n",
        "<a href=\"https://github.com/HandsOnLLM/Hands-On-Large-Language-Models\"><img src=\"https://img.shields.io/badge/GitHub%20Repository-black?logo=github\"></a>\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter01/Chapter%201%20-%20Introduction%20to%20Language%20Models.ipynb)\n",
        "\n",
        "---\n",
        "\n",
        "This notebook is for Chapter 1 of the [Hands-On Large Language Models](https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961) book by [Jay Alammar](https://www.linkedin.com/in/jalammar) and [Maarten Grootendorst](https://www.linkedin.com/in/mgrootendorst/).\n",
        "\n",
        "---\n",
        "\n",
        "<a href=\"https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961\">\n",
        "<img src=\"https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/images/book_cover.png\" width=\"350\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CijioVCqR56"
      },
      "source": [
        "### [OPTIONAL] - Installing Packages on <img src=\"https://colab.google/static/images/icons/colab.png\" width=100>\n",
        "\n",
        "If you are viewing this notebook on Google Colab (or any other cloud vendor), you need to **uncomment and run** the following codeblock to install the dependencies for this chapter:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ve135ClqR57"
      },
      "source": [
        "---\n",
        "\n",
        "ðŸ’¡ **NOTE**: We will want to use a GPU to run the examples in this notebook. In Google Colab, go to\n",
        "**Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4**.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding What We're Building\n",
        "\n",
        "As discussed in Chapter 1, Large Language Models are built on the transformer architecture from the 2017 \"Attention is All You Need\" paper. The model we're using today, **Phi-3**, is a **decoder-only model** - similar to GPT. These models generate text autoregressively, meaning they predict one token at a time based on all previous tokens.\n",
        "\n",
        "Remember from Chapter 1: the \"4k\" in Phi-3-mini-4k-instruct refers to the context window - it can process up to 4,000 tokens at once. This is important because, as we learned, the context length determines how much text the model can \"remember\" during generation."
      ],
      "metadata": {
        "id": "YJBP6BNgVbis"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating Your First Text"
      ],
      "metadata": {
        "id": "lmvSGigartoK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main source for finding and downloading LLMs is the [HuggingFace Hub](https://huggingface.co/docs/hub/en/index)\n",
        "\n",
        "**HuggingFace** is the organization behind the well-known Transformers\n",
        "package, discussed heavily in the slides."
      ],
      "metadata": {
        "id": "w-OVXIf9r2HD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "M5KpcGTdqR58"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers>=4.40.1 accelerate>=0.27.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXp09JFsFBXi"
      },
      "source": [
        "# Phi-3\n",
        "\n",
        "The first step is to load our model onto the GPU for faster inference. Note that we load the model and tokenizer separately (although that isn't always necessary)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RSNalRXZyTTk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 797,
          "referenced_widgets": [
            "cd48d527634a4093924550e1e50e1091",
            "286ae02f232d4ffabb703143057ba9d5",
            "649d32c57e8c4907ba993931a541ce0f",
            "8c4e6b021cba47cf816f4d471c3a77af",
            "f0f09d4bc0974efeb5846eacd6dc633e",
            "98f42b37b27b42c2b1121ec3f38ffd21",
            "8cc86502398a4614b343094d28631cd8",
            "ed1b61ae39ea4a199e60243c3613de8c",
            "88820a7abd804a9bbeb5c3b3fd4970c2",
            "1aa9c979ead341b48970b892e9c90c9d",
            "ade763c3aaac43da8b6742355a9bf9cf",
            "3c4ec728e1e64383ab9d5fbbaa5a69dc",
            "edf07dc66a12414d9b473ccf842f3ad7",
            "fc2764f7b0554599b22b52f38a3ba519",
            "b785e967608840f89f3b08cd548acdb4",
            "652083b86fc24c978c1e6ca109f683cc",
            "56e984bb46164abc980e995e1de22074",
            "07a237b43c8649618a3b66f5579400ab",
            "666f828c46a4433dad643d0a947f7b9c",
            "bf7801f2c8b54fb4b3b980802d18a2f7",
            "9a138f19a92948f3b844d4f5b91c378a",
            "143895ec7d4045d1a6b3869138e92368",
            "7d5b9a690fd04e168dc87526d5f76260",
            "4cfa3c28fc7a4c74ad3d3cbbae2c8a8f",
            "90cf4c8a4eda4767b3341e536f601817",
            "52181d6d93d14961bb16218bb6879e3f",
            "fcc26bb8a50d4d5a94fb3ecd38d4845a",
            "42de763a7de7415a85116c978d36ef7a",
            "76896e4e93d14edf970df8f3d156f513",
            "231cddc449ce415f8c5714535a8c63fc",
            "227f41e0e18b42e6bfe30ebaef6e3eea",
            "8423f16fadd84da4a35c3b5dba008301",
            "73312a45af344dadb7db46540aa9c4c1",
            "650bc6fafdb6484aae553b1b673ebb5a",
            "4d9947f9b3944b04be32a539f434baa6",
            "cf24cf3241874560bca8f215db189081",
            "829a926284cc4a189ea9877ad21f02e5",
            "2b8b08269bac439f8ad0c88b31064d59",
            "90c5754c65134e42a4026c043bf75c98",
            "1265f696f3904922a8754bfebb799122",
            "d9b6662eaa2c4bc48b2014e409e31a43",
            "387f2971fa2f4a039a33096756148304",
            "3aa97a78c8ae47fc897bea740033c7d9",
            "f83ada8553484702b0ed3ee4fac21325",
            "dfa89c167b484b8f91c9171ec778a6aa",
            "43cbdafbfe14436380f21aeddd7ced31",
            "b54c2bbd0a4b4b7eb28d7a1288c7864b",
            "6eaa752c093f45e296e6da38728a0597",
            "8c66d35eff6c47f0b1ccef41b9554744",
            "bd18142cd36e465cb582c03cd9accd50",
            "0b96b2631b5a491e9abd9bed2600797a",
            "a0a4d9b1eade41f7b3f5bb299aed972b",
            "afbad405a84b490ca13728cae3aaf3e9",
            "32f117d1c8e04e63ba85dd0b85b5f42f",
            "6cd7c64e0dab4febadbedd6373d3d0f7",
            "543b6474bdbc4ba2afed7efe82273a2a",
            "2270d09324054233b0c133466f5ec0e2",
            "0175acebcfb94b6b8316548b62241a66",
            "561a78aaaae54ac5be0a701b0866810e",
            "912e9700612a408b8826f523a6670918",
            "e936f63a5d25450db65a51f2b4eca749",
            "92796efc269c4599bcc86edcabdfb16b",
            "a55feddb90de45f6a2ce21e2b019372b",
            "945d1880786043668b79cd07c9ade5cc",
            "ea0876c899d54c91b5a2163f0d5fb9c0",
            "961d55645e7b4249bc2d3d8d7e34b159",
            "c1ad8905808447e0a36bdb9b5de13cdc",
            "3fcfba9b07304a2bba6d851851495b26",
            "1ccc73aeb6a34075bbec9ca7ebcefbaa",
            "37356308064e4134bae2041f104c7fdb",
            "b8a72eec77154d84a1c0a857cefd8476",
            "60c2a4c6ba614ef19994d2cd9d53a4ce",
            "24b6f882abb4475fac34159bcd4b8173",
            "3b54de370e9548d09c62996f787a2105",
            "50062110e02044698324231844021b66",
            "422e63aa8d7546659bf233e3c43e459a",
            "aebd5957748b41719ef884af76b59aaf",
            "773a03956bc64637a43b9171578dcdc3",
            "46662b34bdc440e487e5ff5774ff2d1b",
            "6a342ed5802a4ab784cb80ccab4a62fa",
            "858b7e594eea42c2b89e7097a2f20169",
            "a3422f80f6374fa69cfbe45c7aca72d0",
            "ce148a89b4724345834ca17ffb584ce9",
            "1649ffab68ba43ad9d96aaa5cba3032e",
            "7554883b0c87489ebf6d53cf96d41dfc",
            "b28fd3c9f87340d0aa01abd875a140aa",
            "d626ab32447944688a03b891bdbe4806",
            "d610bcdd12f74c1ea044994573cfee0a",
            "2db16ee431a0405a9d58cbe4bde6011d",
            "86086b753b3b455bb224763529b6d189",
            "86d1f2b63b9741319658b67448f32807",
            "c545b995c9bd4296ba92ef9535d005a8",
            "9992d59580164868861cd8b611d57677",
            "2dced766d67c41cbbb63a7c2fe858502",
            "37a079b11db34a74b830103ce88421e9",
            "91684d1ae12743c8ad931a478ad8dfdc",
            "883c65eb6da74f558ed53d5de28146ef",
            "990cf91b06124f82a065114daac8d179",
            "895c4fd4cc414253ba5662fe7d75f0dc",
            "fc84f9502464459baf1d4e4d26e29437",
            "19d2a20157424d35be926464aa2103b7",
            "ff891afbe1f5439f92d3f72cd0496500",
            "b8ecf22aeda54343805034cfc16d0526",
            "9360669d11ed404680796edac7d49221",
            "2451f50193b741e39b4240792e9b2a7a",
            "72210f88e62f4dcb8d0e41628a7e05f3",
            "448234e1170047cf85357c18b84b109f",
            "02bac9b4aa084b1681d4dcd4a3193d83",
            "d33bd39704874dc29380b7b00fa9b4f3",
            "1fd43cfbf6b94a328359f173595eceb6",
            "8f39d60fe7e445278e215dce4a1b8f0c",
            "f19f5b1e2c3f4f5ea287571a5df95889",
            "89d3919a210d479ca27516ed2ca487ee",
            "0bdf27c1786544a9ab6b09e8ebfcc8d0",
            "ddc4e0917d994d35a9ad29f913e0b874",
            "23d81e769eb94c62bb946c6211029083",
            "b30c4bdc94b14222aebffe96fdcc8680",
            "aa5531e0abf043d4a596524cd9e58ad6",
            "a925e2dc6b3f45d69cf14e16b06c369d",
            "4fc84733151d440d8b5582ed3b00a1c9",
            "e3195ed3dffb4f59abe212683ab8aa3b",
            "270e3bd61193468195462b3ee404ebe8",
            "49285715acd743c2af918798e26e490b",
            "989cdefea96c458a8de77650d64328cb",
            "a6762dc003bc431cb8b340d6a509294d",
            "7f41e447eef54807866c89b5969a6776",
            "b29b496c141e41a9a332e674b104a0b7",
            "72d713cfb96d4d4b885581093e3a044c",
            "b31b3d96dfa64c3090a65164b5a0ddd8",
            "ab7166d76b004d46896f4dda80c7f3cc",
            "f95c7960cfdb4cc5bb7079556f79ba24",
            "ce8349b53fa544dca6ef40d0b69ceccd"
          ]
        },
        "outputId": "036df85b-3c4c-4438-ca84-534b1000bc98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cd48d527634a4093924550e1e50e1091"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c4ec728e1e64383ab9d5fbbaa5a69dc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7d5b9a690fd04e168dc87526d5f76260"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "650bc6fafdb6484aae553b1b673ebb5a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dfa89c167b484b8f91c9171ec778a6aa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "543b6474bdbc4ba2afed7efe82273a2a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c1ad8905808447e0a36bdb9b5de13cdc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "773a03956bc64637a43b9171578dcdc3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2db16ee431a0405a9d58cbe4bde6011d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fc84f9502464459baf1d4e4d26e29437"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f39d60fe7e445278e215dce4a1b8f0c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "270e3bd61193468195462b3ee404ebe8"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=False,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although tokenization will be discussed more comprehensively in Chapter 2, let's have a quick look of how it works:"
      ],
      "metadata": {
        "id": "eW77QK6XWK6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Understanding Tokenization from Chapter 1\n",
        "# We know from Chapter 1 that tokenization is converting text to numbers\n",
        "\n",
        "# Let's see how Phi-3 tokenizes text\n",
        "sample_text = \"I love llamas\"\n",
        "tokens = tokenizer(sample_text)\n",
        "print(f\"Text: {sample_text}\")\n",
        "print(f\"Token IDs: {tokens['input_ids']}\")\n",
        "print(f\"Back to text: {tokenizer.decode(tokens['input_ids'])}\")\n",
        "\n",
        "# Notice how the tokenizer splits text into subword units\n",
        "# This is more sophisticated than the simple word-based tokenization we saw in Chapter 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "JvtN6vDXWF4R",
        "outputId": "d2bd9a79-03aa-464f-c486-aec13d5e71fc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: I love llamas\n",
            "Token IDs: [306, 5360, 11829, 294]\n",
            "Back to text: I love llamas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdyYYS0E5fEU"
      },
      "source": [
        "Although we can now use the model and tokenizer directly, it's much easier to wrap it in a `pipeline` object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DiUi4Wu1FCyN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "c052f4b1-7210-42e1-d831-65380183ff67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nKey generation parameters from Chapter 1:\\n\\n- max_new_tokens: Limits generation length. Remember that GPT-style models\\n  are autoregressive - they generate one token at a time.\\n\\n- do_sample: When False, always picks the most likely next token (greedy).\\n  When True, samples from the probability distribution.\\n\\n- temperature (not set here): Controls randomness. Low values make the model\\n  more focused and deterministic, high values make it more creative.\\n\\n- top_p and top_k: Control nucleus and top-k sampling as alternatives to\\n  temperature-based sampling.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Create a pipeline with hyperparameters we learned about\n",
        "# As discussed in Chapter 1, these control how the model generates text\n",
        "\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False,  # Only return newly generated text\n",
        "    max_new_tokens=500,      # Maximum tokens to generate (remember context windows!)\n",
        "    do_sample=False          # Deterministic output (greedy decoding)\n",
        ")\n",
        "\n",
        "\"\"\"\n",
        "Key generation parameters from Chapter 1:\n",
        "\n",
        "- max_new_tokens: Limits generation length. Remember that GPT-style models\n",
        "  are autoregressive - they generate one token at a time.\n",
        "\n",
        "- do_sample: When False, always picks the most likely next token (greedy).\n",
        "  When True, samples from the probability distribution.\n",
        "\n",
        "- temperature (not set here): Controls randomness. Low values make the model\n",
        "  more focused and deterministic, high values make it more creative.\n",
        "\n",
        "- top_p and top_k: Control nucleus and top-k sampling as alternatives to\n",
        "  temperature-based sampling.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mD49kysT5mMY"
      },
      "source": [
        "Finally, we create our prompt as a user and give it to the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hkR7LBmiyXmY",
        "outputId": "6e3ce597-bb6b-46fb-e31a-17753f28fe0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Why did the chicken join the band? Because it had the drumsticks!\n"
          ]
        }
      ],
      "source": [
        "# The prompt (user input / query)\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Create a funny joke about chickens.\"}\n",
        "]\n",
        "\n",
        "# Generate output\n",
        "output = generator(messages)\n",
        "print(output[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Your Turn: Applying Chapter 1 Concepts\n",
        "\n",
        "Now let's practice the concepts from Chapter 1 with hands-on exercises.\n"
      ],
      "metadata": {
        "id": "Wh9PGmSRu0Pw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 1: Token Vocabulary Exploration\n",
        "Run this to see how different types of text tokenize differently. Try using different characters."
      ],
      "metadata": {
        "id": "A8J-TmVLXjrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 1: Now modify test_texts and add your own examples\n",
        "print(\"Different text types tokenize differently:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "test_texts = {\n",
        "    \"English\": \"The cat sat on the mat\",\n",
        "    \"Code\": \"def fibonacci(n): return n if n <= 1 else fibonacci(n-1) + fibonacci(n-2)\",\n",
        "    \"Numbers\": \"3.14159 2.71828 1.41421\",\n",
        "    \"Mixed\": \"GPT-3 has 175B parameters.\",\n",
        "    \"Special\": \"Hello ä¸–ç•Œ ðŸ¦™ #AI\"\n",
        "}\n",
        "\n",
        "for text_type, text in test_texts.items():\n",
        "    tokens = tokenizer(text)\n",
        "    print(f\"\\n{text_type}:\")\n",
        "    print(f\"  Text: '{text[:50]}{'...' if len(text) > 50 else ''}'\")\n",
        "    print(f\"  Tokens: {len(tokens['input_ids'])}\")\n",
        "    print(f\"  First 5 token IDs: {tokens['input_ids'][:5]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "rGI0cD8JaCQ-",
        "outputId": "b0e740e4-9647-42ff-8a31-e1a3a498f040"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Different text types tokenize differently:\n",
            "============================================================\n",
            "\n",
            "English:\n",
            "  Text: 'The cat sat on the mat'\n",
            "  Tokens: 6\n",
            "  First 5 token IDs: [450, 6635, 3290, 373, 278]\n",
            "\n",
            "Code:\n",
            "  Text: 'def fibonacci(n): return n if n <= 1 else fibonacc...'\n",
            "  Tokens: 32\n",
            "  First 5 token IDs: [822, 18755, 265, 21566, 29898]\n",
            "\n",
            "Numbers:\n",
            "  Text: '3.14159 2.71828 1.41421'\n",
            "  Tokens: 24\n",
            "  First 5 token IDs: [29871, 29941, 29889, 29896, 29946]\n",
            "\n",
            "Mixed:\n",
            "  Text: 'GPT-3 has 175B parameters.'\n",
            "  Tokens: 12\n",
            "  First 5 token IDs: [402, 7982, 29899, 29941, 756]\n",
            "\n",
            "Special:\n",
            "  Text: 'Hello ä¸–ç•Œ ðŸ¦™ #AI'\n",
            "  Tokens: 11\n",
            "  First 5 token IDs: [15043, 29871, 30793, 30967, 29871]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Questions:\n",
        "\n",
        "**Q1)** Why does the code example tokenize differently from plain English text (32 tokens vs 6 tokens)? What does this tell you about how the tokenizer handles programming syntax versus natural language?\n",
        "\n",
        "---\n",
        "\n",
        "**Q2)** How would tokenization differ if you input text in a non-English language like Chinese or Arabic? What effects does this have for model performance across different languages?\n",
        "\n",
        "---\n",
        "\n",
        "**Q3)** Why do numbers like \"3.14159\" result in many more tokens (24 total) than you might expect? What does this tell about how transformers process numerical data?\n",
        "\n",
        "---\n",
        "\n",
        "**Q4)** What would happen if you tried to process text with a vocabulary that wasn't seen during the tokenizer's training? How might this affect the model's ability to understand and generate responses?\n",
        "\n",
        "---\n",
        "\n",
        "**Q5)** Why is it important that the tokenizer can handle special characters and mixed content (like \"GPT-3 has 175B parameters\")? How does subword tokenization help with out-of-vocabulary words?"
      ],
      "metadata": {
        "id": "7FzDJNepR3mJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 2: Autoregressive Generation Steps\n",
        "Watch how the model generates text token by token:"
      ],
      "metadata": {
        "id": "DPlibOOtXwym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 2: Change the prompt to see different patterns\n",
        "print(\"AUTOREGRESSIVE GENERATION\")\n",
        "print(\"Chapter 1: Decoder models generate one token at a time\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "prompt = [{\"role\": \"user\", \"content\": \"The attention mechanism in transformers\"}]\n",
        "\n",
        "# Generate different lengths to see the progression\n",
        "for num_tokens in [5, 15, 30]:\n",
        "    output = generator(prompt, max_new_tokens=num_tokens, do_sample=False)\n",
        "    print(f\"\\nAfter {num_tokens} tokens:\")\n",
        "    print(f\"'{output[0]['generated_text']}'\")\n",
        "\n",
        "print(\"\\nNotice how each output builds on the previous tokens!\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "h4MFNUa2Xy7-",
        "outputId": "3e3203f6-74c6-40b1-bff4-397944b7648c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUTOREGRESSIVE GENERATION\n",
            "Chapter 1: Decoder models generate one token at a time\n",
            "============================================================\n",
            "\n",
            "After 5 tokens:\n",
            "' The attention mechanism in transform'\n",
            "\n",
            "After 15 tokens:\n",
            "' The attention mechanism in transformers is a critical component that allows the model to'\n",
            "\n",
            "After 30 tokens:\n",
            "' The attention mechanism in transformers is a critical component that allows the model to weigh the importance of different parts of the input data differently. It was'\n",
            "\n",
            "Notice how each output builds on the previous tokens!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Questions:\n",
        "\n",
        "**Q1)** Why does the model generate text one token at a time rather than producing the entire output at once? What are the computational implications of this approach?\n",
        "\n",
        "---\n",
        "\n",
        "**Q2)** How does the context window limitation (4,000 tokens for Phi-3) affect what the model can \"remember\" during generation? What happens when you exceed this limit?\n",
        "\n",
        "---\n",
        "\n",
        "**Q3)** What patterns do you notice in how the model completes the sentence as more tokens are generated? Why does the output become more coherent with additional tokens?\n",
        "\n",
        "---\n",
        "\n",
        "**Q4)** Why might setting `do_sample=False` (greedy decoding) produce more predictable but potentially less creative outputs? When would you want deterministic vs. probabilistic generation?\n",
        "\n",
        "---\n",
        "\n",
        "**Q5)** How would changing the prompt from \"The attention mechanism in transformers\" to a more specific technical question affect the autoregressive generation process and the quality of intermediate outputs?"
      ],
      "metadata": {
        "id": "xVqf_ErjSAFf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Challenge 3: Comparing Models - Decoder Architecture\n",
        "Chapter 1 discussed how decoder-only models like GPT generate text.\n",
        "\n",
        "Load Qwen/Qwen2.5-1.5B-Instruct and compare it (This could take a little long):"
      ],
      "metadata": {
        "id": "avt5lVc4Zp29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 3: Comparing decoder-only models\n",
        "print(\"LOADING QWEN MODEL FOR COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load Qwen (smaller model for faster loading)\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "print(\"Loading Qwen/Qwen2.5-1.5B-Instruct...\")\n",
        "qwen_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\"\n",
        ")\n",
        "qwen_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
        "\n",
        "qwen_generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=qwen_model,\n",
        "    tokenizer=qwen_tokenizer,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=50,\n",
        "    do_sample=False\n",
        ")\n",
        "\n",
        "# Compare outputs for different prompts\n",
        "test_prompts = [\n",
        "    \"Explain neural networks in simple terms:\",\n",
        "    \"Write a Python function to sort a list:\",\n",
        "    \"What is the meaning of life?\"\n",
        "]\n",
        "\n",
        "for prompt_text in test_prompts:\n",
        "    prompt = [{\"role\": \"user\", \"content\": prompt_text}]\n",
        "\n",
        "    print(f\"\\nPrompt: '{prompt_text}'\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    phi3_output = generator(prompt, max_new_tokens=30, do_sample=False)\n",
        "    print(f\"Phi-3: {phi3_output[0]['generated_text'][:100]}...\")\n",
        "\n",
        "    qwen_output = qwen_generator(prompt, max_new_tokens=30, do_sample=False)\n",
        "    print(f\"Qwen:  {qwen_output[0]['generated_text'][:100]}...\")\n",
        "\n",
        "print(\"\\nNotice the different styles and capabilities of each model!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734,
          "referenced_widgets": [
            "9120785588f64d57aaef26469ce35955",
            "d240fdb1aff44277854c228109f7bd27",
            "583e0ccf1d504bea9acf30a2f52662b2",
            "5e286d3357b249d0b181621457b5778d",
            "0e45bea470484e3fbccdb126263d79b0",
            "fbe37456168e4906bb77775aefd01e93",
            "cf1cd2ce55354d1d8603e9ac43865db1",
            "ab69fe1d6e6f4d0eba76882612e9c91d",
            "facd7a9feb984b5b8de3e657ea98d212",
            "aa4bab1e69984673b745471f84ff1278",
            "f18d5ae280354fe1b2ec003c8aca15af",
            "6510c6f37add49e2aa75a8767a3db324",
            "4ae5df769a33498bb7513fd4960ac6f9",
            "ad077a70d8c14e7bbaf3e4d1b9c52920",
            "c604a3d7b533406e9b412a8c30274a03",
            "c36de6cfb48e41b5af14dad0dc50d0ab",
            "81cd5f4988044da885ba7bd1c2e19e8c",
            "e68d0872790a4bc39604c52e32db50e2",
            "2e3fdb5a551f4dec83d3b6c01c54b8cb",
            "06d4e7c540354685844259ef886855f9",
            "39d9cf37977d42009cb14ba0b899cc55",
            "9dcff68c9d95490b8a47197b143b1b5f",
            "e2eb89c3d41e44849568c5427d73917f",
            "9ffbba5528ec4354a72409d8c6592c5c",
            "e96aa5569e5a49a1a376609fa6961e0d",
            "b78abea78c0d4ea78f91b6cc18882e7a",
            "1e7a9c62021949cbbf9b9df67e2ae483",
            "b63956f67f494d13a96bf020bc7fb6bc",
            "8dd0777bd3bf459bade1ae916359721c",
            "5f291add5a1d4ce3bbed160d8263600b",
            "ba2d8a234b9245d2a979658d6421d896",
            "e5a1351bf4bd46fa90d9831bb3174714",
            "29f49b1eec3c47809a34b34147e7b0f8",
            "9de11fcccde34aceac1580adcc2de623",
            "a9487cac541743c486e6b717d1e78e03",
            "2da76f6f61f84594b68e90464b616bda",
            "7b4796740b3545b8b0a7afdfe2e2d4ed",
            "23857bb287054dcb9cf32055a8136503",
            "abb0b51fb8c244b5b400b5daa5328060",
            "ee8e558462ad435e87a15b9388d53094",
            "ff2d5d8918ad4885b3aa3d3b9c3a1f64",
            "655c07ee8f6942a6af91f3d042e85d57",
            "f14297fadd144626b6830d6027c42f62",
            "182f4dc03e3c4c2598543d563b8c903b",
            "753d43333cf04de395eafbe4f8b65c30",
            "8134e79f994246a59925fb15efacee72",
            "22c4464961c04d508bd6591c8957b0e3",
            "03126cf96ad645c6b4a2a3fad9f07e1a",
            "d9317965f8974d6da5186cd7086e9455",
            "2eb8c06a9c7f479f98b7dba21a82c788",
            "abe19766692443e38a26ff610fa86dd7",
            "a3cc620434de4c178bddac0a4a9dadc6",
            "82e161fa51a748ea97a23a1e394ed4e9",
            "ce8dd3c5efb94c07a97eeb67aa6471d3",
            "f951e362aeb6439d9399c76e4b3a33c3",
            "3cd2f5058ef0441da4de81e5a8251891",
            "4b84cc55d2bc48fa846c56870270fc61",
            "8524c4023cbf4f1cb12cb7987f4970fa",
            "bdc1e83b81bc4c60ae6c93fa14bfc1ec",
            "55565ecbabd342e0ae53eac46e94f66c",
            "bd4b0b6ae6c6413daeee9959c0a12a9f",
            "4545cfe41dce43729ffa8c95567ca60c",
            "f190ba320c6740238942110942078fde",
            "5860bfc87cfb4e6a9d328a9e2d99820b",
            "008a3721e39c4a9b87dfb6dff26d74bf",
            "83b02968c2894a5e9930272bd9f9923b",
            "11a9a276a73941ef926231f627cb682e",
            "772c7c38f1ae4bf986e165e904d4893a",
            "4286949f1ea244bd829f715725c70236",
            "eeef5e45096e412099d47ec5a0cfb3c8",
            "301aa36d8a76421190f38c5528cebaa2",
            "d577fbc3c56c4a869e0923aa156716d4",
            "6233fb98d6ac45d09bd2a9cc9c76d7d2",
            "d3e510be8deb4a5197ad14fe6819b6b0",
            "e6d0119c1e0b414b9dc1be9ee22b792f",
            "5ff7e4aa06f24bdf9c2766e767c9337c",
            "4f67364be93f4a5bb34ed191848e5b88"
          ]
        },
        "id": "Yo0J9B61b2nX",
        "outputId": "09d816b1-4610-41bd-fda1-ff753a03dca1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOADING QWEN MODEL FOR COMPARISON\n",
            "============================================================\n",
            "Loading Qwen/Qwen2.5-1.5B-Instruct...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9120785588f64d57aaef26469ce35955"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6510c6f37add49e2aa75a8767a3db324"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e2eb89c3d41e44849568c5427d73917f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9de11fcccde34aceac1580adcc2de623"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "753d43333cf04de395eafbe4f8b65c30"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3cd2f5058ef0441da4de81e5a8251891"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "11a9a276a73941ef926231f627cb682e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prompt: 'Explain neural networks in simple terms:'\n",
            "----------------------------------------\n",
            "Phi-3:  Neural networks are a series of algorithms that attempt to recognize underlying relationships in a ...\n",
            "Qwen:  Neural networks are a type of machine learning model inspired by the structure and function of the h...\n",
            "\n",
            "Prompt: 'Write a Python function to sort a list:'\n",
            "----------------------------------------\n",
            "Phi-3:  Certainly! Below is a Python function that sorts a list using the built-in `sorted()` function, whi...\n",
            "Qwen:  Certainly! Here's an example of a Python function that sorts a list using the built-in `sorted()` fu...\n",
            "\n",
            "Prompt: 'What is the meaning of life?'\n",
            "----------------------------------------\n",
            "Phi-3:  The meaning of life is a philosophical question concerning the significance of life or existence in...\n",
            "Qwen:  As an AI language model, I don't have personal beliefs or opinions about what constitutes the \"meani...\n",
            "\n",
            "Notice the different styles and capabilities of each model!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Questions:\n",
        "\n",
        "**Q1)** Why do Phi-3 (7.6B parameters) and Qwen (1.5B parameters) produce different styles of responses to the same prompt? How does model size affect response quality and style?\n",
        "\n",
        "---\n",
        "\n",
        "**Q2)** What architectural differences between these decoder-only models might account for their varying approaches to explaining concepts or generating code?\n",
        "\n",
        "---\n",
        "\n",
        "**Q3)** How does the training data for each model influence their responses? Why might one model be better at technical explanations while another excels at creative tasks?\n",
        "\n",
        "---\n",
        "\n",
        "**Q4)** Why do both models use the same fundamental autoregressive generation approach despite their size differences? What does this tell you about the transformer architecture's scalability?\n",
        "\n",
        "---\n",
        "\n",
        "**Q5)** What trade-offs are involved in choosing a smaller model like Qwen versus a larger one like Phi-3? Consider factors like inference speed, memory usage, and task performance."
      ],
      "metadata": {
        "id": "4y8vlPokSIAa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Challenge 4: Autoregressive Generation\n",
        "Chapter 1 explained that decoder models generate text one token at a time. Let's observe this:"
      ],
      "metadata": {
        "id": "aqI5G2r1Z1NK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 4: Try different prompts, temperatures, top_p values, etc.\n",
        "print(\"EXPERIMENT\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Change these examples and play with the models:\")\n",
        "\n",
        "# Example 1: Story generation with different temperatures\n",
        "story_prompt = [{\"role\": \"user\", \"content\": \"Once upon a time in a world where\"}]\n",
        "\n",
        "print(\"\\n1. Story Generation:\")\n",
        "for temp in [0.5, 1.0, 1.5, 5.0]:\n",
        "    output = generator(story_prompt, max_new_tokens=40, do_sample=True, temperature=temp)\n",
        "    print(f\"\\nTemperature {temp}: {output[0]['generated_text'][:80]}...\")\n",
        "\n",
        "# Example 2: Technical explanation\n",
        "tech_prompt = [{\"role\": \"user\", \"content\": \"The transformer architecture works by\"}]\n",
        "\n",
        "print(\"\\n\\n2. Technical Explanation:\")\n",
        "output = generator(tech_prompt, max_new_tokens=50, do_sample=False)\n",
        "print(output[0]['generated_text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "TUk2fV_mZ3m2",
        "outputId": "d294a166-df4b-4a5b-b8c4-9662ef3d77b3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EXPERIMENT\n",
            "============================================================\n",
            "Change these examples and play with the models:\n",
            "\n",
            "1. Story Generation:\n",
            "\n",
            "Temperature 0.5:  Once upon a time in a world where magic and technology coexisted harmoniously, ...\n",
            "\n",
            "Temperature 1.0:  Once upon a time in a world where...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Temperature 1.5:  Ah, where shall our story take place to set the scene? Any universe is availabl...\n",
            "\n",
            "Temperature 5.0:  once Upon This Time Again begins deep in Tarniaville Village at exactly four.  ...\n",
            "\n",
            "\n",
            "2. Technical Explanation:\n",
            " The transformer architecture works by using self-attention mechanisms to process input data in parallel, rather than sequentially. This allows for more efficient handling of long-range dependencies in data, such as in natural language processing tasks. The transformer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment with the hyperparameters:\n",
        "'''\n",
        "print(\"\\nExperiment:\")\n",
        "temp2 = x.x # change to a float value\n",
        "output = generator(story_prompt, max_new_tokens=40, do_sample=True, temperature=temp2)\n",
        "print(f\"\\nTemperature {temp2}: {output[0]['generated_text'][:80]}...\")\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "gT4jYANOempd",
        "outputId": "7048520a-b886-45d8-f04e-baf5469ffbc8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nprint(\"\\nExperiment:\")\\ntemp2 = x.x # change to a float value\\noutput = generator(story_prompt, max_new_tokens=40, do_sample=True, temperature=temp2)\\nprint(f\"\\nTemperature {temp2}: {output[0][\\'generated_text\\'][:80]}...\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Questions:\n",
        "\n",
        "**Q1)** Why does temperature 0.5 produce more coherent but predictable text while temperature 5.0 generates seemingly random or nonsensical output? What is happening to the probability distribution at different temperature values?\n",
        "\n",
        "---\n",
        "\n",
        "**Q2)** How does temperature mathematically modify the softmax distribution over the vocabulary? Why does dividing logits by temperature before softmax affect randomness?\n",
        "\n",
        "---\n",
        "\n",
        "**Q3)** What types of tasks would benefit from low temperature (0.5) versus high temperature (1.5) settings? Consider use cases like technical documentation vs. creative writing.\n",
        "\n",
        "---\n",
        "\n",
        "**Q4)** Why does the model sometimes produce grammatically incorrect or illogical text at very high temperatures even though it was trained on coherent text? What does this reveal about how language models store and retrieve information?\n",
        "\n",
        "---\n",
        "\n",
        "**Q5)** How would combining temperature with other sampling strategies like top-p (nucleus sampling) or top-k affect the generation quality? Why might you want to use multiple sampling techniques together?"
      ],
      "metadata": {
        "id": "aNhAgYTkSRSG"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
