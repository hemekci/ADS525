{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43c4fa1b",
   "metadata": {},
   "source": [
    "<h1>Chapter 7 - Hard Tasks</h1>\n",
    "<i>Memory and agent-like behavior.</i>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96fa010",
   "metadata": {},
   "source": [
    "## Using OpenAI API\n",
    "- This notebook uses OpenAI via LangChain (`ChatOpenAI`).\n",
    "- Set `OPENAI_API_KEY` in your environment before running.\n",
    "- All tasks use `llm`, which is the OpenAI chat model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "05146d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install langchain>=0.1.17 openai>=1.13.3 langchain_openai>=0.1.6 transformers>=4.40.1 datasets>=2.18.0 accelerate>=0.27.2 sentence-transformers>=2.5.1 duckduckgo-search>=5.2.2 langchain_community\n",
    "#%pip install llama-cpp-python==0.2.69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8997778c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ API key set\n"
     ]
    }
   ],
   "source": [
    "# OpenAI API setup\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Set the API key directly\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"api-key\"\n",
    "\n",
    "print(\"✓ API key set\")\n",
    "\n",
    "# Fast model\n",
    "openai_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
    "\n",
    "# Default LLM for the tasks below\n",
    "llm = openai_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c892dfb",
   "metadata": {},
   "source": [
    "<h2>Hard Tasks: Memory</h2>\n",
    "<p>LLMs forget by default. Let's attach memory and watch what changes.</p>\n",
    "\n",
    "<h3>Hard Task 1 - Conversation Buffer (full history)</h3>\n",
    "<p>Goal: Keep the entire conversation history so the model remembers everything.</p>\n",
    "<ol>\n",
    "  <li>Create a prompt that includes chat history.</li>\n",
    "  <li>Attach a ConversationBufferMemory.</li>\n",
    "  <li>Tell the model your name, ask a question, then ask it to recall your name.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9a2557cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Prompt that includes chat history\n",
    "buffer_template = \"Current conversation:\\n{chat_history}\\n{input_prompt}\"\n",
    "mem_prompt = PromptTemplate(template=buffer_template, input_variables=[\"input_prompt\", \"chat_history\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ba87bbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "\n",
    "memory_full = ConversationBufferMemory(memory_key=\"chat_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0b2cfbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains import LLMChain\n",
    "\n",
    "mem_chain_full = LLMChain(llm=llm, prompt=mem_prompt, memory=memory_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6c4f1f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First interaction: introduce yourself\n",
    "_ = mem_chain_full.invoke({\"input_prompt\": \"Hi! My name is Alex. What is 1 + 1?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bfa02d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_prompt': 'What is my name?', 'chat_history': 'Human: Hi! My name is Alex. What is 1 + 1?\\nAI: Hi Alex! 1 + 1 equals 2. How can I assist you further?', 'text': 'Your name is Alex. How can I help you today?'}\n"
     ]
    }
   ],
   "source": [
    "# Second interaction: ask if it remembers your name\n",
    "recall = mem_chain_full.invoke({\"input_prompt\": \"What is my name?\"})\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "53ebcd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Loaded Memory] {'chat_history': 'Human: Hi! My name is Alex. What is 1 + 1?\\nAI: Hi Alex! 1 + 1 equals 2. How can I assist you further?\\nHuman: What is my name?\\nAI: Your name is Alex. How can I help you today?'}\n"
     ]
    }
   ],
   "source": [
    "# Peek at what's stored in memory\n",
    "print(\"[Loaded Memory]\", memory_full.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8da906",
   "metadata": {},
   "source": [
    "<h3>Hard Task 2 - Windowed Buffer (last k turns)</h3>\n",
    "<p>Goal: Keep only the last few interactions to save tokens.</p>\n",
    "<ol>\n",
    "  <li>Keep only the last 2 interactions.</li>\n",
    "  <li>Share two facts (name + age), then see which one is remembered later.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "985c554f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# Only keep last 2 interactions\n",
    "memory_k2 = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5f70ed93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chain with windowed memory\n",
    "mem_chain_k2 = LLMChain(llm=llm, prompt=mem_prompt, memory=memory_k2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c19f67f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn 1: share name and age\n",
    "_ = mem_chain_k2.invoke({\"input_prompt\": \"Hi! My name is Casey and I am 33 years old. What is 2 + 2?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "354aa7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn 2: ask another math question\n",
    "_ = mem_chain_k2.invoke({\"input_prompt\": \"What is 3 + 3?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "af0faa36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall name: {'input_prompt': 'What is my name?', 'chat_history': \"Human: Hi! My name is Casey and I am 33 years old. What is 2 + 2?\\nAI: Hi Casey! 2 + 2 equals 4. How can I assist you today?\\nHuman: What is 3 + 3?\\nAI: AI: 3 + 3 equals 6. Is there anything else you'd like to know?\", 'text': 'Your name is Casey. How can I help you further?'}\n"
     ]
    }
   ],
   "source": [
    "# Turn 3 & 4: Ask recall questions\n",
    "recall_name = mem_chain_k2.invoke({\"input_prompt\": \"What is my name?\"})\n",
    "print(\"Recall name:\", recall_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "34a65202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall age: {'input_prompt': 'What is my age?', 'chat_history': \"Human: What is 3 + 3?\\nAI: AI: 3 + 3 equals 6. Is there anything else you'd like to know?\\nHuman: What is my name?\\nAI: Your name is Casey. How can I help you further?\", 'text': \"AI: I don't have that information. If you'd like to share your age or any other details, feel free to let me know!\"}\n"
     ]
    }
   ],
   "source": [
    "recall_age = mem_chain_k2.invoke({\"input_prompt\": \"What is my age?\"})\n",
    "print(\"Recall age:\", recall_age)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb7a8b4",
   "metadata": {},
   "source": [
    "<h3>Hard Task 3 - Conversation Summary (compress history)</h3>\n",
    "<p>Goal: Summarize the growing chat so it fits into fewer tokens.</p>\n",
    "<ol>\n",
    "  <li>Create a summary prompt.</li>\n",
    "  <li>Attach ConversationSummaryMemory that uses the LLM to summarize.</li>\n",
    "  <li>Observe how the model answers recall questions based on a summary.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bc83e149",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "summary_template = (\n",
    "    \"Summarize the conversations and update with the new lines.\\n\\n\"\n",
    "    \"Current summary:\\n{summary}\\n\\n\"\n",
    "    \"new lines of conversation:\\n{new_lines}\\n\\n\"\n",
    "    \"New summary:\"\n",
    ")\n",
    "sum_prompt = PromptTemplate(input_variables=[\"new_lines\", \"summary\"], template=summary_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1d22f461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.memory import ConversationSummaryMemory\n",
    "\n",
    "memory_sum = ConversationSummaryMemory(llm=llm, memory_key=\"chat_history\", prompt=sum_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "54c2fb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains import LLMChain\n",
    "\n",
    "mem_chain_sum = LLMChain(llm=llm, prompt=mem_prompt, memory=memory_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b6dcafa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First interaction\n",
    "_ = mem_chain_sum.invoke({\"input_prompt\": \"Hi! My name is Morgan. What is 4 + 4?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "30b00d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_prompt': 'What is my name?', 'chat_history': 'Current summary:\\n\\nHuman: Hi! My name is Morgan. What is 4 + 4?  \\nAI: Hi Morgan! 4 + 4 equals 8.', 'text': 'Your name is Morgan.'}\n"
     ]
    }
   ],
   "source": [
    "# Ask for recall\n",
    "recall = mem_chain_sum.invoke({\"input_prompt\": \"What is my name?\"})\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "861f08ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Current Summary] {'chat_history': 'Current summary:\\n\\nHuman: Hi! My name is Morgan. What is 4 + 4?  \\nAI: Hi Morgan! 4 + 4 equals 8.  \\nHuman: What is my name?  \\nAI: Your name is Morgan.'}\n"
     ]
    }
   ],
   "source": [
    "# View the current summary (not the full history)\n",
    "print(\"[Current Summary]\", memory_sum.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2124210",
   "metadata": {},
   "source": [
    "<h2>Hard Task 4 - Simulated Agent (ReAct-style thinking)</h2>\n",
    "<p>Let's simulate the <b>Thought → Action → Observation</b> pattern and let the model reason step-by-step.</p>\n",
    "<ol>\n",
    "  <li>Show a small list of pretend tools.</li>\n",
    "  <li>Ask a question the model can solve without external data.</li>\n",
    "  <li>Prompt the model to display its thinking as Thought/Action/Observation and then give a Final Answer.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "07b9fffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated ReAct prompt (no external APIs, no real tools)\n",
    "react_prompt = (\n",
    "    \"Answer the following question as best you can. You have access to the following tools:\\n\\n\"\n",
    "    \"- Calculator: good at basic arithmetic when you provide the numbers.\\n\"\n",
    "    \"- Notes: a scratchpad where you can keep short facts you derive.\\n\\n\"\n",
    "    \"Use the following format:\\n\"\n",
    "    \"Question: the input question you must answer\\n\"\n",
    "    \"Thought: you should always think about what to do\\n\"\n",
    "    \"Action: the action to take, e.g., Calculator or Notes\\n\"\n",
    "    \"Action Input: the input to the action\\n\"\n",
    "    \"Observation: the result of the action\\n\"\n",
    "    \"... (this Thought/Action/Action Input/Observation can repeat N times)\\n\"\n",
    "    \"Thought: I now know the final answer\\n\"\n",
    "    \"Final Answer: the final answer to the original input question\\n\\n\"\n",
    "    \"Begin!\\n\\n\"\n",
    "    \"Question: You have two dogs (each has 4 legs) and one bird (2 legs). How many legs in total?\\n\"\n",
    "    \"Thought:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bfa14907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='To find the total number of legs, I need to calculate the legs of the dogs and the bird separately and then sum them up. Each dog has 4 legs, and there are 2 dogs, so I will calculate the total legs from the dogs first. Then I will add the legs from the bird.\\n\\nAction: Calculator  \\nAction Input: (2 dogs * 4 legs) + (1 bird * 2 legs)  \\nObservation: 8 + 2 = 10  \\n\\nThought: I now know the final answer.  \\nFinal Answer: 10 legs in total.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 118, 'prompt_tokens': 175, 'total_tokens': 293, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CV3oK5KI6XnuJRa7uaIHRyOrjEN9k', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--7f2c1d99-9f96-4896-92a1-7357f34474aa-0' usage_metadata={'input_tokens': 175, 'output_tokens': 118, 'total_tokens': 293, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Invoke the simulated agent-style prompt\n",
    "react_output = llm.invoke(react_prompt)\n",
    "print(react_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a41b11",
   "metadata": {},
   "source": [
    "<h3>Reflect</h3>\n",
    "<ul>\n",
    "  <li>What did you notice about buffer vs. windowed vs. summary memory?</li>\n",
    "  <li>Which memory type would be best for a long conversation? Why?</li>\n",
    "  <li>In the simulated agent step, did the model clearly separate <i>Thought</i>, <i>Action</i>, and <i>Observation</i> before the final answer?</li>\n",
    "  <li>What happens if you change the ReAct question to something more complex?</li>\n",
    "</ul>\n",
    "<p>If any cell feels slow or costly, you can lower the temperature or switch to a smaller model.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ads525",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
