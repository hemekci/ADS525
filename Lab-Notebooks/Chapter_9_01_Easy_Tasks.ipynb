{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14ceb23e",
   "metadata": {},
   "source": [
    "# Multimodal LLMs - Easy Tasks\n",
    "\n",
    "Basic concepts with CLIP and BLIP-2. Loading models, making embeddings, simple image tasks.\n",
    "\n",
    "**Topics:**\n",
    "- CLIP text/image embeddings\n",
    "- Computing similarity scores\n",
    "- Basic image captioning\n",
    "- Simple visual Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d88c73b",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run all cells in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14273017",
   "metadata": {},
   "source": [
    "### [Optional] - Installing Packages on Google Colab\n",
    "\n",
    "If you are viewing this notebook on Google Colab, uncomment and run the following code to install dependencies.\n",
    "\n",
    "**Note**: Use a GPU for this notebook. In Google Colab, go to Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4802616a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install matplotlib transformers datasets accelerate sentence-transformers pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8eaecd",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ca67de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import CLIPTokenizerFast, CLIPProcessor, CLIPModel\n",
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
    "\n",
    "print(\"Imports ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dcf797",
   "metadata": {},
   "source": [
    "### Load Sample Images\n",
    "\n",
    "We'll use a few AI-generated images for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def17eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image URLs\n",
    "puppy = \"https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/chapter09/images/puppy.png\"\n",
    "cat = \"https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/chapter09/images/cat.png\"\n",
    "car = \"https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/chapter09/images/car.png\"\n",
    "\n",
    "print(\"Image URLs loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b1ad66",
   "metadata": {},
   "source": [
    "## Easy Tasks\n",
    "\n",
    "Basic operations with multimodal models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a506379",
   "metadata": {},
   "source": [
    "### Task 1: CLIP Text Embeddings\n",
    "\n",
    "Create embeddings for text using CLIP. Text embeddings capture semantic meaning.\n",
    "\n",
    "**Goal**: Embed a caption and inspect the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5b36ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP model\n",
    "model_id = \"openai/clip-vit-base-patch32\"\n",
    "print(\"Loading CLIP...\")\n",
    "\n",
    "clip_tok = CLIPTokenizerFast.from_pretrained(model_id)\n",
    "clip_proc = CLIPProcessor.from_pretrained(model_id)\n",
    "clip_model = CLIPModel.from_pretrained(model_id)\n",
    "\n",
    "print(\"Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f44418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "caption = \"a puppy playing in the snow\"\n",
    "\n",
    "inputs = clip_tok(caption, return_tensors=\"pt\")\n",
    "print(f\"Caption: {caption}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78e1eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check tokens\n",
    "tokens = clip_tok.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "print(f\"Tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5ca695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text embedding\n",
    "txt_emb = clip_model.get_text_features(**inputs)\n",
    "\n",
    "print(f\"Embedding shape: {txt_emb.shape}\")\n",
    "print(f\"First 5 values: {txt_emb[0][:5].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15c9099",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "1. Try a different caption - how does the embedding change?\n",
    "2. What happens with very long text?\n",
    "3. Try text in another language - does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d20529",
   "metadata": {},
   "source": [
    "### Task 2: CLIP Image Embeddings\n",
    "\n",
    "Same idea but for images. Images get processed into patches, then embedded.\n",
    "\n",
    "**Goal**: Load an image, embed it, check the shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecad3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "img = Image.open(urlopen(puppy)).convert(\"RGB\")\n",
    "\n",
    "print(f\"Loaded image\")\n",
    "print(f\"Size: {img.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd44e4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show image\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title(\"Puppy image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d14869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "proc_img = clip_proc(\n",
    "    text=None, \n",
    "    images=img, \n",
    "    return_tensors='pt'\n",
    ")['pixel_values']\n",
    "\n",
    "print(f\"Processed shape: {proc_img.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142959d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original was probably different size\n",
    "print(f\"Original: {img.size}\")\n",
    "print(f\"After processing: 224x224 (required by CLIP)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360cf7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create image embedding\n",
    "img_emb = clip_model.get_image_features(proc_img)\n",
    "\n",
    "print(f\"Image embedding shape: {img_emb.shape}\")\n",
    "print(f\"Same as text? {img_emb.shape == txt_emb.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c2470f",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "1. Load the cat or car image - what's different?\n",
    "2. Why does CLIP resize to 224x224?\n",
    "3. What happens if you load a very small image?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae46770",
   "metadata": {},
   "source": [
    "### Task 3: Text-Image Similarity\n",
    "\n",
    "Compare embeddings to see if image matches caption.\n",
    "\n",
    "**Goal**: Calculate similarity score between text and image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520995fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize embeddings (required for cosine similarity)\n",
    "txt_emb = txt_emb / txt_emb.norm(dim=-1, keepdim=True)\n",
    "img_emb = img_emb / img_emb.norm(dim=-1, keepdim=True)\n",
    "\n",
    "print(\"Normalized embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784dcbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate similarity\n",
    "txt_np = txt_emb.detach().cpu().numpy()\n",
    "img_np = img_emb.detach().cpu().numpy()\n",
    "\n",
    "sim = txt_np @ img_np.T\n",
    "\n",
    "print(f\"Similarity: {sim[0][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb74d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try wrong caption\n",
    "wrong = \"a car driving at sunset\"\n",
    "wrong_inp = clip_tok(wrong, return_tensors=\"pt\")\n",
    "\n",
    "wrong_emb = clip_model.get_text_features(**wrong_inp)\n",
    "wrong_emb = wrong_emb / wrong_emb.norm(dim=-1, keepdim=True)\n",
    "\n",
    "wrong_np = wrong_emb.detach().cpu().numpy()\n",
    "sim_wrong = wrong_np @ img_np.T\n",
    "\n",
    "print(f\"\\nCorrect caption: {sim[0][0]:.4f}\")\n",
    "print(f\"Wrong caption: {sim_wrong[0][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec48b50",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "1. Try other caption variations - which scores highest?\n",
    "2. What's a \"good\" similarity score?\n",
    "3. Can you find a caption that scores even lower?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127609f6",
   "metadata": {},
   "source": [
    "### Task 4: Zero-Shot Image Classification\n",
    "\n",
    "Use CLIP to classify images without training. Just compare image to class descriptions.\n",
    "\n",
    "**Goal**: Given an image, find which class description matches best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d55113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cat image\n",
    "cat_img = Image.open(urlopen(cat)).convert(\"RGB\")\n",
    "\n",
    "plt.imshow(cat_img)\n",
    "plt.axis('off')\n",
    "plt.title(\"What is this?\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c07cbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible classes\n",
    "classes = [\n",
    "    \"a photo of a dog\",\n",
    "    \"a photo of a cat\", \n",
    "    \"a photo of a car\",\n",
    "    \"a photo of a bird\"\n",
    "]\n",
    "\n",
    "print(\"Classes:\")\n",
    "for i, c in enumerate(classes, 1):\n",
    "    print(f\"{i}. {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c461e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed image\n",
    "cat_proc = clip_proc(images=cat_img, return_tensors='pt')['pixel_values']\n",
    "cat_emb = clip_model.get_image_features(cat_proc)\n",
    "cat_emb = cat_emb / cat_emb.norm(dim=-1, keepdim=True)\n",
    "\n",
    "print(\"Image embedded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45b1e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed all classes\n",
    "class_embs = []\n",
    "\n",
    "for cls in classes:\n",
    "    inp = clip_tok(cls, return_tensors=\"pt\")\n",
    "    emb = clip_model.get_text_features(**inp)\n",
    "    emb = emb / emb.norm(dim=-1, keepdim=True)\n",
    "    class_embs.append(emb)\n",
    "\n",
    "print(f\"Embedded {len(class_embs)} classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec44e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate similarities\n",
    "cat_np = cat_emb.detach().cpu().numpy()\n",
    "scores = []\n",
    "\n",
    "for emb in class_embs:\n",
    "    emb_np = emb.detach().cpu().numpy()\n",
    "    sc = cat_np @ emb_np.T\n",
    "    scores.append(sc[0][0])\n",
    "\n",
    "print(\"\\nScores:\")\n",
    "for cls, sc in zip(classes, scores):\n",
    "    print(f\"{cls}: {sc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db68c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best match\n",
    "best_idx = np.argmax(scores)\n",
    "best_class = classes[best_idx]\n",
    "\n",
    "print(f\"\\nPrediction: {best_class}\")\n",
    "print(f\"Confidence: {scores[best_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88f3b4c",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "1. Try the car image - does it classify correctly?\n",
    "2. Add more classes - does accuracy drop?\n",
    "3. What if you use very specific class names?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b259497a",
   "metadata": {},
   "source": [
    "### Task 5: Basic Image Captioning with BLIP-2\n",
    "\n",
    "Generate text descriptions of images. BLIP-2 bridges vision and language.\n",
    "\n",
    "**Goal**: Load an image, generate a caption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee18065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BLIP-2\n",
    "print(\"Loading BLIP-2 (this takes a minute)...\")\n",
    "\n",
    "blip_proc = AutoProcessor.from_pretrained(\n",
    "    \"Salesforce/blip2-opt-2.7b\",\n",
    "    revision=\"51572668da0eb669e01a189dc22abe6088589a24\"\n",
    ")\n",
    "\n",
    "blip_model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip2-opt-2.7b\",\n",
    "    revision=\"51572668da0eb669e01a189dc22abe6088589a24\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "dev = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "blip_model.to(dev)\n",
    "\n",
    "print(f\"Loaded on {dev}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebe381c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load car image\n",
    "car_img = Image.open(urlopen(car)).convert(\"RGB\")\n",
    "\n",
    "plt.imshow(car_img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e42045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "inp = blip_proc(car_img, return_tensors=\"pt\").to(dev, torch.float16)\n",
    "\n",
    "print(\"Image preprocessed\")\n",
    "print(f\"Shape: {inp['pixel_values'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37683fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate caption\n",
    "gen_ids = blip_model.generate(**inp, max_new_tokens=20)\n",
    "\n",
    "caption = blip_proc.batch_decode(gen_ids, skip_special_tokens=True)\n",
    "caption = caption[0].strip()\n",
    "\n",
    "print(f\"Caption: {caption}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550ba22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with puppy\n",
    "pup_img = Image.open(urlopen(puppy)).convert(\"RGB\")\n",
    "\n",
    "inp = blip_proc(pup_img, return_tensors=\"pt\").to(dev, torch.float16)\n",
    "gen_ids = blip_model.generate(**inp, max_new_tokens=20)\n",
    "caption = blip_proc.batch_decode(gen_ids, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "print(f\"Puppy caption: {caption}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3ee8eb",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "1. Try your own images - how accurate are captions?\n",
    "2. What happens with abstract or artistic images?\n",
    "3. Increase max_new_tokens to 50 - do captions get better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4887f8",
   "metadata": {},
   "source": [
    "### Task 6: Simple Visual Q&A\n",
    "\n",
    "Ask questions about images. Model processes both image and question.\n",
    "\n",
    "**Goal**: Give BLIP-2 an image and a question, get an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a67231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load car image again\n",
    "car_img = Image.open(urlopen(car)).convert(\"RGB\")\n",
    "\n",
    "plt.imshow(car_img)\n",
    "plt.axis('off')\n",
    "plt.title(\"Ask me about this image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26f107d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question\n",
    "q = \"Question: What color is the car? Answer:\"\n",
    "\n",
    "inp = blip_proc(car_img, text=q, return_tensors=\"pt\")\n",
    "inp = inp.to(dev, torch.float16)\n",
    "\n",
    "print(f\"Question: {q}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54671fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate answer\n",
    "gen_ids = blip_model.generate(**inp, max_new_tokens=20)\n",
    "ans = blip_proc.batch_decode(gen_ids, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "print(f\"Answer: {ans}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945d21ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try another question\n",
    "q2 = \"Question: Is this indoors or outdoors? Answer:\"\n",
    "\n",
    "inp = blip_proc(car_img, text=q2, return_tensors=\"pt\")\n",
    "inp = inp.to(dev, torch.float16)\n",
    "\n",
    "gen_ids = blip_model.generate(**inp, max_new_tokens=20)\n",
    "ans2 = blip_proc.batch_decode(gen_ids, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "print(f\"Q: {q2}\")\n",
    "print(f\"A: {ans2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b045657f",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "1. Try yes/no questions - does it answer correctly?\n",
    "2. Ask about details not in the image - what happens?\n",
    "3. What types of questions work best?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcffe4d",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "1. Try yes/no questions - does it answer correctly?\n",
    "2. Ask about details not in the image - what happens?\n",
    "3. What types of questions work best?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
