{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_a9QvUFVCUR"
   },
   "source": [
    "<h1>Chapter 4 - Text Classification</h1>\n",
    "<i>Classifying text with both representative and generative models</i>\n",
    "\n",
    "<a href=\"https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961\"><img src=\"https://img.shields.io/badge/Buy%20the%20Book!-grey?logo=amazon\"></a>\n",
    "<a href=\"https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/\"><img src=\"https://img.shields.io/badge/O'Reilly-white.svg?logo=data:image/svg%2bxml;base64,PHN2ZyB3aWR0aD0iMzQiIGhlaWdodD0iMjciIHZpZXdCb3g9IjAgMCAzNCAyNyIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGNpcmNsZSBjeD0iMTMiIGN5PSIxNCIgcj0iMTEiIHN0cm9rZT0iI0Q0MDEwMSIgc3Ryb2tlLXdpZHRoPSI0Ii8+CjxjaXJjbGUgY3g9IjMwLjUiIGN5PSIzLjUiIHI9IjMuNSIgZmlsbD0iI0Q0MDEwMSIvPgo8L3N2Zz4K\"></a>\n",
    "<a href=\"https://github.com/HandsOnLLM/Hands-On-Large-Language-Models\"><img src=\"https://img.shields.io/badge/GitHub%20Repository-black?logo=github\"></a>\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter04/Chapter%204%20-%20Text%20Classification.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "This notebook is for Chapter 4 of the [Hands-On Large Language Models](https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961) book by [Jay Alammar](https://www.linkedin.com/in/jalammar) and [Maarten Grootendorst](https://www.linkedin.com/in/mgrootendorst/).\n",
    "\n",
    "---\n",
    "\n",
    "<a href=\"https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961\">\n",
    "<img src=\"https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/images/book_cover.png\" width=\"350\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LGW2SD-c864"
   },
   "source": [
    "### [OPTIONAL] - Installing Packages on <img src=\"https://colab.google/static/images/icons/colab.png\" width=100>\n",
    "\n",
    "\n",
    "If you are viewing this notebook on Google Colab (or any other cloud vendor), you need to **uncomment and run** the following codeblock to install the dependencies for this chapter:\n",
    "\n",
    "---\n",
    "\n",
    "\ud83d\udca1 **NOTE**: We will want to use a GPU to run the examples in this notebook. In Google Colab, go to\n",
    "**Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "N-PxmOIhc865"
   },
   "outputs": [],
   "source": [
    " %%capture\n",
    "!pip install transformers sentence-transformers openai\n",
    "!pip install -U datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBeVnXxQWy7-"
   },
   "source": [
    "# **Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 784,
     "referenced_widgets": [
      "169816892c8646e3888f213295349f00",
      "4a590f6ceb104873b97b9620d6107017",
      "2d14e5528bbd446e8a34c235201f88ac",
      "f4c9e362c7ff40559a1a5632a8b6e907",
      "3c4c633f0af84d099ffc74bac8901b07",
      "35226b53943b4db8a5d46aae09720818",
      "1a6faf9a5ed748f0809834cc52435b3d",
      "4fe179f7fab44513aeaa33cedf44f1bb",
      "5befa9362a09459695f4489c7173c34d",
      "458ad67bb385497889eab5bbdd977de5",
      "027a7c3f8a9e4114ac6d4e64fb62d440",
      "0cce924a44f94cbfaa69a78735ece1d5",
      "28ac2aeffcb6424f97fb47bfaee8e66f",
      "ab02cefe39774db3844d914b65a48790",
      "6bf6e7d710eb4924a3144e6e5d0679ca",
      "b7b774839b134ee687d9d89a0e15d166",
      "c583fda17dd44217875e86209fbad80a",
      "cd4eeedee4fc4f28acf650785bbaa5f3",
      "e5138fd5028f48f9ad8eea2b41b1592a",
      "82f4409e3f854415a871e3da31e9393e",
      "aeb337caa109443a9fc4f93e9f92753d",
      "e7e6399dec234052b7f30e3135a21b00",
      "44a764c1267042259b2c9597a69f6f76",
      "05e8f57e23ff47f0be24d3f9c18554d4",
      "616321e6e87a4d8589d9ed939e19c7b2",
      "8d7284d945004b98b28b6c631a2c4726",
      "97d864e9c07149aeb22a844ced3909d5",
      "dbadb6dfe23141ec8f38ffb747ea882e",
      "82d0c0a18606421f92ed77d8d4c61e10",
      "0c64a3bc4d4d461cbdc3c3c0acae2f94",
      "cf707e36593b4071b8253fd33d18b665",
      "079412e5345a4a09875ed0c86dde93e3",
      "1e2ffd31336946cf952abb5f62651c16",
      "8466edc3103c45d8a5d3fe8166508648",
      "f8d0d8f24551457e862d2e041f3699ab",
      "426852cccf704c58bdc7decbb1c583e6",
      "6b2663befc574a8e8154631c3ac95ff3",
      "091bd22cb6d94ae5b7c962d18e528ca2",
      "5ba5883cc2144929b9d7ddd980e4508d",
      "971d17ca015446b4bbf8a908835295cd",
      "c7086b545eae4b09af3b2d77b78af4b9",
      "5f47aeddde3449a5b5ed280d0ec9bc18",
      "43791444ad3e49f993d17887e6c15dbf",
      "c2c3403eb38b420cba7390d385f325be",
      "9097dd47971f445ea0fd55e7c42c15a0",
      "18cbf6cbaa4c436d9074224f83472863",
      "831180b810da4b549a8ca75536607e31",
      "d9779ccf530e49b1ac52172252ec5ef4",
      "78d69219fad44722a9bad3a12a3ccb61",
      "f08fa19e07644f4d9fff30d02040e315",
      "e1a2a1e765f042a7b9b081688688dab5",
      "76d299a97ab54dbe96f4525095afedcb",
      "73ac94bdcaf8427bbbcdd808f1428dff",
      "6f9a5d0dfe2f4aa49c57e942b927521b",
      "2d7810d299f649a2ac84541e7215d29a",
      "40dc8a185cf34e99a427f7c81bc76540",
      "bcac3e31740e4a2894b4e8bc9656a0c5",
      "87f29bce5b5d4df08b5337d62ffa9568",
      "3129b232f1e3411ead0d913654a11eaa",
      "4e7c3e87552640018f89c6cf9e070642",
      "01ccc633ad5d4350ad04651e360cc478",
      "735737e7c1eb477fae9d06d7324e81e8",
      "882b69fb84034d2dace625ef267f51cb",
      "8c77be8e5b74452bb028b97fd9edba36",
      "55a6296e28394f41b2a6d1f3d76e5944",
      "8f6c7a732bb6498db1de4d66f4b3c623",
      "22541d211626493e87c168143671e5ce",
      "111b9c6182994e2e85a323d07980b1ab",
      "3c53665ac233434fa899399da13650f0",
      "c4867626e6eb41bfaade350790de9f40",
      "a11866f208c4411dbc3d627ef6dbd74f",
      "3485c0ebc7f24965a0fadd3a6570d51e",
      "d111b1f753554eeda57cdbe420335fda",
      "8fb77e0530e946d38c007c9012c37a79",
      "8d669f0719d04980a6f04ffc2f65cf7c",
      "d79a649275a14fdeb2cb01e5ab75021f",
      "14186c7dd75e43d99183d5c8b308c0de"
     ]
    },
    "id": "5phRS_z2U_3T",
    "outputId": "e51c9c23-a48a-4c61-e7ee-cd0ef03915a4"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "169816892c8646e3888f213295349f00"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "train.parquet:   0%|          | 0.00/699k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0cce924a44f94cbfaa69a78735ece1d5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "validation.parquet:   0%|          | 0.00/90.0k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "44a764c1267042259b2c9597a69f6f76"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "test.parquet:   0%|          | 0.00/92.2k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8466edc3103c45d8a5d3fe8166508648"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9097dd47971f445ea0fd55e7c42c15a0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating validation split:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "40dc8a185cf34e99a427f7c81bc76540"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating test split:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "22541d211626493e87c168143671e5ce"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 8530\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load our data\n",
    "data = load_dataset(\"rotten_tomatoes\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xJJmaJzHDLZv",
    "outputId": "fd2ef721-8635-466b-d787-515fc8e4dffd"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'text': ['the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .',\n",
       "  'things really get weird , though not particularly scary : the movie is all portent and no content .'],\n",
       " 'label': [1, 0]}"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "data[\"train\"][0, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xya5dfmVoR1R"
   },
   "source": [
    "# **Text Classification with Representation Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "co68g-Eloknf"
   },
   "source": [
    "## **Using a Task-specific Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448,
     "referenced_widgets": [
      "1fb504f12d6e4b40937b0ef9db1b2644",
      "7b89f8211c56444f82518264593156dd",
      "a4a66adab7f641a9bdb6496e0401881f",
      "bd8c5ad9440742bcb4779705379b98fc",
      "ba257dbbadc742bcaa34780cc1fd7db5",
      "9e2e1629f04643fb893e0a1bb99589ae",
      "925169e5d07c4d229b342fe4b41b2544",
      "0dc110695d3943a5afbaabc2ccb74bce",
      "92249295eebb4dcfbb17b2eb348bcfc6",
      "11b59aee3f1c43bc821a8b2abba4d9a6",
      "7e4bfde8cc0f41409e5703b50ca951ae",
      "a52864b4ba6f443e95be535dfa78bd81",
      "b79bbe20431c4eaa8ab39e71c682a4a5",
      "e3128b82fd3442d39afed90fc9ef77b7",
      "a72769becce048b6b2d5fd92a7b7e105",
      "e4c4c6616a5d458499fb035f8c96a72b",
      "4e741246929d40fa93a0d4bd178dad14",
      "207aa57d2b224b9fb42fdecb67864c7a",
      "4527a8a8f8e945c5b1e289fa84aa2dd9",
      "0aeaac43201c41328a4f009378893889",
      "8d378ae5d43d484abe48e3d84e9ede8e",
      "83051aac1ae6471cace09a99e3e48d78",
      "1c69a7234956470dae17b814673702c1",
      "53a98f00da5b4373bfc55dc68338e33b",
      "b8a04b7c0a0942b58c906b2742613aed",
      "b481591887bd4b4aa7796fa1f08db48d",
      "13f95d964fca4ebfb805883f914ac552",
      "f342b12a14bc4679af668aacdc56e755",
      "2eeb334e09d848fdba87cd8e9763b55e",
      "87016c44c90147a591311e9122b990ab",
      "eda4d37856194c1da4ae1b1166857222",
      "8f7d09967c32447bad6b421af147a499",
      "dbda5498ffdf48c2970f7b80e3bd3981",
      "6cdfde3fe9e947609ac08516e75402b5",
      "a22a276031104274b9ae00ae0d492f05",
      "299b4336b07743d2904424f31adf6088",
      "abb5d0a0710d464e938bc080687ec765",
      "ff8c599d1975469e89c4b0c255c28dba",
      "569e6c83aef24232a4b37207418fde55",
      "cb917089e8894de9a97199092a924527",
      "c439af89c3814f5abf12f422e5eacd60",
      "a70c2a858aab45f28a118da4d0a2816e",
      "7afc0a026ff54a658bd608bd18e03fd8",
      "763bef1d422b436ba1bcc959016c1417",
      "80f655a52a2b4f269889f0fa0db922f1",
      "4924034b3042401ca21c0c0229262267",
      "c9c3b424188b49a39ef396a501aa7b51",
      "5a0e9e8e59704764b93adb898325e0ff",
      "66ef85a159ae4abf8deb5710a49b3bf5",
      "87e8a8450bda49f9918213b9808beffe",
      "a9693c19e60344a8b5d246571fe65cde",
      "7bcfaa94e7af4c0ab5dd1353b36982f6",
      "53a7294ac44e49eaa2bf5ba46b78496e",
      "365e12fd2f3a4f9e817163e571ced1f0",
      "45d9ba9b2c36424bbf4b38d814ee95a2",
      "209836d671e548f7a126c95a6ea4f039",
      "abe9c4abaf204c5fb54d074d43a4a0e3",
      "ede9f93cc7064bfcb27331753580651c",
      "e9d028695e614841912c29586f6aa0ed",
      "cd8c7e3ed61b4c18820a1eac523c2ec8",
      "051b04994a2a44e98a0456ec7fcc4364",
      "403f3c6a21e84992b9a1715c61611919",
      "b192ab52b45a463cbd8ec4fa2be30e93",
      "d82f5d2fd051445291e7efef43b0e07a",
      "4fedc855d66a408b8ef6be24e0f126cc",
      "df6477cd708245fcaf89ed3a0243426d"
     ]
    },
    "id": "ph-3T3XJopdN",
    "outputId": "bba715ac-9776-4816-fb43-d72a1b915fb6"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1fb504f12d6e4b40937b0ef9db1b2644"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a52864b4ba6f443e95be535dfa78bd81"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1c69a7234956470dae17b814673702c1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6cdfde3fe9e947609ac08516e75402b5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "80f655a52a2b4f269889f0fa0db922f1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "209836d671e548f7a126c95a6ea4f039"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Device set to use cuda:0\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Path to our HF model\n",
    "model_path = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "\n",
    "# Load model into pipeline\n",
    "pipe = pipeline(\n",
    "    model=model_path,\n",
    "    tokenizer=model_path,\n",
    "    return_all_scores=True,\n",
    "    device=\"cuda:0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B2gbnL5Q69Y5",
    "outputId": "b51721d2-df8f-4e0d-8066-127b9ca1beac"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1066/1066 [00:10<00:00, 103.45it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "\n",
    "# Run inference\n",
    "y_pred = []\n",
    "for output in tqdm(pipe(KeyDataset(data[\"test\"], \"text\")), total=len(data[\"test\"])):\n",
    "    negative_score = output[0][\"score\"]\n",
    "    positive_score = output[2][\"score\"]\n",
    "    assignment = np.argmax([negative_score, positive_score])\n",
    "    y_pred.append(assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "X0KyKHtqyjn3"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_performance(y_true, y_pred):\n",
    "    \"\"\"Create and print the classification report\"\"\"\n",
    "    performance = classification_report(\n",
    "        y_true, y_pred,\n",
    "        target_names=[\"Negative Review\", \"Positive Review\"]\n",
    "    )\n",
    "    print(performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fum3MTSyymlW",
    "outputId": "d7cbdbbb-91be-48b4-da8e-1aae9936dd08"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Negative Review       0.76      0.88      0.81       533\n",
      "Positive Review       0.86      0.72      0.78       533\n",
      "\n",
      "       accuracy                           0.80      1066\n",
      "      macro avg       0.81      0.80      0.80      1066\n",
      "   weighted avg       0.81      0.80      0.80      1066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_performance(data[\"test\"][\"label\"], y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wr3WT4jzoNZE"
   },
   "source": [
    "## **Classification Tasks that Leverage Embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8yuSP3heMzT"
   },
   "source": [
    "### Supervised Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 685,
     "referenced_widgets": [
      "4e81be8e5ffc43099d1768f2b379c2ce",
      "dcc81d53b875455885c741b3af3a5b0a",
      "be43233653e14ef99a9fe4d307c831df",
      "c7a2b1dc071c40f4bfb7bdf23d46628d",
      "5a93e11d2d4c400492a77216349bd690",
      "e5612408eb3547a881b5bbc976f1fe29",
      "9b8c5d6b959e4e03b4d29ad32986b659",
      "1ca82d860d0344cb856382534a6bfefc",
      "022fed1441914a0ca352a734940836fd",
      "42349dbbd1534a1bb309353b2d642c1b",
      "86ee3f61b6c443e7b0cc14ccacb8d93c",
      "0962ea3d1fa34efcace74c949a3f636a",
      "18ade018745d429680f5f6d6b2a12217",
      "5ff845b3c271455eb7de17b621c4f8da",
      "2f46c4961c1941f68bd6744d910c4951",
      "aab906d02f894947a7a939919112c82d",
      "0607d79940504e49a9c404192b9f0ab5",
      "57d43902023d4014bd0662e730b4185e",
      "31c0895f9513498ca76c57e3826f7816",
      "60aed70d6d1d46d0846c6f8953316176",
      "0a343810af264b8d9218f78c1425358a",
      "cf1a3a0460a54b738b3a4fbce9595efc",
      "ff44125d52054f29ba06ceae0a6821b1",
      "83cf3e9f22764776826ba9916361fa1d",
      "57eabd446bb642a0ac6ffcd48b51bf20",
      "3adb5f1d06d44efb937aa60d4dc6b590",
      "d054cd84029042478bf37a80a5afca2d",
      "cacef97c29894887b41e3ff709a6212d",
      "24c5e567304f4f708e00a0232bf74bc7",
      "bfed59c1582b4524a7fc2273de2bb03f",
      "c3e8bb57d7b3464f9faf0ac08778ef68",
      "4a6134d8cebb474192acc22bcaafcd95",
      "ec4bdfc264144ac9a3ddff215370e2c9",
      "cf7e368bd70e479bb003b8688deb27d5",
      "af4541b7787a40dfa185badaad4fc7d9",
      "bbe7a95510a24ebf99a8df491fc59d1a",
      "3261ae1c6fdb45cf96b4a4552a45626c",
      "1debcc698a0843e1a879beb195930aff",
      "a69251a68dd34672a0a336e140663e2f",
      "dec830ff5c8f49d9ba5fba56eb2e9071",
      "08e8167e9ca74af7aa71f83249cf47e1",
      "4ad52778626948618e3f025cf018dcf0",
      "6318fa44715a42bca4c84a92c19c3dea",
      "d47538747b4c4c2ba233610bf0564134",
      "4e256742aed947089df526b210c16290",
      "5f70584f9eb84ba3b460ba9ac76ee453",
      "f37ca89700644feba278d17b4c6aeaed",
      "140e5095c3d3461ead6ee0be72b00f5d",
      "25cd477eadfb483fb8d8574a22b8c6ea",
      "daa4ca445a37481587114eaa8cf4ee76",
      "fc2ad0c3efcc4fdea1a4e6e51e3252e2",
      "79fbb1c65d374f49a96940ad9433e29b",
      "02f96098408e4b7e96fe30c01edec74f",
      "afec417c406e44e8864a7d91fcab9c70",
      "06720654f3d8410da53c7d47d60e5e48",
      "3a2239bbac764e18a6d3e54bd8e24385",
      "78e8da5c28724c2592b94ad087fb447f",
      "55853935d9da448e9f65708aefbb433a",
      "3e614baa3e1d4501838b59fc0c8be5b8",
      "e0a029527cdc44f2a2bd79c2adc2c948",
      "f1dd73af95b446cbbb626badb15dedee",
      "ef459535fd044f02b738551eb3202ab7",
      "5d8475a83bc249d5915bd436657b2138",
      "472eb71cf8cd4e6cabda9324530b1a88",
      "71258f3809544398958756a56b939501",
      "eb517a787a5242d1b899029a064dc729",
      "66d1c59207bd4b579b17f91a173b64d3",
      "45b88debf5bd44fab095277825c5a61d",
      "85cf17b86a404041a4357d114ee31380",
      "028cc741e4ad48158c9f7bafef680816",
      "b23100f08efe4daa97201b09d1cb589d",
      "123308249ab74cad8f04383520f57ea2",
      "f9694cb1ee8f4dd1b48f3eee91faa0fc",
      "278c539dd782455f80eb7d28b11596b3",
      "ebceb670f6324a69b417a65dde28844c",
      "852ab216c2cd448bbcda88ba4516dfc7",
      "9ba5975196e441538f7d27cc7ad8d5c3",
      "9b9b1d35635e453e9dc7be625b5e22ab",
      "8c6be67889bb45548d17945ea1c7748b",
      "a3db156c5ae4456490622f69d9d7a419",
      "48e1e45ee4514353b66a5ede013c4126",
      "85d46881181c4a409ae830f2e8d8bc0a",
      "63f2725708e942759ee5f97d0ba543d5",
      "76e69a4aba9a44f6804ba2f5b31987c5",
      "6bbd2f542b5f4509bc701a8f3eca85d4",
      "c8f79a7929234d25af837a9accf1c7c6",
      "f4020fe822374f6d931ec3156f5f8290",
      "d49d651103c647deb11a79c9a8594720",
      "0ec62022b7f14c17aaf7ec388bf7a35f",
      "9905cf8fecd74170823db01f048d6c18",
      "5257ebdade3347bab07ed3592c388644",
      "2d6ca74dc7f547d19f4ccc7e2c344ba6",
      "52b07ec3bf20486f968697aac159e3ec",
      "57ce50bb57d843f68aa6502d87cbb39c",
      "bbebb6d8b43d4d97abf04701510aa678",
      "15e435cf2f174a36aa154847da606f89",
      "e476841f60734793bd50fe0803d2ff5a",
      "f33cf0e51acd4f0285ea91b9c697125a",
      "8bb620a2b07145b4b201c6d016e5183c",
      "8165eeff3f4b453fa6236e8fab4f6fcb",
      "e7dd7f4a1c9d46ef92faa9839d6206f5",
      "76630bfcf6c740f99f23e654a5c78e93",
      "c2fd1cb6aabb4d2ba1abdba8dde381a1",
      "2c3095624df04d638a1b5b6c76e1739c",
      "0c61efba3db24edf93e0ad5eadfbfe94",
      "e22ce558f5fc4787bd7d13d5001cd0b4",
      "3ebe369233fa44b4a0a5d42d081b4e81",
      "6a44d72ebb0c4ea8b63474895e9a678c",
      "1d6324eb9d034df2973a75d0aba93224",
      "f32e93567eb74ce8a26f2f935674f522",
      "da6359244454407c847ec5af84461ff4",
      "d1252bef4d2647b5ae41d30fb25dfcb3",
      "f81f8b2ee6e34ae89231077455703fb1",
      "791836d8a7b34eabb236e60320d319f4",
      "588504b532aa43deb77e717cd191cc7c",
      "54824268dc954706b1e460b741f103c2",
      "1492d718e5884b6cb3dbfd12a1dba7e9",
      "740baaf426be416a9fc2c0707fbaff92",
      "cbeb5b778ba34bc6b1a744ad95739f84",
      "22f0c566b1e84415a4df68ce85ba2940",
      "fae11149bf684ef0803fbcd0fa4ce6f0",
      "2dcd3d5224cf42dc9d46b00eb1f84c59",
      "fb5ef3cfc0e143caa20a81824a3f7596",
      "1ebbf46483864b0080e5ffee82bb5f8c",
      "e142cbdfa66941fba47fbb93c36d6776",
      "25c8b5c5ef38486f94ffa3af6ab4674e",
      "ba56a8a3e951454ea17717b754a60383",
      "b2761db375ed401b99d1a59fef5dcc57",
      "3b67c923023944b8a306190719df4c93",
      "01e53b31eebd4a1ca26d11040bcdedbf",
      "26809486bea84d8bad34827ba420c316",
      "0072cd435347438c842245818f9a5ec6",
      "9786eefbd37b4c92b77e878216a10ac0",
      "662ba5d6c58047cb99a58f4e6ed92f00",
      "4ebf212b49604116ba04bfb42d090199",
      "42e594681a844633a8fc13977f695539",
      "5709e500046240938a7c4fc83deb6f83",
      "8f6f96b4ea33432dafbfd1197eea0f28",
      "aa752f8c13e543f8a1d228a7f7fd7aec",
      "132d3c773337426fa8cfe56ec177f2af",
      "c7b6a57e5818487fb100d89d84f40ea4",
      "a4f3f29846074639a369b57b28ccfbd4",
      "8a4c41494ed54c6797c6dab0b8dd676d"
     ]
    },
    "id": "jGV9VS4bhq7f",
    "outputId": "f585566a-d8c7-4abc-d93b-a40933470a86"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4e81be8e5ffc43099d1768f2b379c2ce"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0962ea3d1fa34efcace74c949a3f636a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ff44125d52054f29ba06ceae0a6821b1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cf7e368bd70e479bb003b8688deb27d5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4e256742aed947089df526b210c16290"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3a2239bbac764e18a6d3e54bd8e24385"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "66d1c59207bd4b579b17f91a173b64d3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9b9b1d35635e453e9dc7be625b5e22ab"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0ec62022b7f14c17aaf7ec388bf7a35f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8165eeff3f4b453fa6236e8fab4f6fcb"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "da6359244454407c847ec5af84461ff4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/267 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2dcd3d5224cf42dc9d46b00eb1f84c59"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/34 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9786eefbd37b4c92b77e878216a10ac0"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load model\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# Convert text to embeddings\n",
    "train_embeddings = model.encode(data[\"train\"][\"text\"], show_progress_bar=True)\n",
    "test_embeddings = model.encode(data[\"test\"][\"text\"], show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B5L5CLcOxIeA",
    "outputId": "8df2014c-4d1e-41b8-e686-30f539c22b08"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(8530, 768)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "train_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "id": "8A7oTxoph6bn",
    "outputId": "87cff4c5-f6f7-4cc2-c226-d0bed2eb5ab5"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=42)"
      ],
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"\u25b8\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"\u25be\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(random_state=42)</pre></div> </div></div></div></div>"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train a Logistic Regression on our train embeddings\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(train_embeddings, data[\"train\"][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tFvO9KhMokF7",
    "outputId": "e615b92d-b487-4ed0-f7e0-d313d55fe093"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Negative Review       0.85      0.86      0.85       533\n",
      "Positive Review       0.86      0.85      0.85       533\n",
      "\n",
      "       accuracy                           0.85      1066\n",
      "      macro avg       0.85      0.85      0.85      1066\n",
      "   weighted avg       0.85      0.85      0.85      1066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict previously unseen instances\n",
    "y_pred = clf.predict(test_embeddings)\n",
    "evaluate_performance(data[\"test\"][\"label\"], y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwGIHxXpJgrC"
   },
   "source": [
    "**Tip!**  \n",
    "\n",
    "What would happen if we would not use a classifier at all? Instead, we can average the embeddings per class and apply cosine similarity to predict which classes match the documents best:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3f_DnG1uJ7Sk",
    "outputId": "fc8fb382-b6e9-481a-9c70-8ac1a1a5c96a"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Negative Review       0.85      0.84      0.84       533\n",
      "Positive Review       0.84      0.85      0.84       533\n",
      "\n",
      "       accuracy                           0.84      1066\n",
      "      macro avg       0.84      0.84      0.84      1066\n",
      "   weighted avg       0.84      0.84      0.84      1066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Average the embeddings of all documents in each target label\n",
    "df = pd.DataFrame(np.hstack([train_embeddings, np.array(data[\"train\"][\"label\"]).reshape(-1, 1)]))\n",
    "averaged_target_embeddings = df.groupby(768).mean().values\n",
    "\n",
    "# Find the best matching embeddings between evaluation documents and target embeddings\n",
    "sim_matrix = cosine_similarity(test_embeddings, averaged_target_embeddings)\n",
    "y_pred = np.argmax(sim_matrix, axis=1)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_performance(data[\"test\"][\"label\"], y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCWdzjMIjzx0"
   },
   "source": [
    "### Zero-shot Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "YSj6CdAetsNp"
   },
   "outputs": [],
   "source": [
    "# Create embeddings for our labels\n",
    "label_embeddings = model.encode([\"A negative review\",  \"A positive review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ZEIN7XnbtsQJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Find the best matching label for each document\n",
    "sim_matrix = cosine_similarity(test_embeddings, label_embeddings)\n",
    "y_pred = np.argmax(sim_matrix, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u6LyeuEUxIbW",
    "outputId": "97ecffce-64bc-406e-c4a1-e339d5d38661"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Negative Review       0.78      0.77      0.78       533\n",
      "Positive Review       0.77      0.79      0.78       533\n",
      "\n",
      "       accuracy                           0.78      1066\n",
      "      macro avg       0.78      0.78      0.78      1066\n",
      "   weighted avg       0.78      0.78      0.78      1066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_performance(data[\"test\"][\"label\"], y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ox27Rg71zclg"
   },
   "source": [
    "**Tip!**  \n",
    "\n",
    "What would happen if you were to use different descriptions? Use **\"A very negative movie review\"** and **\"A very positive movie review\"** to see what happens!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4CC9iEGcuUit"
   },
   "source": [
    "## **Classification with Generative Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFPPzUHoEESB"
   },
   "source": [
    "### Encoder-decoder Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "nVbTUMktEfJ3",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343,
     "referenced_widgets": [
      "e3c53af41c05458b91c8cb4cb69fba8a",
      "32f800b9e8804e979cbd8aadd4571254",
      "ae1b8719376041f0a458c2d7b0f99408",
      "a11cebfe657841c5b8024e6d52b0063a",
      "5565c64a2fe24d7ca9cf3d661cce5bb4",
      "fc6d796c33d945e69401032520bef25d",
      "1910b69faa9447ffbfb5e198ef2d01d0",
      "b78c4127c28947d0a15059dbc3948862",
      "c5044f7398254708bea7f0125768c60d",
      "a08f96a06fd1441481350d4f48f09dc6",
      "2f4dec3cf6db49eb97cb2a346f1f7782",
      "1f82b98231d744f6ad34b4fed6384f7d",
      "313e7d753b70480c94f60f75213dfeda",
      "5e816195da1a4fcdb9c72aa605fd3956",
      "f310ebc9821f49ae8096313253bd423a",
      "d4e2e5fb626941d5becad8f1262cd047",
      "ace3ace81d844f8dbb57c249e4471551",
      "4a62f65d928e49ef997855445372a6ea",
      "97a4a4526cc84972bae0bd0ef45803b7",
      "202e73910ddd437a818ff575ebfffc5e",
      "3be66085a1a541fc94f026ce4d5f1dc1",
      "7b4175623f7f450fb9c13142fa5df589",
      "98cfd3afaec74253bfb56598bc537d60",
      "f1aef66b2467475bb60851c07f9fd03d",
      "cca8f5e8f43d4a9ca8b2a301350499e4",
      "f282524923ba40c283e35eaa8d8973a6",
      "b7f204ddbb0a4ef493b448640140827f",
      "b5461053b1374b60aff966cb0c26ebf6",
      "0395d66d89cb47ed85b69b2bce090e40",
      "70130491fbc9424cb5eb9b728d03ccf3",
      "fb805f536d3347e2961f7543ad3b03e0",
      "dd0547a8c7f84d36b2be515b136ca497",
      "7b00da40af594ae19147c74b3e37b8b4",
      "446e7f92920940908ba16181855828aa",
      "f968e67ffcea4ef3943ccd6fefc11b1a",
      "cec59a1dba4444548e19015cbe4255f3",
      "306647e5f13b4c15a2d7710549f7fc3f",
      "dab122cefd714332b80bfea6bb7d1d2c",
      "3756a087647b4c45913ea4b3fbd71cda",
      "4b360fd029ae416e9417898f6c43dcf1",
      "ebd2071e9c054e9dafc323f9ffa69f56",
      "721e7b44907b4b5e8c7d015b8906db79",
      "ed0483749d114f15bf37b844739a9234",
      "ede8e21ca6814d36b4bd5bdb847f70dc",
      "e9c027409c9e49368d0cc36b5d62c52c",
      "4bff2a2a0bf0472abe011c2b1ded9d4b",
      "5c88c245bc244c12aae4898400f7a053",
      "4b633b7133b34d1ba5ba62c8916003ed",
      "358e7cef13554483a708050d45396f72",
      "f668805e6b094b3f9a205e697dd4f6c4",
      "4fc88a1f6e6543e19d0aea370bab49f6",
      "4a3a23c9ff1742be896976e832bd83ac",
      "d02b9e66f5304c089c7beaa1af709816",
      "1fd40b5711174c999dedc1624ee9cf68",
      "30119dd2b3e74e3e9eaf7e82a25613bb",
      "2348e8f1a3634ab0a21512ecd3d00a71",
      "e659fc84b605407c8698f6046295b6d2",
      "99445d5215b14415b1213a8553ef67f8",
      "5fafa1e845a040e085902c32e0a6d8f1",
      "4aee19ea1fb248fca3c22f75f775f128",
      "72eb81a28e98419697a3a4250a0f33ee",
      "70759d5046d148c7a7414ee7ac4182aa",
      "18a56bfe25994f78b2e9a713138ece21",
      "11d379e1c59d45949df0a98e3c38088e",
      "9c22d0584a1d4156a92807c930b22447",
      "b9e3266f8ef241e18ee2ab60acc1368f",
      "7066c85998e54a0585eb7319941aee51",
      "2a097c1ea6f04e0297ab1f405495d2e6",
      "174a0595415448f7ad17ba019fc31170",
      "d7bf9447859049859be60fe1323d66c2",
      "6c9700179b1c4c638be98c7d62716ae8",
      "e335b820cc0a4c27a1d5343b3e91e06a",
      "1c4968d9f46e442eb6889b2f9338adc2",
      "872814d874304911b8263611db91ef5b",
      "660a831c47a94a41866f689cc876e049",
      "bf466d43270441aca764f96766a16573",
      "eaea684194e048b395dd2cb94dd84bf0"
     ]
    },
    "outputId": "0211e7cc-0836-4b0d-d526-05d3ee92b661"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e3c53af41c05458b91c8cb4cb69fba8a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1f82b98231d744f6ad34b4fed6384f7d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "98cfd3afaec74253bfb56598bc537d60"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "446e7f92920940908ba16181855828aa"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e9c027409c9e49368d0cc36b5d62c52c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2348e8f1a3634ab0a21512ecd3d00a71"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7066c85998e54a0585eb7319941aee51"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load our model\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"google/flan-t5-small\",\n",
    "    device=\"cuda:0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446,
     "referenced_widgets": [
      "f371013970fa447199a0672409ca70d9",
      "f2ce79614eb240c9b2eb96a71ac3f532",
      "e96ac2968930406c895c0203b8b92dd2",
      "db31757768e447b5b87635793fd94496",
      "d074df2d615344e8a8e4569fc8fe5ff7",
      "1d0ad5f0de564c289aa20c650e6adfcc",
      "0526f08248534ce080105dbfb6c901fa",
      "bcbc5ea6342a4fd6b2da578ed9258092",
      "69cddc90c5eb420d9ba3e369fa63b112",
      "9398d573b052468daeb05caa4255b30f",
      "6bee68353c6f431180a1064057b42360",
      "589e03f4c1644a73a4771fb39ad0e87e",
      "1944ae02aec34fd3878e2da29a95679b",
      "25f3517569744349b1c511808ca5becc",
      "d9102e55ed214a6a8d7f2959b31be319",
      "2cde42e7d54c46849aac2bbaeeec93c7",
      "5dcee430ce99434e9fc6b1cb315cd5d8",
      "657ff7ae6829438d90b9c976e2245630",
      "3c94bf0d5f8e43f0a35763d1a68d2e9c",
      "c1d3b184bdcb44fcbdc5b0a086c36194",
      "e79ae4f58417440c9633cbcfc3fb6b39",
      "d91225a41d3b4c899195ff79a10b3e79",
      "e6200d97c09e4ae181d95717fbf8b0ab",
      "8cc5f640bf3f45229b211df862d56bfa",
      "9af77df882c04a938c5121acc1f474cb",
      "b84801470689411181a7bddcdcf9d9b3",
      "3a8aba383ca442cfb37c889a01ec7c56",
      "86df831bdf7e46f09014d61edfdb43ae",
      "a84a781fe2bd4ae68261b3da62633766",
      "6e2d1a2fc4834dd6a3e0279082c329d6",
      "1e17a5653e0340e9afbc37b952ea55f9",
      "0877cc0218f04641a1f00787cd20f6e5",
      "4cc3afb2051c465cbca05af9dd76f098"
     ]
    },
    "id": "o5nWQORcFlNn",
    "outputId": "22e0ed41-c2fc-4d27-f9c2-429368f19415"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f371013970fa447199a0672409ca70d9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "589e03f4c1644a73a4771fb39ad0e87e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e6200d97c09e4ae181d95717fbf8b0ab"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 't5'],\n",
       "        num_rows: 8530\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 't5'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 't5'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "# Prepare our data\n",
    "prompt = \"Is the following sentence positive or negative? \"\n",
    "data = data.map(lambda example: {\"t5\": prompt + example['text']})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nas574KFFSvR",
    "outputId": "43d00d4d-6ac1-4bb3-8ade-e6d650d170e5"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1066/1066 [00:55<00:00, 19.05it/s]\n"
     ]
    }
   ],
   "source": [
    "# Run inference\n",
    "y_pred = []\n",
    "for output in tqdm(pipe(KeyDataset(data[\"test\"], \"t5\")), total=len(data[\"test\"])):\n",
    "    text = output[0][\"generated_text\"]\n",
    "    y_pred.append(0 if text == \"negative\" else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Wk2i856GnCv",
    "outputId": "27546e63-a211-4a39-bd01-f0a9588220bb"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Negative Review       0.83      0.85      0.84       533\n",
      "Positive Review       0.85      0.83      0.84       533\n",
      "\n",
      "       accuracy                           0.84      1066\n",
      "      macro avg       0.84      0.84      0.84      1066\n",
      "   weighted avg       0.84      0.84      0.84      1066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_performance(data[\"test\"][\"label\"], y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4V9iq_EELWx"
   },
   "source": [
    "### ChatGPT for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "tES6HFOwNjF6"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Create client\n",
    "client = openai.OpenAI(api_key=\"YOUR_KEY_HERE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "dGiovm3wyCOz"
   },
   "outputs": [],
   "source": [
    "def chatgpt_generation(prompt, document, model=\"gpt-3.5-turbo-0125\"):\n",
    "    \"\"\"Generate an output based on a prompt and an input document.\"\"\"\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\"\n",
    "            },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\":   prompt.replace(\"[DOCUMENT]\", document)\n",
    "            }\n",
    "    ]\n",
    "    chat_completion = client.chat.completions.create(\n",
    "      messages=messages,\n",
    "      model=model,\n",
    "      temperature=0\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "id": "qL_kMwQEvMcd",
    "outputId": "c4d38322-4195-4f81-b055-eab69e17607d"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: YOUR_KEY*HERE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2321951056.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Predict the target using GPT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"unpretentious , charming , quirky , original\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mchatgpt_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipython-input-1298530805.py\u001b[0m in \u001b[0;36mchatgpt_generation\u001b[0;34m(prompt, document, model)\u001b[0m\n\u001b[1;32m     11\u001b[0m             }\n\u001b[1;32m     12\u001b[0m     ]\n\u001b[0;32m---> 13\u001b[0;31m     chat_completion = client.chat.completions.create(\n\u001b[0m\u001b[1;32m     14\u001b[0m       \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m   1146\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m             body=maybe_transform(\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: YOUR_KEY*HERE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "# Define a prompt template as a base\n",
    "prompt = \"\"\"Predict whether the following document is a positive or negative movie review:\n",
    "\n",
    "[DOCUMENT]\n",
    "\n",
    "If it is positive return 1 and if it is negative return 0. Do not give any other answers.\n",
    "\"\"\"\n",
    "\n",
    "# Predict the target using GPT\n",
    "document = \"unpretentious , charming , quirky , original\"\n",
    "chatgpt_generation(prompt, document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ea-8XYpY3jp6"
   },
   "source": [
    "The next step would be to run one of OpenAI's model against the entire evaluation dataset. However, only run this when you have sufficient tokens as this will call the API for the entire test dataset (1066 records)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "id": "gEyGElIv25Aq",
    "outputId": "f6e758d6-b807-472a-8a1f-4be01d6c1ac9"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/1066 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: YOUR_KEY*HERE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1878078064.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# You can skip this if you want to save your (free) credits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mchatgpt_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipython-input-1298530805.py\u001b[0m in \u001b[0;36mchatgpt_generation\u001b[0;34m(prompt, document, model)\u001b[0m\n\u001b[1;32m     11\u001b[0m             }\n\u001b[1;32m     12\u001b[0m     ]\n\u001b[0;32m---> 13\u001b[0;31m     chat_completion = client.chat.completions.create(\n\u001b[0m\u001b[1;32m     14\u001b[0m       \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m   1146\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m             body=maybe_transform(\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: YOUR_KEY*HERE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "# You can skip this if you want to save your (free) credits\n",
    "predictions = [chatgpt_generation(prompt, doc) for doc in tqdm(data[\"test\"][\"text\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B07O8wtZ05x1",
    "outputId": "48ab3146-85e7-4d92-e550-04bd98e39cb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Negative Review       0.87      0.97      0.92       533\n",
      "Positive Review       0.96      0.86      0.91       533\n",
      "\n",
      "       accuracy                           0.91      1066\n",
      "      macro avg       0.92      0.91      0.91      1066\n",
      "   weighted avg       0.92      0.91      0.91      1066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract predictions\n",
    "y_pred = [int(pred) for pred in predictions]\n",
    "\n",
    "# Evaluate performance\n",
    "evaluate_performance(data[\"test\"][\"label\"], y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Your Turn - Text Classification Experiments\n",
    "\n",
    "Run each task first to see the baseline results. Follow the instructions to modify and experiment."
   ],
   "metadata": {
    "id": "NKYNfoaVC4hU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This section is divided into EASY, MEDIUM, & HARD."
   ],
   "metadata": {
    "id": "hHVONn85DElL"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Easy Tasks - Hands-On Exploration"
   ],
   "metadata": {
    "id": "gwqUbziIDNee"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Easy Task 1: Zero-Shot Classifier\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Execute the code to see baseline predictions for 3 basic reviews\n",
    "2. Uncomment the larger `test_reviews` list and run again to test harder cases\n",
    "3. Uncomment one label option to see how wording affects predictions\n",
    "4. Compare which label style works best for ambiguous reviews"
   ],
   "metadata": {
    "id": "TnHfnLsxDQcA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# Test reviews - THESE WORK AS-IS\n",
    "test_reviews = [\n",
    "    \"This movie was absolutely fantastic! A masterpiece!\",\n",
    "    \"Terrible waste of time. Very disappointing.\",\n",
    "    \"An okay film, nothing special but watchable.\",\n",
    "]\n",
    "\n",
    "# TODO: After running once, uncomment these to test harder cases:\n",
    "# test_reviews = [\n",
    "#     \"This movie was absolutely fantastic! A masterpiece!\",\n",
    "#     \"Terrible waste of time. Very disappointing.\",\n",
    "#     \"An okay film, nothing special but watchable.\",\n",
    "#     \"Oh great, another masterpiece... NOT!\",  # Sarcastic\n",
    "#     \"Boring.\",  # Very short\n",
    "#     \"Great acting but terrible plot.\",  # Mixed sentiment\n",
    "# ]\n",
    "\n",
    "# Label descriptions - THESE WORK AS-IS\n",
    "labels = [\n",
    "    \"A negative movie review\",\n",
    "    \"A positive movie review\"\n",
    "]\n",
    "\n",
    "# TODO: After seeing baseline, try uncommenting ONE of these:\n",
    "# labels = [\"negative\", \"positive\"]  # Option 1: Simple\n",
    "# labels = [\"bad movie review\", \"good movie review\"]  # Option 2: Different wording\n",
    "# labels = [\"a scathing negative movie review\", \"an enthusiastic positive movie review\"]  # Option 3: Detailed\n",
    "\n",
    "# Create embeddings and calculate similarity\n",
    "label_embeddings = model.encode(labels)\n",
    "review_embeddings = model.encode(test_reviews)\n",
    "sim_matrix = cosine_similarity(review_embeddings, label_embeddings)\n",
    "\n",
    "print(\"Classification Results:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, review in enumerate(test_reviews):\n",
    "    prediction = np.argmax(sim_matrix[i])\n",
    "    confidence = sim_matrix[i][prediction]\n",
    "    margin = abs(sim_matrix[i][0] - sim_matrix[i][1])\n",
    "\n",
    "    print(f\"\\nReview {i+1}: '{review}'\")\n",
    "    print(f\"Predicted: {labels[prediction]}\")\n",
    "    print(f\"Confidence: {confidence:.3f}\")\n",
    "    print(f\"Scores -> Negative: {sim_matrix[i][0]:.3f}, Positive: {sim_matrix[i][1]:.3f}\")\n",
    "    print(f\"Margin (certainty): {margin:.3f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZC6N2A3LC6JZ",
    "outputId": "f1423ca6-a625-4355-d290-3cd8b5ab4371"
   },
   "execution_count": 26,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classification Results:\n",
      "================================================================================\n",
      "\n",
      "Review 1: 'This movie was absolutely fantastic! A masterpiece!'\n",
      "Predicted: A positive movie review\n",
      "Confidence: 0.493\n",
      "Scores -> Negative: 0.382, Positive: 0.493\n",
      "Margin (certainty): 0.111\n",
      "\n",
      "Review 2: 'Terrible waste of time. Very disappointing.'\n",
      "Predicted: A negative movie review\n",
      "Confidence: 0.439\n",
      "Scores -> Negative: 0.439, Positive: 0.299\n",
      "Margin (certainty): 0.140\n",
      "\n",
      "Review 3: 'An okay film, nothing special but watchable.'\n",
      "Predicted: A positive movie review\n",
      "Confidence: 0.542\n",
      "Scores -> Negative: 0.503, Positive: 0.542\n",
      "Margin (certainty): 0.039\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Questions:**\n",
    "\n",
    "1. How did the classifier handle the sarcastic review (\"Oh great, another masterpiece... NOT!\")? What semantic features did embeddings miss?\n",
    "\n",
    "2. Which reviews changed predictions when you modified label descriptions? Why are some reviews more sensitive to label wording than others?\n",
    "\n",
    "3. Which reviews have low confidence margins (<0.1)? What linguistic features make certain reviews harder to classify?"
   ],
   "metadata": {
    "id": "hAfvoaoTDidJ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Easy Task 2: Classifier Strategy Analysis\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Execute code to see three pre-built classifiers (conservative, aggressive, balanced)\n",
    "2. Study each confusion matrix to identify error patterns\n",
    "3. Modify `classifier_yours` to create a very conservative classifier (precision > 0.9)\n",
    "4. Uncomment the TODO section to analyze your classifier\n",
    "5. Experiment with creating different strategy combinations"
   ],
   "metadata": {
    "id": "Rhuy6BY6DxDr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# True labels: first 5 are negative (0), last 5 are positive (1)\n",
    "y_true = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])\n",
    "\n",
    "# Pre-built classifiers to analyze\n",
    "classifier_conservative = np.array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1])  # Rarely predicts positive\n",
    "classifier_aggressive = np.array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1])     # Often predicts positive\n",
    "classifier_balanced = np.array([0, 0, 0, 1, 0, 0, 1, 1, 1, 1])       # Balanced approach\n",
    "\n",
    "# TODO: Modify these predictions to make YOUR classifier\n",
    "# Goal: Try to achieve precision > 0.9 (be very selective about predicting 1)\n",
    "classifier_yours = np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1])\n",
    "\n",
    "def analyze_classifier(name, y_true, y_pred):\n",
    "    \"\"\"Analyze classifier performance with detailed breakdown\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{name}\")\n",
    "    print('='*70)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Show confusion matrix with labels\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"                    Predicted Neg | Predicted Pos\")\n",
    "    print(f\"Actual Neg (0):          {cm[0][0]}       |       {cm[0][1]} <- False Positives (BAD)\")\n",
    "    print(f\"Actual Pos (1):          {cm[1][0]}       |       {cm[1][1]} <- True Positives (GOOD)\")\n",
    "    print(f\"                          \u2191                           \")\n",
    "    print(f\"                   False Negatives (BAD)              \")\n",
    "\n",
    "    # Calculate metrics\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "    print(f\"\\nMetrics:\")\n",
    "    print(f\"Precision: {precision:.3f} = TP/(TP+FP) = {cm[1][1]}/({cm[1][1]}+{cm[0][1]})\")\n",
    "    print(f\"           \u2192 Of {cm[1][1]+cm[0][1]} positive predictions, {cm[1][1]} were correct\")\n",
    "    print(f\"\\nRecall:    {recall:.3f} = TP/(TP+FN) = {cm[1][1]}/({cm[1][1]}+{cm[1][0]})\")\n",
    "    print(f\"           \u2192 Of {cm[1][1]+cm[1][0]} actual positives, found {cm[1][1]}\")\n",
    "    print(f\"\\nF1 Score:  {f1:.3f} = 2*(P*R)/(P+R)\")\n",
    "\n",
    "    # Explain strategy\n",
    "    if precision > recall + 0.1:\n",
    "        print(f\"\\n\u2192 Strategy: CONSERVATIVE (careful about predicting positive)\")\n",
    "        print(f\"  \u2713 Few false alarms (only {cm[0][1]} false positives)\")\n",
    "        print(f\"  \u2717 Misses actual positives ({cm[1][0]} false negatives)\")\n",
    "    elif recall > precision + 0.1:\n",
    "        print(f\"\\n\u2192 Strategy: AGGRESSIVE (liberal about predicting positive)\")\n",
    "        print(f\"  \u2713 Finds most positives (only {cm[1][0]} false negatives)\")\n",
    "        print(f\"  \u2717 Many false alarms ({cm[0][1]} false positives)\")\n",
    "    else:\n",
    "        print(f\"\\n\u2192 Strategy: BALANCED\")\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "# Analyze pre-built classifiers\n",
    "results = {}\n",
    "for name, classifier in [\n",
    "    (\"Conservative Classifier\", classifier_conservative),\n",
    "    (\"Aggressive Classifier\", classifier_aggressive),\n",
    "    (\"Balanced Classifier\", classifier_balanced),\n",
    "]:\n",
    "    p, r, f = analyze_classifier(name, y_true, classifier)\n",
    "    results[name] = (p, r, f)\n",
    "\n",
    "# TODO: After modifying classifier_yours above, uncomment these lines:\n",
    "# print(\"\\n\" + \"=\"*70)\n",
    "# print(\"ANALYZING YOUR CLASSIFIER\")\n",
    "# print(\"=\"*70)\n",
    "# p, r, f = analyze_classifier(\"YOUR Classifier\", y_true, classifier_yours)\n",
    "# results[\"YOUR Classifier\"] = (p, r, f)\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Classifier':<25} {'Precision':<12} {'Recall':<12} {'F1':<12}\")\n",
    "print(\"-\"*70)\n",
    "for name, (p, r, f) in results.items():\n",
    "    print(f\"{name:<25} {p:.3f}        {r:.3f}       {f:.3f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Sw3mcCXDddB",
    "outputId": "9c042055-b756-419d-ec81-ce1e08ce5958"
   },
   "execution_count": 27,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "======================================================================\n",
      "Conservative Classifier\n",
      "======================================================================\n",
      "\n",
      "Confusion Matrix:\n",
      "                    Predicted Neg | Predicted Pos\n",
      "Actual Neg (0):          5       |       0 <- False Positives (BAD)\n",
      "Actual Pos (1):          2       |       3 <- True Positives (GOOD)\n",
      "                          \u2191                           \n",
      "                   False Negatives (BAD)              \n",
      "\n",
      "Metrics:\n",
      "Precision: 1.000 = TP/(TP+FP) = 3/(3+0)\n",
      "           \u2192 Of 3 positive predictions, 3 were correct\n",
      "\n",
      "Recall:    0.600 = TP/(TP+FN) = 3/(3+2)\n",
      "           \u2192 Of 5 actual positives, found 3\n",
      "\n",
      "F1 Score:  0.750 = 2*(P*R)/(P+R)\n",
      "\n",
      "\u2192 Strategy: CONSERVATIVE (careful about predicting positive)\n",
      "  \u2713 Few false alarms (only 0 false positives)\n",
      "  \u2717 Misses actual positives (2 false negatives)\n",
      "\n",
      "======================================================================\n",
      "Aggressive Classifier\n",
      "======================================================================\n",
      "\n",
      "Confusion Matrix:\n",
      "                    Predicted Neg | Predicted Pos\n",
      "Actual Neg (0):          1       |       4 <- False Positives (BAD)\n",
      "Actual Pos (1):          0       |       5 <- True Positives (GOOD)\n",
      "                          \u2191                           \n",
      "                   False Negatives (BAD)              \n",
      "\n",
      "Metrics:\n",
      "Precision: 0.556 = TP/(TP+FP) = 5/(5+4)\n",
      "           \u2192 Of 9 positive predictions, 5 were correct\n",
      "\n",
      "Recall:    1.000 = TP/(TP+FN) = 5/(5+0)\n",
      "           \u2192 Of 5 actual positives, found 5\n",
      "\n",
      "F1 Score:  0.714 = 2*(P*R)/(P+R)\n",
      "\n",
      "\u2192 Strategy: AGGRESSIVE (liberal about predicting positive)\n",
      "  \u2713 Finds most positives (only 0 false negatives)\n",
      "  \u2717 Many false alarms (4 false positives)\n",
      "\n",
      "======================================================================\n",
      "Balanced Classifier\n",
      "======================================================================\n",
      "\n",
      "Confusion Matrix:\n",
      "                    Predicted Neg | Predicted Pos\n",
      "Actual Neg (0):          4       |       1 <- False Positives (BAD)\n",
      "Actual Pos (1):          1       |       4 <- True Positives (GOOD)\n",
      "                          \u2191                           \n",
      "                   False Negatives (BAD)              \n",
      "\n",
      "Metrics:\n",
      "Precision: 0.800 = TP/(TP+FP) = 4/(4+1)\n",
      "           \u2192 Of 5 positive predictions, 4 were correct\n",
      "\n",
      "Recall:    0.800 = TP/(TP+FN) = 4/(4+1)\n",
      "           \u2192 Of 5 actual positives, found 4\n",
      "\n",
      "F1 Score:  0.800 = 2*(P*R)/(P+R)\n",
      "\n",
      "\u2192 Strategy: BALANCED\n",
      "\n",
      "======================================================================\n",
      "SUMMARY COMPARISON\n",
      "======================================================================\n",
      "Classifier                Precision    Recall       F1          \n",
      "----------------------------------------------------------------------\n",
      "Conservative Classifier   1.000        0.600       0.750\n",
      "Aggressive Classifier     0.556        1.000       0.714\n",
      "Balanced Classifier       0.800        0.800       0.800\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Questions:**\n",
    "\n",
    "1. The conservative classifier has 2 false negatives. What real-world mistake does this represent? Provide a movie review example.\n",
    "\n",
    "2. What strategy did you use to achieve high precision in `classifier_yours`? Why does predicting positive less frequently increase precision?\n",
    "\n",
    "3. Which classifier won on F1 score? Why doesn't the aggressive classifier win despite high recall?"
   ],
   "metadata": {
    "id": "GHv-B_ebD8CU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Easy Task 3: Temperature Effects on Text Generation\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Execute code to see how temperature affects token selection with a confident model\n",
    "2. Observe how probabilities and samples change across temperatures\n",
    "3. Uncomment the uncertain probability distribution and run again\n",
    "4. Compare temperature effects on confident vs uncertain models\n",
    "5. Uncomment TODO to add a new temperature value and analyze results"
   ],
   "metadata": {
    "id": "YkEaGnlqEEWU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Starting with a CONFIDENT model (one token much more likely)\n",
    "original_probs = np.array([0.50, 0.30, 0.12, 0.05, 0.03])\n",
    "tokens = [\"positive\", \"negative\", \"neutral\", \"good\", \"bad\"]\n",
    "\n",
    "# TODO: After first run, uncomment this UNCERTAIN distribution:\n",
    "# original_probs = np.array([0.25, 0.24, 0.22, 0.18, 0.11])  # Much more balanced!\n",
    "# Run again and compare the temperature effects\n",
    "\n",
    "def apply_temperature(probs, temperature):\n",
    "    \"\"\"Apply temperature scaling to change distribution sharpness\"\"\"\n",
    "    if temperature == 0:\n",
    "        # Deterministic: always pick the highest\n",
    "        result = np.zeros_like(probs)\n",
    "        result[np.argmax(probs)] = 1.0\n",
    "        return result\n",
    "\n",
    "    # Apply temperature scaling\n",
    "    logits = np.log(probs + 1e-10)\n",
    "    scaled_logits = logits / temperature\n",
    "    exp_logits = np.exp(scaled_logits)\n",
    "    return exp_logits / np.sum(exp_logits)\n",
    "\n",
    "def visualize_distribution(probs, tokens):\n",
    "    \"\"\"Show probability distribution as bar chart\"\"\"\n",
    "    for i, token in enumerate(tokens):\n",
    "        bar_length = int(probs[i] * 100)\n",
    "        bar = '\u2588' * bar_length\n",
    "        print(f\"  {token:10s}: {probs[i]:.3f} {bar}\")\n",
    "\n",
    "# Test different temperatures\n",
    "temperatures = [0, 0.5, 1.0, 2.0]\n",
    "\n",
    "# TODO: After analyzing the results, uncomment this to add temperature=3.0:\n",
    "# temperatures = [0, 0.5, 1.0, 2.0, 3.0]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"Original (temperature=1.0) probabilities:\")\n",
    "print(\"=\"*70)\n",
    "visualize_distribution(original_probs, tokens)\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Temperature = {temp}\")\n",
    "    print('='*70)\n",
    "\n",
    "    # Apply temperature\n",
    "    new_probs = apply_temperature(original_probs, temp)\n",
    "\n",
    "    # Visualize\n",
    "    visualize_distribution(new_probs, tokens)\n",
    "\n",
    "    # Sample tokens\n",
    "    print(f\"\\n  Sampling 10 tokens:\")\n",
    "    if temp == 0:\n",
    "        samples = [tokens[np.argmax(new_probs)]] * 10\n",
    "    else:\n",
    "        samples = np.random.choice(tokens, size=10, p=new_probs)\n",
    "\n",
    "    print(f\"  {samples}\")\n",
    "\n",
    "    # Show diversity metric\n",
    "    unique_tokens = len(set(samples))\n",
    "    print(f\"  \u2192 Diversity: {unique_tokens}/10 unique tokens\")\n",
    "\n",
    "    # Explain what's happening\n",
    "    if temp == 0:\n",
    "        print(f\"  \u2192 Effect: DETERMINISTIC - always outputs '{samples[0]}'\")\n",
    "    elif temp < 1.0:\n",
    "        print(f\"  \u2192 Effect: SHARPENED - makes confident tokens more likely\")\n",
    "    elif temp == 1.0:\n",
    "        print(f\"  \u2192 Effect: UNCHANGED - original distribution\")\n",
    "    else:\n",
    "        print(f\"  \u2192 Effect: FLATTENED - makes all tokens more equally likely\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ls7jVLYD1qa",
    "outputId": "441af855-8a20-4d81-88c6-127538c77ecd"
   },
   "execution_count": 28,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "======================================================================\n",
      "Original (temperature=1.0) probabilities:\n",
      "======================================================================\n",
      "  positive  : 0.500 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n",
      "  negative  : 0.300 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n",
      "  neutral   : 0.120 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n",
      "  good      : 0.050 \u2588\u2588\u2588\u2588\u2588\n",
      "  bad       : 0.030 \u2588\u2588\u2588\n",
      "\n",
      "======================================================================\n",
      "Temperature = 0\n",
      "======================================================================\n",
      "  positive  : 1.000 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n",
      "  negative  : 0.000 \n",
      "  neutral   : 0.000 \n",
      "  good      : 0.000 \n",
      "  bad       : 0.000 \n",
      "\n",
      "  Sampling 10 tokens:\n",
      "  ['positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive']\n",
      "  \u2192 Diversity: 1/10 unique tokens\n",
      "  \u2192 Effect: DETERMINISTIC - always outputs 'positive'\n",
      "\n",
      "======================================================================\n",
      "Temperature = 0.5\n",
      "======================================================================\n",
      "  positive  : 0.699 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n",
      "  negative  : 0.252 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n",
      "  neutral   : 0.040 \u2588\u2588\u2588\u2588\n",
      "  good      : 0.007 \n",
      "  bad       : 0.003 \n",
      "\n",
      "  Sampling 10 tokens:\n",
      "  ['positive' 'positive' 'positive' 'neutral' 'positive' 'positive'\n",
      " 'positive' 'positive' 'negative' 'positive']\n",
      "  \u2192 Diversity: 3/10 unique tokens\n",
      "  \u2192 Effect: SHARPENED - makes confident tokens more likely\n",
      "\n",
      "======================================================================\n",
      "Temperature = 1.0\n",
      "======================================================================\n",
      "  positive  : 0.500 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n",
      "  negative  : 0.300 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n",
      "  neutral   : 0.120 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n",
      "  good      : 0.050 \u2588\u2588\u2588\u2588\u2588\n",
      "  bad       : 0.030 \u2588\u2588\u2588\n",
      "\n",
      "  Sampling 10 tokens:\n",
      "  ['positive' 'bad' 'positive' 'positive' 'good' 'good' 'negative'\n",
      " 'negative' 'positive' 'good']\n",
      "  \u2192 Diversity: 4/10 unique tokens\n",
      "  \u2192 Effect: UNCHANGED - original distribution\n",
      "\n",
      "======================================================================\n",
      "Temperature = 2.0\n",
      "======================================================================\n",
      "  positive  : 0.354 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n",
      "  negative  : 0.274 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n",
      "  neutral   : 0.173 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n",
      "  good      : 0.112 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n",
      "  bad       : 0.087 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n",
      "\n",
      "  Sampling 10 tokens:\n",
      "  ['positive' 'negative' 'positive' 'negative' 'positive' 'neutral'\n",
      " 'negative' 'positive' 'negative' 'negative']\n",
      "  \u2192 Diversity: 3/10 unique tokens\n",
      "  \u2192 Effect: FLATTENED - makes all tokens more equally likely\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Questions:**\n",
    "\n",
    "1. Why is temperature=0 critical for classification tasks? What would go wrong with temperature=1.0?\n",
    "\n",
    "2. Compare temperature=0.5 vs 2.0. At what temperature did low-probability tokens like \"bad\" start appearing in samples?\n",
    "\n",
    "3. With the uncertain distribution ([0.25, 0.24, 0.22, 0.18, 0.11]), how did temperature effects differ from the confident model?"
   ],
   "metadata": {
    "id": "pA4FHgvIENtG"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Easy Task 4: Embedding Similarity Analysis\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Execute code to see similarity matrix for movie reviews\n",
    "2. Identify which reviews cluster together and which are distant\n",
    "3. Uncomment TODO to add reviews from different domains\n",
    "4. Analyze whether restaurant/product reviews cluster with movie reviews\n",
    "5. Add a random sentence to test similarity boundaries"
   ],
   "metadata": {
    "id": "Y2tgqir4Ecu0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# Movie reviews - different types\n",
    "texts = [\n",
    "    # Positive reviews\n",
    "    \"Amazing movie! Absolutely loved it!\",\n",
    "    \"Fantastic film, highly recommend!\",\n",
    "    \"Great cinematography and acting\",\n",
    "\n",
    "    # Negative reviews\n",
    "    \"Terrible waste of time\",\n",
    "    \"Very disappointing and boring\",\n",
    "    \"Poor acting and weak plot\",\n",
    "\n",
    "    # Neutral reviews\n",
    "    \"It was okay, nothing special\",\n",
    "    \"Some good parts, some bad parts\",\n",
    "\n",
    "    # Off-topic\n",
    "    \"The weather is nice today\",\n",
    "    \"I like eating pizza\"\n",
    "]\n",
    "\n",
    "# TODO: After first run, uncomment these to test domain transfer:\n",
    "# texts = [\n",
    "#     # Positive movie reviews\n",
    "#     \"Amazing movie! Absolutely loved it!\",\n",
    "#     \"Fantastic film, highly recommend!\",\n",
    "#     \"Great cinematography and acting\",\n",
    "#\n",
    "#     # Negative movie reviews\n",
    "#     \"Terrible waste of time\",\n",
    "#     \"Very disappointing and boring\",\n",
    "#     \"Poor acting and weak plot\",\n",
    "#\n",
    "#     # Neutral movie reviews\n",
    "#     \"It was okay, nothing special\",\n",
    "#     \"Some good parts, some bad parts\",\n",
    "#\n",
    "#     # Positive restaurant review (different domain!)\n",
    "#     \"Amazing food! Absolutely loved it!\",\n",
    "#     \"Fantastic restaurant, highly recommend!\",\n",
    "#\n",
    "#     # Off-topic\n",
    "#     \"The weather is nice today\",\n",
    "#     \"I like eating pizza\",\n",
    "# ]\n",
    "\n",
    "labels = [f\"Text {i+1}\" for i in range(len(texts))]\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = model.encode(texts)\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "print(f\"Each text converted to {embeddings.shape[1]}-dimensional vector\")\n",
    "print(f\"Comparing {embeddings.shape[0]} texts\\n\")\n",
    "\n",
    "# Show full similarity matrix\n",
    "print(\"Similarity Matrix (0=unrelated, 1=identical):\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'':10s}\", end=\"\")\n",
    "for i in range(len(texts)):\n",
    "    print(f\"T{i+1:2d} \", end=\"\")\n",
    "print()\n",
    "\n",
    "for i in range(len(texts)):\n",
    "    print(f\"Text {i+1:2d}:  \", end=\"\")\n",
    "    for j in range(len(texts)):\n",
    "        if i == j:\n",
    "            print(\"---- \", end=\"\")\n",
    "        else:\n",
    "            sim = similarity_matrix[i][j]\n",
    "            if sim > 0.6:\n",
    "                print(f\"{sim:.2f}*\", end=\"\")  # High similarity\n",
    "            else:\n",
    "                print(f\"{sim:.2f} \", end=\"\")\n",
    "            print(\" \", end=\"\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n* = High similarity (>0.6)\")\n",
    "\n",
    "# Detailed comparisons\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED COMPARISONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparisons = [\n",
    "    (0, 1, \"Positive review vs Positive review\"),\n",
    "    (3, 4, \"Negative review vs Negative review\"),\n",
    "    (0, 3, \"Positive review vs Negative review\"),\n",
    "    (0, 8, \"Movie review vs Off-topic text\"),\n",
    "]\n",
    "\n",
    "# TODO: After adding restaurant reviews, uncomment this:\n",
    "# comparisons.append((0, 8, \"Positive MOVIE vs Positive RESTAURANT\"))\n",
    "\n",
    "for i, j, description in comparisons:\n",
    "    if i < len(texts) and j < len(texts):\n",
    "        sim = similarity_matrix[i][j]\n",
    "        print(f\"\\n{description}:\")\n",
    "        print(f\"  Text {i+1}: '{texts[i]}'\")\n",
    "        print(f\"  Text {j+1}: '{texts[j]}'\")\n",
    "        print(f\"  Similarity: {sim:.3f}\")\n",
    "\n",
    "        if sim > 0.7:\n",
    "            print(f\"  \u2192 Very similar! These texts are closely related in meaning\")\n",
    "        elif sim > 0.4:\n",
    "            print(f\"  \u2192 Moderately similar. Some shared concepts\")\n",
    "        else:\n",
    "            print(f\"  \u2192 Different topics or sentiments\")\n",
    "\n",
    "# Find clusters\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLUSTERS (which texts group together?)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find texts similar to first positive review\n",
    "positive_idx = 0\n",
    "similar_to_positive = []\n",
    "for i in range(len(texts)):\n",
    "    if i != positive_idx and similarity_matrix[positive_idx][i] > 0.5:\n",
    "        similar_to_positive.append((i, similarity_matrix[positive_idx][i]))\n",
    "\n",
    "print(f\"\\nTexts similar to '{texts[positive_idx]}':\")\n",
    "for idx, sim in sorted(similar_to_positive, key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  Text {idx+1} (sim={sim:.3f}): '{texts[idx]}'\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mahDQbyaEHUn",
    "outputId": "228a42e9-6c33-457d-966c-6e3c3004dc16"
   },
   "execution_count": 29,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Each text converted to 768-dimensional vector\n",
      "Comparing 10 texts\n",
      "\n",
      "Similarity Matrix (0=unrelated, 1=identical):\n",
      "================================================================================\n",
      "          T 1 T 2 T 3 T 4 T 5 T 6 T 7 T 8 T 9 T10 \n",
      "Text  1:  ---- 0.77* 0.52  0.09  0.24  0.23  0.31  0.16  0.10  0.03  \n",
      "Text  2:  0.77* ---- 0.47  0.11  0.26  0.23  0.21  0.14  0.09  0.01  \n",
      "Text  3:  0.52  0.47  ---- 0.10  0.32  0.51  0.36  0.26  0.12  0.06  \n",
      "Text  4:  0.09  0.11  0.10  ---- 0.55  0.29  0.34  0.21  0.03  0.07  \n",
      "Text  5:  0.24  0.26  0.32  0.55  ---- 0.50  0.55  0.36  0.09  0.13  \n",
      "Text  6:  0.23  0.23  0.51  0.29  0.50  ---- 0.37  0.25  -0.03  0.08  \n",
      "Text  7:  0.31  0.21  0.36  0.34  0.55  0.37  ---- 0.34  0.16  0.11  \n",
      "Text  8:  0.16  0.14  0.26  0.21  0.36  0.25  0.34  ---- 0.12  0.21  \n",
      "Text  9:  0.10  0.09  0.12  0.03  0.09  -0.03  0.16  0.12  ---- 0.20  \n",
      "Text 10:  0.03  0.01  0.06  0.07  0.13  0.08  0.11  0.21  0.20  ---- \n",
      "\n",
      "* = High similarity (>0.6)\n",
      "\n",
      "================================================================================\n",
      "DETAILED COMPARISONS\n",
      "================================================================================\n",
      "\n",
      "Positive review vs Positive review:\n",
      "  Text 1: 'Amazing movie! Absolutely loved it!'\n",
      "  Text 2: 'Fantastic film, highly recommend!'\n",
      "  Similarity: 0.768\n",
      "  \u2192 Very similar! These texts are closely related in meaning\n",
      "\n",
      "Negative review vs Negative review:\n",
      "  Text 4: 'Terrible waste of time'\n",
      "  Text 5: 'Very disappointing and boring'\n",
      "  Similarity: 0.545\n",
      "  \u2192 Moderately similar. Some shared concepts\n",
      "\n",
      "Positive review vs Negative review:\n",
      "  Text 1: 'Amazing movie! Absolutely loved it!'\n",
      "  Text 4: 'Terrible waste of time'\n",
      "  Similarity: 0.090\n",
      "  \u2192 Different topics or sentiments\n",
      "\n",
      "Movie review vs Off-topic text:\n",
      "  Text 1: 'Amazing movie! Absolutely loved it!'\n",
      "  Text 9: 'The weather is nice today'\n",
      "  Similarity: 0.095\n",
      "  \u2192 Different topics or sentiments\n",
      "\n",
      "================================================================================\n",
      "CLUSTERS (which texts group together?)\n",
      "================================================================================\n",
      "\n",
      "Texts similar to 'Amazing movie! Absolutely loved it!':\n",
      "  Text 2 (sim=0.768): 'Fantastic film, highly recommend!'\n",
      "  Text 3 (sim=0.516): 'Great cinematography and acting'\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Questions:**\n",
    "\n",
    "1. Compare similarity between Text 1 and Text 2 (both positive) vs Text 1 and Text 4 (positive vs negative). What aspects of semantic meaning do embeddings prioritize?\n",
    "\n",
    "2. Find similarity scores between two negative reviews (Text 4 and Text 5) and two positive reviews (Text 1 and Text 2). Why would averaging embeddings per class work for classification?\n",
    "\n",
    "3. After adding restaurant reviews: How similar was \"Amazing food!\" to \"Amazing movie!\"? What does this reveal about domain transfer?"
   ],
   "metadata": {
    "id": "TL1g6q0kEjPw"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Medium Tasks - Building Real Classifiers\n",
    "\n",
    "These tasks require more modification and experimentation. You'll build complete classification systems."
   ],
   "metadata": {
    "id": "2ipz0pJYEne6"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Medium Task 1: Multi-Class Sentiment Classifier with Custom Categories\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Execute code to see how the 5-level sentiment classifier works\n",
    "2. Analyze confusion matrix to identify which categories get confused\n",
    "3. Uncomment TODO to add a 6th sentiment level\n",
    "4. Write 3 reviews specifically for your new category\n",
    "5. Compare performance before and after adding the category"
   ],
   "metadata": {
    "id": "ddr-_SRIEtH8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# Test reviews covering different sentiment intensities\n",
    "test_reviews = [\n",
    "    \"This is the best movie I've ever seen! Absolute masterpiece!\",\n",
    "    \"Pretty good movie, I enjoyed it\",\n",
    "    \"It was okay, nothing special\",\n",
    "    \"Not great, pretty disappointing\",\n",
    "    \"Absolutely terrible, worst movie ever\",\n",
    "    \"Amazing performances and stunning visuals!\",\n",
    "    \"Mediocre at best\",\n",
    "    \"Quite bad, wouldn't recommend\",\n",
    "]\n",
    "\n",
    "# TODO: After first run, add 3 reviews that should fit your new category:\n",
    "# test_reviews.extend([\n",
    "#     \"Your review 1 here\",\n",
    "#     \"Your review 2 here\",\n",
    "#     \"Your review 3 here\",\n",
    "# ])\n",
    "\n",
    "# 5-level sentiment classification\n",
    "sentiment_labels = [\n",
    "    \"extremely negative sentiment\",\n",
    "    \"somewhat negative sentiment\",\n",
    "    \"neutral sentiment\",\n",
    "    \"somewhat positive sentiment\",\n",
    "    \"extremely positive sentiment\"\n",
    "]\n",
    "\n",
    "# TODO: After analyzing results, uncomment to add 6th category between somewhat and extremely positive:\n",
    "# sentiment_labels = [\n",
    "#     \"extremely negative sentiment\",\n",
    "#     \"somewhat negative sentiment\",\n",
    "#     \"neutral sentiment\",\n",
    "#     \"somewhat positive sentiment\",\n",
    "#     \"very positive sentiment\",  # NEW CATEGORY\n",
    "#     \"extremely positive sentiment\"\n",
    "# ]\n",
    "\n",
    "# Create embeddings\n",
    "label_embeddings = model.encode(sentiment_labels)\n",
    "review_embeddings = model.encode(test_reviews)\n",
    "sim_matrix = cosine_similarity(review_embeddings, label_embeddings)\n",
    "\n",
    "# Classify\n",
    "predictions = np.argmax(sim_matrix, axis=1)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"MULTI-CLASS CLASSIFICATION ({len(sentiment_labels)} categories)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, review in enumerate(test_reviews):\n",
    "    predicted_idx = predictions[i]\n",
    "    predicted_label = sentiment_labels[predicted_idx]\n",
    "    confidence = sim_matrix[i][predicted_idx]\n",
    "\n",
    "    # Get top 2 predictions to see confusion\n",
    "    top2_indices = np.argsort(sim_matrix[i])[-2:][::-1]\n",
    "    second_best_idx = top2_indices[1]\n",
    "    second_best_label = sentiment_labels[second_best_idx]\n",
    "    second_best_score = sim_matrix[i][second_best_idx]\n",
    "    margin = confidence - second_best_score\n",
    "\n",
    "    print(f\"\\nReview {i+1}: '{review[:60]}...'\")\n",
    "    print(f\"  1st: {predicted_label:30s} ({confidence:.3f})\")\n",
    "    print(f\"  2nd: {second_best_label:30s} ({second_best_score:.3f})\")\n",
    "    print(f\"  Margin: {margin:.3f}\", end=\"\")\n",
    "\n",
    "    if margin < 0.05:\n",
    "        print(\" \u26a0\ufe0f  VERY UNCERTAIN - almost tied!\")\n",
    "    elif margin < 0.15:\n",
    "        print(\" \u26a0\ufe0f  Uncertain\")\n",
    "    else:\n",
    "        print(\" \u2713 Confident\")\n",
    "\n",
    "# Analyze category confusion\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CATEGORY CONFUSION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"How similar are the category descriptions to each other?\")\n",
    "print(\"(High similarity = easy to confuse)\\n\")\n",
    "\n",
    "label_similarity = cosine_similarity(label_embeddings)\n",
    "\n",
    "print(f\"{'Category Pair':<60s} {'Similarity':<12s}\")\n",
    "print(\"-\"*75)\n",
    "\n",
    "confusions = []\n",
    "for i in range(len(sentiment_labels)):\n",
    "    for j in range(i+1, len(sentiment_labels)):\n",
    "        sim = label_similarity[i][j]\n",
    "        confusions.append((i, j, sim))\n",
    "\n",
    "# Sort by similarity (most confusing first)\n",
    "for i, j, sim in sorted(confusions, key=lambda x: x[2], reverse=True)[:10]:\n",
    "    pair_name = f\"{sentiment_labels[i]} <-> {sentiment_labels[j]}\"\n",
    "    marker = \"\u26a0\ufe0f \" if sim > 0.7 else \"\"\n",
    "    print(f\"{marker}{pair_name:<60s} {sim:.3f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p5Zs_XkNEfiF",
    "outputId": "753f121d-3519-4b3f-e067-a89c0568986f"
   },
   "execution_count": 30,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "================================================================================\n",
      "MULTI-CLASS CLASSIFICATION (5 categories)\n",
      "================================================================================\n",
      "\n",
      "Review 1: 'This is the best movie I've ever seen! Absolute masterpiece!...'\n",
      "  1st: extremely positive sentiment   (0.221)\n",
      "  2nd: extremely negative sentiment   (0.119)\n",
      "  Margin: 0.102 \u26a0\ufe0f  Uncertain\n",
      "\n",
      "Review 2: 'Pretty good movie, I enjoyed it...'\n",
      "  1st: somewhat positive sentiment    (0.320)\n",
      "  2nd: extremely positive sentiment   (0.313)\n",
      "  Margin: 0.007 \u26a0\ufe0f  VERY UNCERTAIN - almost tied!\n",
      "\n",
      "Review 3: 'It was okay, nothing special...'\n",
      "  1st: somewhat negative sentiment    (0.411)\n",
      "  2nd: somewhat positive sentiment    (0.395)\n",
      "  Margin: 0.016 \u26a0\ufe0f  VERY UNCERTAIN - almost tied!\n",
      "\n",
      "Review 4: 'Not great, pretty disappointing...'\n",
      "  1st: somewhat negative sentiment    (0.451)\n",
      "  2nd: extremely negative sentiment   (0.443)\n",
      "  Margin: 0.008 \u26a0\ufe0f  VERY UNCERTAIN - almost tied!\n",
      "\n",
      "Review 5: 'Absolutely terrible, worst movie ever...'\n",
      "  1st: extremely negative sentiment   (0.442)\n",
      "  2nd: somewhat negative sentiment    (0.354)\n",
      "  Margin: 0.087 \u26a0\ufe0f  Uncertain\n",
      "\n",
      "Review 6: 'Amazing performances and stunning visuals!...'\n",
      "  1st: extremely positive sentiment   (0.272)\n",
      "  2nd: extremely negative sentiment   (0.124)\n",
      "  Margin: 0.148 \u26a0\ufe0f  Uncertain\n",
      "\n",
      "Review 7: 'Mediocre at best...'\n",
      "  1st: somewhat negative sentiment    (0.428)\n",
      "  2nd: extremely negative sentiment   (0.418)\n",
      "  Margin: 0.010 \u26a0\ufe0f  VERY UNCERTAIN - almost tied!\n",
      "\n",
      "Review 8: 'Quite bad, wouldn't recommend...'\n",
      "  1st: somewhat negative sentiment    (0.402)\n",
      "  2nd: extremely negative sentiment   (0.399)\n",
      "  Margin: 0.003 \u26a0\ufe0f  VERY UNCERTAIN - almost tied!\n",
      "\n",
      "================================================================================\n",
      "CATEGORY CONFUSION ANALYSIS\n",
      "================================================================================\n",
      "How similar are the category descriptions to each other?\n",
      "(High similarity = easy to confuse)\n",
      "\n",
      "Category Pair                                                Similarity  \n",
      "---------------------------------------------------------------------------\n",
      "\u26a0\ufe0f somewhat negative sentiment <-> somewhat positive sentiment  0.950\n",
      "\u26a0\ufe0f somewhat negative sentiment <-> neutral sentiment            0.866\n",
      "\u26a0\ufe0f neutral sentiment <-> somewhat positive sentiment            0.862\n",
      "\u26a0\ufe0f extremely negative sentiment <-> somewhat negative sentiment 0.847\n",
      "\u26a0\ufe0f somewhat positive sentiment <-> extremely positive sentiment 0.808\n",
      "\u26a0\ufe0f extremely negative sentiment <-> extremely positive sentiment 0.788\n",
      "\u26a0\ufe0f extremely negative sentiment <-> neutral sentiment           0.786\n",
      "\u26a0\ufe0f somewhat negative sentiment <-> extremely positive sentiment 0.768\n",
      "\u26a0\ufe0f extremely negative sentiment <-> somewhat positive sentiment 0.759\n",
      "\u26a0\ufe0f neutral sentiment <-> extremely positive sentiment           0.710\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Questions:**\n",
    "\n",
    "1. Which reviews have low margins (<0.10)? What linguistic features do they share? How does multi-class classification differ from binary?\n",
    "\n",
    "2. Which adjacent categories have highest similarity in the confusion analysis? How could you rewrite label descriptions to create clearer boundaries?\n",
    "\n",
    "3. After adding your 6th category: Did reviews switch to it? Did the new category create more uncertainty or resolve confusion?"
   ],
   "metadata": {
    "id": "8viLeEAdE4CB"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Medium Task 2: Classifier Performance with Limited Training Data\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Execute code to see task-specific model vs embedding classifier with 1000 training samples\n",
    "2. Modify `train_size` to 100, then 2000, then 5000 - run after each change\n",
    "3. Fill in the results table in the TODO section\n",
    "4. Analyze at what point the embedding classifier matches the task-specific model"
   ],
   "metadata": {
    "id": "Lzba7C3rE_Xg"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "data = load_dataset(\"rotten_tomatoes\")\n",
    "\n",
    "# TODO: EXPERIMENT WITH THIS VALUE - Try: 100, 500, 1000, 2000, 5000\n",
    "train_size = 1000\n",
    "test_size = 300\n",
    "\n",
    "train_subset = data[\"train\"].shuffle(seed=42).select(range(min(train_size, len(data[\"train\"]))))\n",
    "test_subset = data[\"test\"].shuffle(seed=42).select(range(test_size))\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"EXPERIMENT: Training Size = {train_size}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Approach 1: Task-Specific Model (pre-trained for sentiment)\n",
    "print(\"\\n[1/2] Testing Task-Specific Model...\")\n",
    "print(\"Note: This model doesn't use our training data - it's already trained!\")\n",
    "\n",
    "task_model = pipeline(\n",
    "    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    tokenizer=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    return_all_scores=True,\n",
    "    device=-1\n",
    ")\n",
    "\n",
    "y_pred_task = []\n",
    "for text in test_subset[\"text\"]:\n",
    "    output = task_model(text)[0]\n",
    "    neg_score = output[0][\"score\"]\n",
    "    pos_score = output[2][\"score\"]\n",
    "    y_pred_task.append(1 if pos_score > neg_score else 0)\n",
    "\n",
    "task_f1 = f1_score(test_subset[\"label\"], y_pred_task, average='weighted')\n",
    "\n",
    "print(f\"\u2713 Task-Specific Model F1: {task_f1:.4f}\")\n",
    "\n",
    "# Approach 2: Embedding + Classifier (uses our training data)\n",
    "print(f\"\\n[2/2] Training Embedding Classifier on {train_size} samples...\")\n",
    "\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "train_embeddings = embedding_model.encode(train_subset[\"text\"], show_progress_bar=False)\n",
    "test_embeddings = embedding_model.encode(test_subset[\"text\"], show_progress_bar=False)\n",
    "\n",
    "clf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "clf.fit(train_embeddings, train_subset[\"label\"])\n",
    "\n",
    "y_pred_embed = clf.predict(test_embeddings)\n",
    "embed_f1 = f1_score(test_subset[\"label\"], y_pred_embed, average='weighted')\n",
    "\n",
    "print(f\"\u2713 Embedding Classifier F1: {embed_f1:.4f}\")\n",
    "\n",
    "# Comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Training samples used: {train_size}\")\n",
    "print(f\"\\nTask-Specific (pre-trained):  F1 = {task_f1:.4f}\")\n",
    "print(f\"Embedding + Classifier:       F1 = {embed_f1:.4f}\")\n",
    "print(f\"Difference:                       {embed_f1 - task_f1:+.4f}\")\n",
    "\n",
    "if embed_f1 > task_f1:\n",
    "    print(f\"\\n\u2192 Embedding approach WINS with {train_size} samples!\")\n",
    "elif embed_f1 > task_f1 - 0.01:\n",
    "    print(f\"\\n\u2192 Essentially TIED - both perform similarly\")\n",
    "else:\n",
    "    print(f\"\\n\u2192 Task-specific model wins - embedding needs more data\")\n",
    "\n",
    "# Show some predictions\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXAMPLE PREDICTIONS (first 5 test samples)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in range(5):\n",
    "    true_label = \"Positive\" if test_subset[\"label\"][i] == 1 else \"Negative\"\n",
    "    task_pred = \"Positive\" if y_pred_task[i] == 1 else \"Negative\"\n",
    "    embed_pred = \"Positive\" if y_pred_embed[i] == 1 else \"Negative\"\n",
    "\n",
    "    task_correct = \"\u2713\" if y_pred_task[i] == test_subset[\"label\"][i] else \"\u2717\"\n",
    "    embed_correct = \"\u2713\" if y_pred_embed[i] == test_subset[\"label\"][i] else \"\u2717\"\n",
    "\n",
    "    print(f\"\\n{i+1}. '{test_subset['text'][i][:60]}...'\")\n",
    "    print(f\"   True: {true_label}\")\n",
    "    print(f\"   Task-Specific: {task_pred} {task_correct}\")\n",
    "    print(f\"   Embedding:     {embed_pred} {embed_correct}\")\n",
    "\n",
    "# TODO: RECORD YOUR RESULTS HERE\n",
    "# After running with different train_size values, fill in this table:\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"YOUR EXPERIMENT RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Run the code multiple times with different train_size values and record:\")\n",
    "print()\n",
    "print(\"| Train Size | Task F1 | Embedding F1 | Winner      |\")\n",
    "print(\"|------------|---------|--------------|-------------|\")\n",
    "print(\"| 100        | ?.????  | ?.????       | ?           |\")\n",
    "print(\"| 500        | ?.????  | ?.????       | ?           |\")\n",
    "print(\"| 1000       | ?.????  | ?.????       | ?           |\")\n",
    "print(\"| 2000       | ?.????  | ?.????       | ?           |\")\n",
    "print(\"| 5000       | ?.????  | ?.????       | ?           |\")\n",
    "print()\n",
    "print(f\"Current run: | {train_size:<10} | {task_f1:.4f}  | {embed_f1:.4f}       | {'Embed' if embed_f1 > task_f1 else 'Task':<11} |\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YHHauraaEyLb",
    "outputId": "44dd4a93-3ca0-48df-be6a-7541453e0d89"
   },
   "execution_count": 31,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "================================================================================\n",
      "EXPERIMENT: Training Size = 1000\n",
      "================================================================================\n",
      "\n",
      "[1/2] Testing Task-Specific Model...\n",
      "Note: This model doesn't use our training data - it's already trained!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2713 Task-Specific Model F1: 0.7709\n",
      "\n",
      "[2/2] Training Embedding Classifier on 1000 samples...\n",
      "\u2713 Embedding Classifier F1: 0.8699\n",
      "\n",
      "================================================================================\n",
      "RESULTS SUMMARY\n",
      "================================================================================\n",
      "Training samples used: 1000\n",
      "\n",
      "Task-Specific (pre-trained):  F1 = 0.7709\n",
      "Embedding + Classifier:       F1 = 0.8699\n",
      "Difference:                       +0.0990\n",
      "\n",
      "\u2192 Embedding approach WINS with 1000 samples!\n",
      "\n",
      "================================================================================\n",
      "EXAMPLE PREDICTIONS (first 5 test samples)\n",
      "================================================================================\n",
      "\n",
      "1. 'unpretentious , charming , quirky , original...'\n",
      "   True: Positive\n",
      "   Task-Specific: Positive \u2713\n",
      "   Embedding:     Positive \u2713\n",
      "\n",
      "2. 'a film really has to be exceptional to justify a three hour ...'\n",
      "   True: Negative\n",
      "   Task-Specific: Negative \u2713\n",
      "   Embedding:     Negative \u2713\n",
      "\n",
      "3. 'working from a surprisingly sensitive script co-written by g...'\n",
      "   True: Positive\n",
      "   Task-Specific: Positive \u2713\n",
      "   Embedding:     Positive \u2713\n",
      "\n",
      "4. 'it may not be particularly innovative , but the film's crisp...'\n",
      "   True: Positive\n",
      "   Task-Specific: Positive \u2713\n",
      "   Embedding:     Positive \u2713\n",
      "\n",
      "5. 'such a premise is ripe for all manner of lunacy , but kaufma...'\n",
      "   True: Negative\n",
      "   Task-Specific: Negative \u2713\n",
      "   Embedding:     Negative \u2713\n",
      "\n",
      "================================================================================\n",
      "YOUR EXPERIMENT RESULTS\n",
      "================================================================================\n",
      "Run the code multiple times with different train_size values and record:\n",
      "\n",
      "| Train Size | Task F1 | Embedding F1 | Winner      |\n",
      "|------------|---------|--------------|-------------|\n",
      "| 100        | ?.????  | ?.????       | ?           |\n",
      "| 500        | ?.????  | ?.????       | ?           |\n",
      "| 1000       | ?.????  | ?.????       | ?           |\n",
      "| 2000       | ?.????  | ?.????       | ?           |\n",
      "| 5000       | ?.????  | ?.????       | ?           |\n",
      "\n",
      "Current run: | 1000       | 0.7709  | 0.8699       | Embed       |\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Questions:**\n",
    "\n",
    "1. At what training size did the embedding classifier match or beat the task-specific model? What does this reveal about data requirements?\n",
    "\n",
    "2. Were there cases where one model was correct and the other wrong? What characteristics did those reviews have?\n",
    "\n",
    "3. With train_size=100, is this enough labeled data? How does this compare to training models from scratch?"
   ],
   "metadata": {
    "id": "ZCgyzR_SFXCC"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Medium Task 3: Confidence-Based Classifier with Uncertainty Handling\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Execute code to see classifier handling uncertain predictions with threshold=0.15\n",
    "2. Analyze which reviews were marked as \"uncertain\" and why\n",
    "3. Change `confidence_threshold` to 0.05, then 0.30 to observe trade-offs\n",
    "4. Uncomment TODO to implement an alternative uncertainty measure\n",
    "5. Compare which uncertainty measure works better"
   ],
   "metadata": {
    "id": "poMISz4VFvWf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# Reviews with varying levels of clarity\n",
    "test_reviews = [\n",
    "    \"Absolutely fantastic! Best movie ever!\",           # Clear positive\n",
    "    \"Pretty good, I liked it\",                          # Weak positive\n",
    "    \"It was fine, nothing special\",                     # Ambiguous\n",
    "    \"Not bad but not great either\",                     # Very ambiguous\n",
    "    \"Quite disappointing\",                              # Weak negative\n",
    "    \"Terrible! Complete waste of time!\",                # Clear negative\n",
    "    \"The movie had some interesting moments\",           # Ambiguous positive\n",
    "    \"Outstanding performances all around!\",             # Clear positive\n",
    "]\n",
    "\n",
    "# True labels (for evaluation)\n",
    "y_true = [1, 1, 0, 0, 0, 0, 1, 1]  # 1=positive, 0=negative\n",
    "\n",
    "labels = [\"A negative movie review\", \"A positive movie review\"]\n",
    "\n",
    "# TODO: EXPERIMENT WITH THIS - Try: 0.05, 0.15, 0.30, 0.50\n",
    "confidence_threshold = 0.15\n",
    "\n",
    "label_embeddings = model.encode(labels)\n",
    "review_embeddings = model.encode(test_reviews)\n",
    "sim_matrix = cosine_similarity(review_embeddings, label_embeddings)\n",
    "\n",
    "def calculate_margin(similarities):\n",
    "    \"\"\"\n",
    "    Margin = difference between top two predictions\n",
    "    Small margin = uncertain (predictions are close)\n",
    "    \"\"\"\n",
    "    sorted_sims = np.sort(similarities)[::-1]\n",
    "    margin = sorted_sims[0] - sorted_sims[1]\n",
    "    return margin\n",
    "\n",
    "# TODO: After first run, uncomment this alternative uncertainty measure:\n",
    "# def calculate_margin(similarities):\n",
    "#     \"\"\"\n",
    "#     Alternative: Use absolute confidence in top prediction\n",
    "#     Low confidence = uncertain\n",
    "#     \"\"\"\n",
    "#     max_confidence = np.max(similarities)\n",
    "#     # Convert to margin-like score (higher = more certain)\n",
    "#     # If max is 0.6, margin = 0.6 - 0.5 = 0.1 (uncertain)\n",
    "#     # If max is 0.9, margin = 0.9 - 0.5 = 0.4 (certain)\n",
    "#     margin = max_confidence - 0.5\n",
    "#     return margin\n",
    "\n",
    "# Classify with confidence threshold\n",
    "results = []\n",
    "predictions = []\n",
    "confidences = []\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"CONFIDENCE-BASED CLASSIFICATION (threshold={confidence_threshold})\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, review in enumerate(test_reviews):\n",
    "    similarities = sim_matrix[i]\n",
    "    predicted_idx = np.argmax(similarities)\n",
    "    top_confidence = similarities[predicted_idx]\n",
    "    margin = calculate_margin(similarities)\n",
    "\n",
    "    # Decision: predict only if confident enough\n",
    "    if margin >= confidence_threshold:\n",
    "        prediction = predicted_idx\n",
    "        status = \"PREDICTED\"\n",
    "        predictions.append(prediction)\n",
    "    else:\n",
    "        prediction = None\n",
    "        status = \"UNCERTAIN\"\n",
    "        predictions.append(None)\n",
    "\n",
    "    true_label = \"Positive\" if y_true[i] == 1 else \"Negative\"\n",
    "    pred_label = labels[predicted_idx] if prediction is not None else \"UNCERTAIN\"\n",
    "\n",
    "    print(f\"\\n{i+1}. '{review}'\")\n",
    "    print(f\"   True label: {true_label}\")\n",
    "    print(f\"   Prediction: {pred_label}\")\n",
    "    print(f\"   Top confidence: {top_confidence:.3f}\")\n",
    "    print(f\"   Margin: {margin:.3f} {'\u2713 Above threshold' if margin >= confidence_threshold else '\u2717 Below threshold'}\")\n",
    "    print(f\"   Status: {status}\", end=\"\")\n",
    "\n",
    "    if prediction is not None:\n",
    "        correct = prediction == y_true[i]\n",
    "        print(f\" - {'\u2713 CORRECT' if correct else '\u2717 INCORRECT'}\")\n",
    "    else:\n",
    "        print()\n",
    "\n",
    "    results.append({\n",
    "        'review': review,\n",
    "        'true': y_true[i],\n",
    "        'pred': prediction,\n",
    "        'margin': margin,\n",
    "        'status': status\n",
    "    })\n",
    "\n",
    "# Calculate metrics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "made_predictions = [r for r in results if r['pred'] is not None]\n",
    "uncertain_cases = [r for r in results if r['pred'] is None]\n",
    "correct_predictions = [r for r in made_predictions if r['pred'] == r['true']]\n",
    "\n",
    "total = len(results)\n",
    "n_predicted = len(made_predictions)\n",
    "n_uncertain = len(uncertain_cases)\n",
    "n_correct = len(correct_predictions)\n",
    "\n",
    "coverage = n_predicted / total\n",
    "accuracy = n_correct / n_predicted if n_predicted > 0 else 0\n",
    "\n",
    "print(f\"\\nCoverage: {n_predicted}/{total} = {coverage:.1%}\")\n",
    "print(f\"  \u2192 Made predictions for {n_predicted} reviews\")\n",
    "print(f\"  \u2192 Refused to predict on {n_uncertain} reviews\")\n",
    "\n",
    "print(f\"\\nAccuracy (on predictions made): {n_correct}/{n_predicted} = {accuracy:.1%}\")\n",
    "print(f\"  \u2192 Of the {n_predicted} predictions, {n_correct} were correct\")\n",
    "\n",
    "print(f\"\\nTrade-off Analysis:\")\n",
    "print(f\"  Threshold = {confidence_threshold}\")\n",
    "print(f\"  \u2192 Higher threshold = fewer predictions but higher accuracy\")\n",
    "print(f\"  \u2192 Lower threshold = more predictions but lower accuracy\")\n",
    "\n",
    "# Show which reviews were uncertain\n",
    "if n_uncertain > 0:\n",
    "    print(f\"\\n\" + \"-\"*80)\n",
    "    print(f\"UNCERTAIN CASES (margin < {confidence_threshold}):\")\n",
    "    print(\"-\"*80)\n",
    "    for r in uncertain_cases:\n",
    "        print(f\"  \u2022 '{r['review']}'\")\n",
    "        print(f\"    Margin: {r['margin']:.3f} (too close to call)\")\n",
    "\n",
    "# TODO: After experimenting with thresholds, analyze the trade-off\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPERIMENT LOG - Fill this in as you try different thresholds:\")\n",
    "print(\"=\"*80)\n",
    "print(\"| Threshold | Coverage | Accuracy | Notes                    |\")\n",
    "print(\"|-----------|----------|----------|--------------------------|\")\n",
    "print(\"| 0.05      | ??.?%    | ??.?%    | ?                        |\")\n",
    "print(\"| 0.15      | ??.?%    | ??.?%    | ?                        |\")\n",
    "print(\"| 0.30      | ??.?%    | ??.?%    | ?                        |\")\n",
    "print(\"| 0.50      | ??.?%    | ??.?%    | ?                        |\")\n",
    "print()\n",
    "print(f\"Current:    | {confidence_threshold:<9.2f} | {coverage*100:>5.1f}%    | {accuracy*100:>5.1f}%    |\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lpzBi9YBFZRi",
    "outputId": "28f21c01-c8d7-4c04-8256-6f0dd47f890c"
   },
   "execution_count": 32,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "================================================================================\n",
      "CONFIDENCE-BASED CLASSIFICATION (threshold=0.15)\n",
      "================================================================================\n",
      "\n",
      "1. 'Absolutely fantastic! Best movie ever!'\n",
      "   True label: Positive\n",
      "   Prediction: UNCERTAIN\n",
      "   Top confidence: 0.451\n",
      "   Margin: 0.092 \u2717 Below threshold\n",
      "   Status: UNCERTAIN\n",
      "\n",
      "2. 'Pretty good, I liked it'\n",
      "   True label: Positive\n",
      "   Prediction: UNCERTAIN\n",
      "   Top confidence: 0.410\n",
      "   Margin: 0.018 \u2717 Below threshold\n",
      "   Status: UNCERTAIN\n",
      "\n",
      "3. 'It was fine, nothing special'\n",
      "   True label: Negative\n",
      "   Prediction: UNCERTAIN\n",
      "   Top confidence: 0.418\n",
      "   Margin: 0.051 \u2717 Below threshold\n",
      "   Status: UNCERTAIN\n",
      "\n",
      "4. 'Not bad but not great either'\n",
      "   True label: Negative\n",
      "   Prediction: UNCERTAIN\n",
      "   Top confidence: 0.414\n",
      "   Margin: 0.053 \u2717 Below threshold\n",
      "   Status: UNCERTAIN\n",
      "\n",
      "5. 'Quite disappointing'\n",
      "   True label: Negative\n",
      "   Prediction: UNCERTAIN\n",
      "   Top confidence: 0.354\n",
      "   Margin: 0.080 \u2717 Below threshold\n",
      "   Status: UNCERTAIN\n",
      "\n",
      "6. 'Terrible! Complete waste of time!'\n",
      "   True label: Negative\n",
      "   Prediction: UNCERTAIN\n",
      "   Top confidence: 0.397\n",
      "   Margin: 0.121 \u2717 Below threshold\n",
      "   Status: UNCERTAIN\n",
      "\n",
      "7. 'The movie had some interesting moments'\n",
      "   True label: Positive\n",
      "   Prediction: UNCERTAIN\n",
      "   Top confidence: 0.506\n",
      "   Margin: 0.118 \u2717 Below threshold\n",
      "   Status: UNCERTAIN\n",
      "\n",
      "8. 'Outstanding performances all around!'\n",
      "   True label: Positive\n",
      "   Prediction: UNCERTAIN\n",
      "   Top confidence: 0.282\n",
      "   Margin: 0.098 \u2717 Below threshold\n",
      "   Status: UNCERTAIN\n",
      "\n",
      "================================================================================\n",
      "PERFORMANCE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Coverage: 0/8 = 0.0%\n",
      "  \u2192 Made predictions for 0 reviews\n",
      "  \u2192 Refused to predict on 8 reviews\n",
      "\n",
      "Accuracy (on predictions made): 0/0 = 0.0%\n",
      "  \u2192 Of the 0 predictions, 0 were correct\n",
      "\n",
      "Trade-off Analysis:\n",
      "  Threshold = 0.15\n",
      "  \u2192 Higher threshold = fewer predictions but higher accuracy\n",
      "  \u2192 Lower threshold = more predictions but lower accuracy\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "UNCERTAIN CASES (margin < 0.15):\n",
      "--------------------------------------------------------------------------------\n",
      "  \u2022 'Absolutely fantastic! Best movie ever!'\n",
      "    Margin: 0.092 (too close to call)\n",
      "  \u2022 'Pretty good, I liked it'\n",
      "    Margin: 0.018 (too close to call)\n",
      "  \u2022 'It was fine, nothing special'\n",
      "    Margin: 0.051 (too close to call)\n",
      "  \u2022 'Not bad but not great either'\n",
      "    Margin: 0.053 (too close to call)\n",
      "  \u2022 'Quite disappointing'\n",
      "    Margin: 0.080 (too close to call)\n",
      "  \u2022 'Terrible! Complete waste of time!'\n",
      "    Margin: 0.121 (too close to call)\n",
      "  \u2022 'The movie had some interesting moments'\n",
      "    Margin: 0.118 (too close to call)\n",
      "  \u2022 'Outstanding performances all around!'\n",
      "    Margin: 0.098 (too close to call)\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT LOG - Fill this in as you try different thresholds:\n",
      "================================================================================\n",
      "| Threshold | Coverage | Accuracy | Notes                    |\n",
      "|-----------|----------|----------|--------------------------|\n",
      "| 0.05      | ??.?%    | ??.?%    | ?                        |\n",
      "| 0.15      | ??.?%    | ??.?%    | ?                        |\n",
      "| 0.30      | ??.?%    | ??.?%    | ?                        |\n",
      "| 0.50      | ??.?%    | ??.?%    | ?                        |\n",
      "\n",
      "Current:    | 0.15      |   0.0%    |   0.0%    |\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Questions:**\n",
    "\n",
    "1. What do uncertain reviews have in common? Are they using hedging language like \"kind of\" or \"somewhat\"?\n",
    "\n",
    "2. Compare results at threshold=0.05 vs 0.30. Describe the coverage vs accuracy trade-off. When would you want high coverage vs high accuracy?\n",
    "\n",
    "3. How could you use confidence-based prediction in production? What should a system do when the model is uncertain?"
   ],
   "metadata": {
    "id": "H7-OE7H9F6vx"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Medium Task 4: Classifier Failure Analysis\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Train classifier and review overall error analysis\n",
    "2. Study error patterns to understand which reviews failed and why\n",
    "3. Uncomment TODO to add your own \"hard cases\" that you predict will fail\n",
    "4. Test hypotheses: Do sarcastic reviews fail? Short reviews? Mixed sentiment?\n",
    "5. Propose fixes based on your analysis"
   ],
   "metadata": {
    "id": "hfzp5dAmGF3T"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "data = load_dataset(\"rotten_tomatoes\")\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# Use subset for faster experimentation\n",
    "train_subset = data[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "test_subset = data[\"test\"].shuffle(seed=42).select(range(200))\n",
    "\n",
    "# Train classifier\n",
    "print(\"Training classifier on 1000 movie reviews...\")\n",
    "train_embeddings = model.encode(train_subset[\"text\"], show_progress_bar=False)\n",
    "test_embeddings = model.encode(test_subset[\"text\"], show_progress_bar=False)\n",
    "\n",
    "clf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "clf.fit(train_embeddings, train_subset[\"label\"])\n",
    "\n",
    "# Get predictions\n",
    "predictions = clf.predict(test_embeddings)\n",
    "probabilities = clf.predict_proba(test_embeddings)\n",
    "\n",
    "# Analyze errors\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ERROR ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "errors = []\n",
    "for i in range(len(test_subset)):\n",
    "    if predictions[i] != test_subset[\"label\"][i]:\n",
    "        confidence = probabilities[i][predictions[i]]\n",
    "        errors.append({\n",
    "            'index': i,\n",
    "            'text': test_subset[\"text\"][i],\n",
    "            'true_label': test_subset[\"label\"][i],\n",
    "            'predicted_label': predictions[i],\n",
    "            'confidence': confidence,\n",
    "            'length': len(test_subset[\"text\"][i].split())\n",
    "        })\n",
    "\n",
    "total_errors = len(errors)\n",
    "total_samples = len(test_subset)\n",
    "accuracy = (total_samples - total_errors) / total_samples\n",
    "\n",
    "print(f\"\\nOverall Performance:\")\n",
    "print(f\"  Correct: {total_samples - total_errors}/{total_samples} ({accuracy:.1%})\")\n",
    "print(f\"  Errors:  {total_errors}/{total_samples} ({total_errors/total_samples:.1%})\")\n",
    "\n",
    "# Categorize errors\n",
    "false_positives = [e for e in errors if e['predicted_label'] == 1]\n",
    "false_negatives = [e for e in errors if e['predicted_label'] == 0]\n",
    "\n",
    "print(f\"\\nError Types:\")\n",
    "print(f\"  False Positives: {len(false_positives)} (predicted positive, actually negative)\")\n",
    "print(f\"  False Negatives: {len(false_negatives)} (predicted negative, actually positive)\")\n",
    "\n",
    "# Show high-confidence errors (most surprising)\n",
    "high_conf_errors = [e for e in errors if e['confidence'] > 0.7]\n",
    "\n",
    "print(f\"\\n\" + \"-\"*80)\n",
    "print(f\"HIGH-CONFIDENCE ERRORS (confidence > 0.7)\")\n",
    "print(f\"These are the most surprising mistakes:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for i, error in enumerate(high_conf_errors[:5]):\n",
    "    true_sent = \"Positive\" if error['true_label'] == 1 else \"Negative\"\n",
    "    pred_sent = \"Positive\" if error['predicted_label'] == 1 else \"Negative\"\n",
    "\n",
    "    print(f\"\\n{i+1}. '{error['text']}'\")\n",
    "    print(f\"   True: {true_sent} | Predicted: {pred_sent} | Confidence: {error['confidence']:.3f}\")\n",
    "    print(f\"   Length: {error['length']} words\")\n",
    "\n",
    "# Analyze by text length\n",
    "print(f\"\\n\" + \"-\"*80)\n",
    "print(\"ERROR ANALYSIS BY TEXT LENGTH\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "error_lengths = [e['length'] for e in errors]\n",
    "correct_lengths = [len(test_subset[\"text\"][i].split())\n",
    "                   for i in range(len(test_subset))\n",
    "                   if predictions[i] == test_subset[\"label\"][i]]\n",
    "\n",
    "avg_error_length = np.mean(error_lengths) if error_lengths else 0\n",
    "avg_correct_length = np.mean(correct_lengths) if correct_lengths else 0\n",
    "\n",
    "print(f\"\\nAverage length of ERROR reviews: {avg_error_length:.1f} words\")\n",
    "print(f\"Average length of CORRECT reviews: {avg_correct_length:.1f} words\")\n",
    "\n",
    "if avg_error_length < avg_correct_length:\n",
    "    print(f\"\u2192 Observation: Errors tend to be SHORTER\")\n",
    "elif avg_error_length > avg_correct_length:\n",
    "    print(f\"\u2192 Observation: Errors tend to be LONGER\")\n",
    "else:\n",
    "    print(f\"\u2192 Observation: No clear length pattern\")\n",
    "\n",
    "# Test edge cases\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING EDGE CASES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "edge_cases = [\n",
    "    (\"Sarcastic\", \"Oh great, another masterpiece. NOT!\", 0),\n",
    "    (\"Mixed\", \"The acting was great but the plot was terrible\", 0),\n",
    "    (\"Backhanded\", \"Not as bad as I expected\", 1),\n",
    "    (\"Double negative\", \"Not unwatchable\", 1),\n",
    "    (\"Very short\", \"Boring\", 0),\n",
    "    (\"Ambiguous\", \"It was a movie\", 0),\n",
    "]\n",
    "\n",
    "# TODO: After analyzing above errors, add your own test cases:\n",
    "# edge_cases.extend([\n",
    "#     (\"Your category\", \"Your test review here\", expected_label_0_or_1),\n",
    "#     (\"Another category\", \"Another test review\", expected_label),\n",
    "# ])\n",
    "\n",
    "print(\"\\nTesting challenging cases that often fail:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "edge_embeddings = model.encode([text for _, text, _ in edge_cases])\n",
    "edge_predictions = clf.predict(edge_embeddings)\n",
    "edge_probs = clf.predict_proba(edge_embeddings)\n",
    "\n",
    "correct_count = 0\n",
    "for i, (category, text, true_label) in enumerate(edge_cases):\n",
    "    pred = edge_predictions[i]\n",
    "    conf = edge_probs[i][pred]\n",
    "    correct = pred == true_label\n",
    "    if correct:\n",
    "        correct_count += 1\n",
    "\n",
    "    true_sent = \"Positive\" if true_label == 1 else \"Negative\"\n",
    "    pred_sent = \"Positive\" if pred == 1 else \"Negative\"\n",
    "\n",
    "    print(f\"\\n{category}: '{text}'\")\n",
    "    print(f\"  True: {true_sent} | Predicted: {pred_sent} | Confidence: {conf:.3f}\")\n",
    "    print(f\"  Result: {'\u2713 CORRECT' if correct else '\u2717 WRONG'}\")\n",
    "\n",
    "edge_accuracy = correct_count / len(edge_cases)\n",
    "print(f\"\\n\" + \"-\"*80)\n",
    "print(f\"Edge Case Accuracy: {correct_count}/{len(edge_cases)} ({edge_accuracy:.1%})\")\n",
    "print(f\"Regular Test Accuracy: {accuracy:.1%}\")\n",
    "print(f\"Difference: {accuracy - edge_accuracy:+.1%}\")\n",
    "\n",
    "# Summary and insights\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY INSIGHTS FROM ERROR ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. Error Distribution:\")\n",
    "print(f\"   - False Positives (predicted too optimistic): {len(false_positives)}\")\n",
    "print(f\"   - False Negatives (predicted too pessimistic): {len(false_negatives)}\")\n",
    "if len(false_positives) > len(false_negatives):\n",
    "    print(f\"   \u2192 Classifier has POSITIVE BIAS\")\n",
    "elif len(false_negatives) > len(false_positives):\n",
    "    print(f\"   \u2192 Classifier has NEGATIVE BIAS\")\n",
    "\n",
    "print(\"\\n2. Challenging Cases:\")\n",
    "failing_categories = [cat for cat, text, true in edge_cases\n",
    "                     if clf.predict(model.encode([text]))[0] != true]\n",
    "if failing_categories:\n",
    "    print(f\"   The classifier struggles with: {', '.join(failing_categories)}\")\n",
    "\n",
    "print(\"\\n3. Confidence Analysis:\")\n",
    "if high_conf_errors:\n",
    "    print(f\"   Found {len(high_conf_errors)} high-confidence errors\")\n",
    "    print(f\"   \u2192 The model is 'confidently wrong' on some cases\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TODO: Based on your error analysis, propose improvements:\")\n",
    "print(\"=\"*80)\n",
    "print(\"# Write your observations here:\")\n",
    "print(\"# 1. What patterns did you notice in the errors?\")\n",
    "print(\"# 2. Which edge cases failed most?\")\n",
    "print(\"# 3. How would you improve the classifier?\")\n",
    "print(\"#    - Better training data?\")\n",
    "print(\"#    - Different features?\")\n",
    "print(\"#    - Ensemble approach?\")\n",
    "print(\"#    - Confidence thresholds?\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sfz2qOmPGHWp",
    "outputId": "341250f6-c025-4e7a-ded8-de541c0ebc44"
   },
   "execution_count": 33,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training classifier on 1000 movie reviews...\n",
      "\n",
      "================================================================================\n",
      "ERROR ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Overall Performance:\n",
      "  Correct: 174/200 (87.0%)\n",
      "  Errors:  26/200 (13.0%)\n",
      "\n",
      "Error Types:\n",
      "  False Positives: 11 (predicted positive, actually negative)\n",
      "  False Negatives: 15 (predicted negative, actually positive)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "HIGH-CONFIDENCE ERRORS (confidence > 0.7)\n",
      "These are the most surprising mistakes:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. 'an uneasy mix of run-of-the-mill raunchy humor and seemingly sincere personal reflection .'\n",
      "   True: Negative | Predicted: Positive | Confidence: 0.701\n",
      "   Length: 13 words\n",
      "\n",
      "2. 'the stunt work is top-notch ; the dialogue and drama often food-spittingly funny .'\n",
      "   True: Negative | Predicted: Positive | Confidence: 0.867\n",
      "   Length: 14 words\n",
      "\n",
      "3. 'goldmember is funny enough to justify the embarrassment of bringing a barf bag to the moviehouse .'\n",
      "   True: Positive | Predicted: Negative | Confidence: 0.710\n",
      "   Length: 17 words\n",
      "\n",
      "4. 'steven soderbergh doesn't remake andrei tarkovsky's solaris so much as distill it .'\n",
      "   True: Positive | Predicted: Negative | Confidence: 0.730\n",
      "   Length: 13 words\n",
      "\n",
      "5. '\" what really happened ? \" is a question for philosophers , not filmmakers ; all the filmmakers need to do is engage an audience .'\n",
      "   True: Positive | Predicted: Negative | Confidence: 0.717\n",
      "   Length: 26 words\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "ERROR ANALYSIS BY TEXT LENGTH\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Average length of ERROR reviews: 19.8 words\n",
      "Average length of CORRECT reviews: 21.4 words\n",
      "\u2192 Observation: Errors tend to be SHORTER\n",
      "\n",
      "================================================================================\n",
      "TESTING EDGE CASES\n",
      "================================================================================\n",
      "\n",
      "Testing challenging cases that often fail:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sarcastic: 'Oh great, another masterpiece. NOT!'\n",
      "  True: Negative | Predicted: Positive | Confidence: 0.648\n",
      "  Result: \u2717 WRONG\n",
      "\n",
      "Mixed: 'The acting was great but the plot was terrible'\n",
      "  True: Negative | Predicted: Negative | Confidence: 0.920\n",
      "  Result: \u2713 CORRECT\n",
      "\n",
      "Backhanded: 'Not as bad as I expected'\n",
      "  True: Positive | Predicted: Negative | Confidence: 0.809\n",
      "  Result: \u2717 WRONG\n",
      "\n",
      "Double negative: 'Not unwatchable'\n",
      "  True: Positive | Predicted: Negative | Confidence: 0.902\n",
      "  Result: \u2717 WRONG\n",
      "\n",
      "Very short: 'Boring'\n",
      "  True: Negative | Predicted: Negative | Confidence: 0.949\n",
      "  Result: \u2713 CORRECT\n",
      "\n",
      "Ambiguous: 'It was a movie'\n",
      "  True: Negative | Predicted: Negative | Confidence: 0.709\n",
      "  Result: \u2713 CORRECT\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Edge Case Accuracy: 3/6 (50.0%)\n",
      "Regular Test Accuracy: 87.0%\n",
      "Difference: +37.0%\n",
      "\n",
      "================================================================================\n",
      "KEY INSIGHTS FROM ERROR ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "1. Error Distribution:\n",
      "   - False Positives (predicted too optimistic): 11\n",
      "   - False Negatives (predicted too pessimistic): 15\n",
      "   \u2192 Classifier has NEGATIVE BIAS\n",
      "\n",
      "2. Challenging Cases:\n",
      "   The classifier struggles with: Sarcastic, Backhanded, Double negative\n",
      "\n",
      "3. Confidence Analysis:\n",
      "   Found 11 high-confidence errors\n",
      "   \u2192 The model is 'confidently wrong' on some cases\n",
      "\n",
      "================================================================================\n",
      "TODO: Based on your error analysis, propose improvements:\n",
      "================================================================================\n",
      "# Write your observations here:\n",
      "# 1. What patterns did you notice in the errors?\n",
      "# 2. Which edge cases failed most?\n",
      "# 3. How would you improve the classifier?\n",
      "#    - Better training data?\n",
      "#    - Different features?\n",
      "#    - Ensemble approach?\n",
      "#    - Confidence thresholds?\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reflection Questions:**\n",
    "\n",
    "1. What do high-confidence errors have in common? How does model confidence relate to correctness?\n",
    "\n",
    "2. Do errors tend to be shorter, longer, or similar length compared to correct predictions? Why might text length affect classification?\n",
    "\n",
    "3. Which edge cases failed most - sarcasm, mixed sentiment, or double negatives? What aspects of language do embeddings not capture well?"
   ],
   "metadata": {
    "id": "M0kp8xwOGU70"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hard Tasks - Advanced Classification Challenges\n",
    "\n",
    "These tasks require significant modifications and deeper understanding. Take your time and experiment"
   ],
   "metadata": {
    "id": "sOcbrBwdGZbi"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Hard Task 1: Hierarchical Multi-Level Classifier\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Execute code to see 2-level hierarchical classifier (sentiment \u2192 specific aspect)\n",
    "2. Analyze whether hierarchy improves accuracy compared to flat classification\n",
    "3. Uncomment TODO to add a third level of granularity\n",
    "4. Create test reviews that specifically target your new level\n",
    "5. Compare 3-level vs 2-level performance"
   ],
   "metadata": {
    "id": "jfAghPh1Gzcl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# Test reviews targeting different aspects\n",
    "test_reviews = [\n",
    "    # Positive - Quality focused\n",
    "    \"Brilliant performances and stunning cinematography\",\n",
    "    \"Exceptional directing and beautiful visuals\",\n",
    "\n",
    "    # Positive - Entertainment focused\n",
    "    \"So much fun! Had a great time watching\",\n",
    "    \"Really entertaining and enjoyable\",\n",
    "\n",
    "    # Negative - Boring\n",
    "    \"Incredibly dull and slow-paced\",\n",
    "    \"Boring, nothing happens for two hours\",\n",
    "\n",
    "    # Negative - Quality issues\n",
    "    \"Poor acting and terrible script\",\n",
    "    \"Awful production values and bad directing\",\n",
    "]\n",
    "\n",
    "# TODO: After understanding the hierarchy, add reviews for your third level:\n",
    "# test_reviews.extend([\n",
    "#     \"The cinematography was breathtaking, truly artistic\",  # Quality -> Visual\n",
    "#     \"Powerful performances, especially the lead actor\",     # Quality -> Acting\n",
    "#     \"Had me laughing throughout the entire film\",           # Entertainment -> Comedy\n",
    "#     \"Edge of my seat thriller, so suspenseful\",            # Entertainment -> Excitement\n",
    "# ])\n",
    "\n",
    "# Level 1: Broad sentiment\n",
    "level1_labels = [\n",
    "    \"negative sentiment review\",\n",
    "    \"positive sentiment review\"\n",
    "]\n",
    "\n",
    "# Level 2: Specific aspects\n",
    "level2_negative = [\n",
    "    \"review criticizing entertainment value and pacing\",\n",
    "    \"review criticizing technical quality and production\"\n",
    "]\n",
    "\n",
    "level2_positive = [\n",
    "    \"review praising technical quality and artistry\",\n",
    "    \"review praising entertainment value and enjoyment\"\n",
    "]\n",
    "\n",
    "# TODO: Uncomment to add Level 3 - more specific categories\n",
    "# level3_positive_quality = [\n",
    "#     \"review praising visual elements and cinematography\",\n",
    "#     \"review praising acting performances and characters\"\n",
    "# ]\n",
    "#\n",
    "# level3_positive_entertainment = [\n",
    "#     \"review praising humor and comedy\",\n",
    "#     \"review praising excitement and suspense\"\n",
    "# ]\n",
    "#\n",
    "# level3_negative_quality = [\n",
    "#     \"review criticizing visual elements and cinematography\",\n",
    "#     \"review criticizing acting performances and characters\"\n",
    "# ]\n",
    "#\n",
    "# level3_negative_entertainment = [\n",
    "#     \"review criticizing humor attempts and comedy\",\n",
    "#     \"review criticizing pacing and excitement\"\n",
    "# ]\n",
    "\n",
    "def hierarchical_classify_2level(text):\n",
    "    \"\"\"Two-level classification: Sentiment -> Aspect\"\"\"\n",
    "    text_embedding = model.encode([text])\n",
    "\n",
    "    # Level 1: Determine sentiment\n",
    "    level1_embeddings = model.encode(level1_labels)\n",
    "    level1_sim = cosine_similarity(text_embedding, level1_embeddings)[0]\n",
    "    level1_pred = np.argmax(level1_sim)\n",
    "    level1_conf = level1_sim[level1_pred]\n",
    "\n",
    "    # Level 2: Determine specific aspect based on Level 1\n",
    "    if level1_pred == 0:  # Negative\n",
    "        level2_labels = level2_negative\n",
    "        sentiment = \"Negative\"\n",
    "    else:  # Positive\n",
    "        level2_labels = level2_positive\n",
    "        sentiment = \"Positive\"\n",
    "\n",
    "    level2_embeddings = model.encode(level2_labels)\n",
    "    level2_sim = cosine_similarity(text_embedding, level2_embeddings)[0]\n",
    "    level2_pred = np.argmax(level2_sim)\n",
    "    level2_conf = level2_sim[level2_pred]\n",
    "\n",
    "    return {\n",
    "        'level1_pred': level1_pred,\n",
    "        'level1_label': level1_labels[level1_pred],\n",
    "        'level1_conf': level1_conf,\n",
    "        'level2_pred': level2_pred,\n",
    "        'level2_label': level2_labels[level2_pred],\n",
    "        'level2_conf': level2_conf,\n",
    "        'sentiment': sentiment,\n",
    "        'path': f\"{sentiment} -> {level2_labels[level2_pred]}\"\n",
    "    }\n",
    "\n",
    "# TODO: Uncomment to implement 3-level classification\n",
    "# def hierarchical_classify_3level(text):\n",
    "#     \"\"\"Three-level classification: Sentiment -> Aspect -> Specific\"\"\"\n",
    "#     # Start with levels 1 and 2\n",
    "#     result = hierarchical_classify_2level(text)\n",
    "#\n",
    "#     text_embedding = model.encode([text])\n",
    "#\n",
    "#     # Level 3: Even more specific based on Level 2\n",
    "#     if result['sentiment'] == \"Positive\":\n",
    "#         if result['level2_pred'] == 0:  # Quality\n",
    "#             level3_labels = level3_positive_quality\n",
    "#         else:  # Entertainment\n",
    "#             level3_labels = level3_positive_entertainment\n",
    "#     else:  # Negative\n",
    "#         if result['level2_pred'] == 0:  # Entertainment\n",
    "#             level3_labels = level3_negative_entertainment\n",
    "#         else:  # Quality\n",
    "#             level3_labels = level3_negative_quality\n",
    "#\n",
    "#     level3_embeddings = model.encode(level3_labels)\n",
    "#     level3_sim = cosine_similarity(text_embedding, level3_embeddings)[0]\n",
    "#     level3_pred = np.argmax(level3_sim)\n",
    "#     level3_conf = level3_sim[level3_pred]\n",
    "#\n",
    "#     result['level3_pred'] = level3_pred\n",
    "#     result['level3_label'] = level3_labels[level3_pred]\n",
    "#     result['level3_conf'] = level3_conf\n",
    "#     result['path'] = f\"{result['sentiment']} -> L2 -> {level3_labels[level3_pred]}\"\n",
    "#\n",
    "#     return result\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"HIERARCHICAL CLASSIFICATION (2 LEVELS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, review in enumerate(test_reviews):\n",
    "    result = hierarchical_classify_2level(review)\n",
    "\n",
    "    print(f\"\\nReview {i+1}: '{review}'\")\n",
    "    print(f\"\\n  Level 1 (Sentiment):\")\n",
    "    print(f\"    \u2192 {result['level1_label']}\")\n",
    "    print(f\"    \u2192 Confidence: {result['level1_conf']:.3f}\")\n",
    "\n",
    "    print(f\"\\n  Level 2 (Specific Aspect):\")\n",
    "    print(f\"    \u2192 {result['level2_label']}\")\n",
    "    print(f\"    \u2192 Confidence: {result['level2_conf']:.3f}\")\n",
    "\n",
    "    print(f\"\\n  Final Classification Path:\")\n",
    "    print(f\"    \u2192 {result['path']}\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "# Compare with flat classification\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON: Hierarchical vs Flat Classification\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Flat: All 4 categories at once\n",
    "flat_labels = [\n",
    "    \"review criticizing entertainment value and pacing\",      # 0\n",
    "    \"review criticizing technical quality and production\",    # 1\n",
    "    \"review praising technical quality and artistry\",         # 2\n",
    "    \"review praising entertainment value and enjoyment\"       # 3\n",
    "]\n",
    "\n",
    "flat_embeddings = model.encode(flat_labels)\n",
    "review_embeddings = model.encode(test_reviews)\n",
    "flat_sim = cosine_similarity(review_embeddings, flat_embeddings)\n",
    "\n",
    "print(\"\\nShowing first 3 reviews:\")\n",
    "for i in range(min(3, len(test_reviews))):\n",
    "    hier_result = hierarchical_classify_2level(test_reviews[i])\n",
    "    flat_pred = np.argmax(flat_sim[i])\n",
    "    flat_conf = flat_sim[i][flat_pred]\n",
    "\n",
    "    print(f\"\\nReview: '{test_reviews[i][:50]}...'\")\n",
    "    print(f\"  Hierarchical: {hier_result['level2_label']}\")\n",
    "    print(f\"    \u2192 Confidence: {hier_result['level2_conf']:.3f}\")\n",
    "    print(f\"  Flat:         {flat_labels[flat_pred]}\")\n",
    "    print(f\"    \u2192 Confidence: {flat_conf:.3f}\")\n",
    "    print(f\"  Confidence Diff: {hier_result['level2_conf'] - flat_conf:+.3f}\")\n",
    "\n",
    "# TODO: After implementing 3-level, uncomment to test it:\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"TESTING 3-LEVEL HIERARCHICAL CLASSIFICATION\")\n",
    "# print(\"=\"*80)\n",
    "#\n",
    "# for i, review in enumerate(test_reviews):\n",
    "#     result = hierarchical_classify_3level(review)\n",
    "#     print(f\"\\n{i+1}. '{review[:60]}...'\")\n",
    "#     print(f\"   Path: {result['path']}\")\n",
    "#     print(f\"   Level 3 confidence: {result['level3_conf']:.3f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ylpZyCpQGvja",
    "outputId": "55c9eda9-2db4-4750-8525-fc41c2bee0f4"
   },
   "execution_count": 35,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "================================================================================\n",
      "HIERARCHICAL CLASSIFICATION (2 LEVELS)\n",
      "================================================================================\n",
      "\n",
      "Review 1: 'Brilliant performances and stunning cinematography'\n",
      "\n",
      "  Level 1 (Sentiment):\n",
      "    \u2192 positive sentiment review\n",
      "    \u2192 Confidence: 0.224\n",
      "\n",
      "  Level 2 (Specific Aspect):\n",
      "    \u2192 review praising technical quality and artistry\n",
      "    \u2192 Confidence: 0.386\n",
      "\n",
      "  Final Classification Path:\n",
      "    \u2192 Positive -> review praising technical quality and artistry\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Review 2: 'Exceptional directing and beautiful visuals'\n",
      "\n",
      "  Level 1 (Sentiment):\n",
      "    \u2192 positive sentiment review\n",
      "    \u2192 Confidence: 0.241\n",
      "\n",
      "  Level 2 (Specific Aspect):\n",
      "    \u2192 review praising technical quality and artistry\n",
      "    \u2192 Confidence: 0.426\n",
      "\n",
      "  Final Classification Path:\n",
      "    \u2192 Positive -> review praising technical quality and artistry\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Review 3: 'So much fun! Had a great time watching'\n",
      "\n",
      "  Level 1 (Sentiment):\n",
      "    \u2192 positive sentiment review\n",
      "    \u2192 Confidence: 0.044\n",
      "\n",
      "  Level 2 (Specific Aspect):\n",
      "    \u2192 review praising entertainment value and enjoyment\n",
      "    \u2192 Confidence: 0.238\n",
      "\n",
      "  Final Classification Path:\n",
      "    \u2192 Positive -> review praising entertainment value and enjoyment\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Review 4: 'Really entertaining and enjoyable'\n",
      "\n",
      "  Level 1 (Sentiment):\n",
      "    \u2192 positive sentiment review\n",
      "    \u2192 Confidence: 0.251\n",
      "\n",
      "  Level 2 (Specific Aspect):\n",
      "    \u2192 review praising entertainment value and enjoyment\n",
      "    \u2192 Confidence: 0.486\n",
      "\n",
      "  Final Classification Path:\n",
      "    \u2192 Positive -> review praising entertainment value and enjoyment\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Review 5: 'Incredibly dull and slow-paced'\n",
      "\n",
      "  Level 1 (Sentiment):\n",
      "    \u2192 negative sentiment review\n",
      "    \u2192 Confidence: 0.292\n",
      "\n",
      "  Level 2 (Specific Aspect):\n",
      "    \u2192 review criticizing entertainment value and pacing\n",
      "    \u2192 Confidence: 0.578\n",
      "\n",
      "  Final Classification Path:\n",
      "    \u2192 Negative -> review criticizing entertainment value and pacing\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Review 6: 'Boring, nothing happens for two hours'\n",
      "\n",
      "  Level 1 (Sentiment):\n",
      "    \u2192 negative sentiment review\n",
      "    \u2192 Confidence: 0.199\n",
      "\n",
      "  Level 2 (Specific Aspect):\n",
      "    \u2192 review criticizing entertainment value and pacing\n",
      "    \u2192 Confidence: 0.412\n",
      "\n",
      "  Final Classification Path:\n",
      "    \u2192 Negative -> review criticizing entertainment value and pacing\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Review 7: 'Poor acting and terrible script'\n",
      "\n",
      "  Level 1 (Sentiment):\n",
      "    \u2192 negative sentiment review\n",
      "    \u2192 Confidence: 0.348\n",
      "\n",
      "  Level 2 (Specific Aspect):\n",
      "    \u2192 review criticizing entertainment value and pacing\n",
      "    \u2192 Confidence: 0.520\n",
      "\n",
      "  Final Classification Path:\n",
      "    \u2192 Negative -> review criticizing entertainment value and pacing\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Review 8: 'Awful production values and bad directing'\n",
      "\n",
      "  Level 1 (Sentiment):\n",
      "    \u2192 negative sentiment review\n",
      "    \u2192 Confidence: 0.324\n",
      "\n",
      "  Level 2 (Specific Aspect):\n",
      "    \u2192 review criticizing entertainment value and pacing\n",
      "    \u2192 Confidence: 0.500\n",
      "\n",
      "  Final Classification Path:\n",
      "    \u2192 Negative -> review criticizing entertainment value and pacing\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "COMPARISON: Hierarchical vs Flat Classification\n",
      "================================================================================\n",
      "\n",
      "Showing first 3 reviews:\n",
      "\n",
      "Review: 'Brilliant performances and stunning cinematography...'\n",
      "  Hierarchical: review praising technical quality and artistry\n",
      "    \u2192 Confidence: 0.386\n",
      "  Flat:         review praising technical quality and artistry\n",
      "    \u2192 Confidence: 0.386\n",
      "  Confidence Diff: +0.000\n",
      "\n",
      "Review: 'Exceptional directing and beautiful visuals...'\n",
      "  Hierarchical: review praising technical quality and artistry\n",
      "    \u2192 Confidence: 0.426\n",
      "  Flat:         review praising technical quality and artistry\n",
      "    \u2192 Confidence: 0.426\n",
      "  Confidence Diff: -0.000\n",
      "\n",
      "Review: 'So much fun! Had a great time watching...'\n",
      "  Hierarchical: review praising entertainment value and enjoyment\n",
      "    \u2192 Confidence: 0.238\n",
      "  Flat:         review praising entertainment value and enjoyment\n",
      "    \u2192 Confidence: 0.238\n",
      "  Confidence Diff: +0.000\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Questions:**\n",
    "\n",
    "1. Compare confidence scores for hierarchical vs flat classification. Why might breaking decisions into steps help with confidence?\n",
    "\n",
    "2. Could the classifier make the right Level 2 decision even if Level 1 was wrong? What does this reveal about error propagation?\n",
    "\n",
    "3. After implementing 3 levels: Did added granularity help or hurt? Are Level 3 confidence scores lower than Level 2?"
   ],
   "metadata": {
    "id": "XsmM9pVIG9pE"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Hard Task 2: Active Learning to Minimize Labeling\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Execute code to see active learning selecting uncertain examples vs random selection\n",
    "2. Study what makes the selected samples uncertain\n",
    "3. Uncomment TODO to implement a different selection strategy\n",
    "4. Compare which strategy reaches high F1 score faster\n",
    "5. Fill in results table and analyze learning curves"
   ],
   "metadata": {
    "id": "MFl60yXCHC_D"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "data = load_dataset(\"rotten_tomatoes\")\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# Prepare datasets\n",
    "pool_size = 2000\n",
    "test_size = 300\n",
    "\n",
    "train_pool = data[\"train\"].shuffle(seed=42).select(range(pool_size))\n",
    "test_set = data[\"test\"].shuffle(seed=42).select(range(test_size))\n",
    "\n",
    "# Generate embeddings upfront (faster)\n",
    "print(\"Generating embeddings for 2000 training pool and 300 test samples...\")\n",
    "pool_embeddings = model.encode(train_pool[\"text\"], show_progress_bar=True)\n",
    "test_embeddings = model.encode(test_set[\"text\"], show_progress_bar=False)\n",
    "print(\"\u2713 Embeddings ready\\n\")\n",
    "\n",
    "def uncertainty_sampling(clf, unlabeled_embeddings, n_samples=50):\n",
    "    \"\"\"\n",
    "    Select samples where model is most uncertain.\n",
    "    Strategy: Pick samples with lowest confidence (closest to 50-50)\n",
    "    \"\"\"\n",
    "    probs = clf.predict_proba(unlabeled_embeddings)\n",
    "    # Uncertainty = 1 - max(prob) = how close to 50-50 the prediction is\n",
    "    uncertainties = 1 - np.max(probs, axis=1)\n",
    "\n",
    "    # Get indices of most uncertain samples\n",
    "    most_uncertain_indices = np.argsort(uncertainties)[-n_samples:]\n",
    "    return most_uncertain_indices\n",
    "\n",
    "# TODO: Uncomment to implement alternative selection strategy\n",
    "# def uncertainty_sampling(clf, unlabeled_embeddings, n_samples=50):\n",
    "#     \"\"\"\n",
    "#     Alternative strategy: Margin sampling\n",
    "#     Select samples where top two predictions are closest\n",
    "#     \"\"\"\n",
    "#     probs = clf.predict_proba(unlabeled_embeddings)\n",
    "#     # Sort probabilities for each sample\n",
    "#     sorted_probs = np.sort(probs, axis=1)\n",
    "#     # Margin = difference between top two\n",
    "#     margins = sorted_probs[:, -1] - sorted_probs[:, -2]\n",
    "#\n",
    "#     # Get indices of smallest margins (most uncertain)\n",
    "#     most_uncertain_indices = np.argsort(margins)[:n_samples]\n",
    "#     return most_uncertain_indices\n",
    "\n",
    "def random_sampling(n_available, n_samples=50):\n",
    "    \"\"\"Baseline: Random selection\"\"\"\n",
    "    return np.random.choice(n_available, size=min(n_samples, n_available), replace=False)\n",
    "\n",
    "# Active Learning Simulation\n",
    "print(\"=\"*80)\n",
    "print(\"ACTIVE LEARNING SIMULATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"Strategy: Start with 100 labeled, then iteratively add 50 most uncertain samples\")\n",
    "print(\"Compare: Active Learning vs Random Sampling\\n\")\n",
    "\n",
    "# Configuration\n",
    "initial_size = 100\n",
    "samples_per_iteration = 50\n",
    "n_iterations = 10\n",
    "\n",
    "# Initialize\n",
    "labeled_indices = list(range(initial_size))\n",
    "unlabeled_indices = list(range(initial_size, pool_size))\n",
    "\n",
    "active_scores = []\n",
    "random_scores = []\n",
    "iteration_labeled_sizes = []\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    current_size = len(labeled_indices)\n",
    "    iteration_labeled_sizes.append(current_size)\n",
    "\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Iteration {iteration + 1}/{n_iterations} - Labeled samples: {current_size}\")\n",
    "    print('='*80)\n",
    "\n",
    "    # Get current labeled data\n",
    "    labeled_embeddings = pool_embeddings[labeled_indices]\n",
    "    labeled_labels = [train_pool[\"label\"][i] for i in labeled_indices]\n",
    "\n",
    "    # Train classifier\n",
    "    clf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    clf.fit(labeled_embeddings, labeled_labels)\n",
    "\n",
    "    # Evaluate\n",
    "    test_pred = clf.predict(test_embeddings)\n",
    "    active_f1 = f1_score(test_set[\"label\"], test_pred, average='weighted')\n",
    "    active_scores.append(active_f1)\n",
    "\n",
    "    print(f\"Active Learning F1: {active_f1:.4f}\")\n",
    "\n",
    "    # Compare with random sampling (same number of samples)\n",
    "    random_indices = list(range(initial_size)) + list(\n",
    "        np.random.choice(range(initial_size, pool_size),\n",
    "                        size=min(current_size - initial_size, pool_size - initial_size),\n",
    "                        replace=False)\n",
    "    )\n",
    "    random_embeddings = pool_embeddings[random_indices]\n",
    "    random_labels = [train_pool[\"label\"][i] for i in random_indices]\n",
    "\n",
    "    clf_random = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    clf_random.fit(random_embeddings, random_labels)\n",
    "    random_pred = clf_random.predict(test_embeddings)\n",
    "    random_f1 = f1_score(test_set[\"label\"], random_pred, average='weighted')\n",
    "    random_scores.append(random_f1)\n",
    "\n",
    "    print(f\"Random Sampling F1:  {random_f1:.4f}\")\n",
    "    print(f\"Improvement:         {active_f1 - random_f1:+.4f}\")\n",
    "\n",
    "    # Select next batch using active learning\n",
    "    if len(unlabeled_indices) < samples_per_iteration:\n",
    "        print(f\"\\n\u2713 Stopping: Only {len(unlabeled_indices)} samples left\")\n",
    "        break\n",
    "\n",
    "    unlabeled_embeddings = pool_embeddings[unlabeled_indices]\n",
    "    uncertain_local_indices = uncertainty_sampling(clf, unlabeled_embeddings, samples_per_iteration)\n",
    "\n",
    "    # Convert to global indices\n",
    "    newly_labeled = [unlabeled_indices[i] for i in uncertain_local_indices]\n",
    "\n",
    "    # Show examples of selected samples\n",
    "    print(f\"\\nExamples of selected UNCERTAIN samples:\")\n",
    "    for i, idx in enumerate(newly_labeled[:3]):\n",
    "        probs = clf.predict_proba(pool_embeddings[idx].reshape(1, -1))[0]\n",
    "        uncertainty = 1 - np.max(probs)\n",
    "        print(f\"  {i+1}. '{train_pool['text'][idx][:60]}...'\")\n",
    "        print(f\"     Uncertainty: {uncertainty:.3f}\")\n",
    "        print(f\"     Probs: [neg={probs[0]:.3f}, pos={probs[1]:.3f}]\")\n",
    "\n",
    "    # Update sets\n",
    "    labeled_indices.extend(newly_labeled)\n",
    "    unlabeled_indices = [idx for idx in unlabeled_indices if idx not in newly_labeled]\n",
    "    print()\n",
    "\n",
    "# Results Summary\n",
    "print(\"=\"*80)\n",
    "print(\"FINAL RESULTS - LEARNING CURVES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n{'Labeled':<10s} {'Active F1':<12s} {'Random F1':<12s} {'Difference':<12s}\")\n",
    "print(\"-\"*50)\n",
    "for size, active, random in zip(iteration_labeled_sizes, active_scores, random_scores):\n",
    "    diff = active - random\n",
    "    marker = \"  \u2713\" if diff > 0.01 else \"\"\n",
    "    print(f\"{size:<10d} {active:.4f}       {random:.4f}       {diff:+.4f}{marker}\")\n",
    "\n",
    "avg_improvement = np.mean(np.array(active_scores) - np.array(random_scores))\n",
    "print(f\"\\nAverage Improvement: {avg_improvement:+.4f}\")\n",
    "\n",
    "# Find when active learning reaches target F1\n",
    "target_f1 = 0.85\n",
    "active_reached = next((size for size, f1 in zip(iteration_labeled_sizes, active_scores) if f1 >= target_f1), None)\n",
    "random_reached = next((size for size, f1 in zip(iteration_labeled_sizes, random_scores) if f1 >= target_f1), None)\n",
    "\n",
    "if active_reached or random_reached:\n",
    "    print(f\"\\nTo reach F1={target_f1}:\")\n",
    "    if active_reached:\n",
    "        print(f\"  Active Learning: {active_reached} labeled samples\")\n",
    "    else:\n",
    "        print(f\"  Active Learning: Did not reach {target_f1}\")\n",
    "\n",
    "    if random_reached:\n",
    "        print(f\"  Random Sampling: {random_reached} labeled samples\")\n",
    "    else:\n",
    "        print(f\"  Random Sampling: Did not reach {target_f1}\")\n",
    "\n",
    "    if active_reached and random_reached:\n",
    "        savings = random_reached - active_reached\n",
    "        print(f\"  \u2192 Active Learning saved {savings} labeled samples ({savings/random_reached:.1%})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TODO: Try different selection strategies and record results:\")\n",
    "print(\"=\"*80)\n",
    "print(\"| Strategy              | Samples to F1=0.85 | Avg Improvement | Notes |\")\n",
    "print(\"|----------------------|-------------------|-----------------|-------|\")\n",
    "print(\"| Uncertainty (1-max)  | ???               | ???             |       |\")\n",
    "print(\"| Margin sampling      | ???               | ???             |       |\")\n",
    "print(\"| YOUR STRATEGY        | ???               | ???             |       |\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "31e52c083fbd45de8bc39eef296bfd2b",
      "19ff4554c5c04b7f8e4f921005ccdd9e",
      "27bb8412e000498295babac8fbe52c3f",
      "f49402608ba44a6e9518f44bcbbdf1e5",
      "3333b0243467411ea154c5f5696008a2",
      "3a6373e168304156ba0ed95c4a641fdb",
      "04f978025bf34d2ba1f5fe98c69f33f5",
      "9de9769d6370464199eed6489432614c",
      "bf33dc78e48d4cbfb6bc9c4793e9b179",
      "33d4750e1b904615ab015b7c940601b8",
      "00e048491fce4d10836ef45f0f307dee"
     ]
    },
    "id": "VSP2ei_2G4hH",
    "outputId": "746d8713-a669-465b-eb65-c1fb471f86a0"
   },
   "execution_count": 36,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Generating embeddings for 2000 training pool and 300 test samples...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/63 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "31e52c083fbd45de8bc39eef296bfd2b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2713 Embeddings ready\n",
      "\n",
      "================================================================================\n",
      "ACTIVE LEARNING SIMULATION\n",
      "================================================================================\n",
      "Strategy: Start with 100 labeled, then iteratively add 50 most uncertain samples\n",
      "Compare: Active Learning vs Random Sampling\n",
      "\n",
      "================================================================================\n",
      "Iteration 1/10 - Labeled samples: 100\n",
      "================================================================================\n",
      "Active Learning F1: 0.7535\n",
      "Random Sampling F1:  0.7535\n",
      "Improvement:         +0.0000\n",
      "\n",
      "Examples of selected UNCERTAIN samples:\n",
      "  1. 'it uses the pain and violence of war as background material ...'\n",
      "     Uncertainty: 0.495\n",
      "     Probs: [neg=0.505, pos=0.495]\n",
      "  2. '. . . the tale of her passionate , tumultuous affair with mu...'\n",
      "     Uncertainty: 0.495\n",
      "     Probs: [neg=0.505, pos=0.495]\n",
      "  3. 'inventive , fun , intoxicatingly sexy , violent , self-indul...'\n",
      "     Uncertainty: 0.495\n",
      "     Probs: [neg=0.505, pos=0.495]\n",
      "\n",
      "================================================================================\n",
      "Iteration 2/10 - Labeled samples: 150\n",
      "================================================================================\n",
      "Active Learning F1: 0.8023\n",
      "Random Sampling F1:  0.8222\n",
      "Improvement:         -0.0199\n",
      "\n",
      "Examples of selected UNCERTAIN samples:\n",
      "  1. 'a vile , incoherent mess . . . a scummy ripoff of david cron...'\n",
      "     Uncertainty: 0.492\n",
      "     Probs: [neg=0.492, pos=0.508]\n",
      "  2. 'why come up with something even quasi-original , when you ca...'\n",
      "     Uncertainty: 0.492\n",
      "     Probs: [neg=0.492, pos=0.508]\n",
      "  3. 'god is great , the movie's not ....'\n",
      "     Uncertainty: 0.492\n",
      "     Probs: [neg=0.508, pos=0.492]\n",
      "\n",
      "================================================================================\n",
      "Iteration 3/10 - Labeled samples: 200\n",
      "================================================================================\n",
      "Active Learning F1: 0.8298\n",
      "Random Sampling F1:  0.8329\n",
      "Improvement:         -0.0031\n",
      "\n",
      "Examples of selected UNCERTAIN samples:\n",
      "  1. 'light the candles , bring out the cake and don't fret about ...'\n",
      "     Uncertainty: 0.488\n",
      "     Probs: [neg=0.488, pos=0.512]\n",
      "  2. 'it's not difficult to spot the culprit early-on in this pred...'\n",
      "     Uncertainty: 0.489\n",
      "     Probs: [neg=0.511, pos=0.489]\n",
      "  3. 'mcconaughey's fun to watch , the dragons are okay , not much...'\n",
      "     Uncertainty: 0.489\n",
      "     Probs: [neg=0.489, pos=0.511]\n",
      "\n",
      "================================================================================\n",
      "Iteration 4/10 - Labeled samples: 250\n",
      "================================================================================\n",
      "Active Learning F1: 0.8332\n",
      "Random Sampling F1:  0.8100\n",
      "Improvement:         +0.0233\n",
      "\n",
      "Examples of selected UNCERTAIN samples:\n",
      "  1. 'i didn't find much fascination in the swinging . what they'r...'\n",
      "     Uncertainty: 0.489\n",
      "     Probs: [neg=0.511, pos=0.489]\n",
      "  2. 'a minor picture with a major identity crisis -- it's sort of...'\n",
      "     Uncertainty: 0.489\n",
      "     Probs: [neg=0.511, pos=0.489]\n",
      "  3. 'howard and his co-stars all give committed performances , bu...'\n",
      "     Uncertainty: 0.489\n",
      "     Probs: [neg=0.511, pos=0.489]\n",
      "\n",
      "================================================================================\n",
      "Iteration 5/10 - Labeled samples: 300\n",
      "================================================================================\n",
      "Active Learning F1: 0.8332\n",
      "Random Sampling F1:  0.8332\n",
      "Improvement:         +0.0001\n",
      "\n",
      "Examples of selected UNCERTAIN samples:\n",
      "  1. 'fred schepisi's tale of four englishmen facing the prospect ...'\n",
      "     Uncertainty: 0.485\n",
      "     Probs: [neg=0.515, pos=0.485]\n",
      "  2. 'a solid examination of the male midlife crisis ....'\n",
      "     Uncertainty: 0.485\n",
      "     Probs: [neg=0.485, pos=0.515]\n",
      "  3. 'a simmering psychological drama in which the bursts of sudde...'\n",
      "     Uncertainty: 0.486\n",
      "     Probs: [neg=0.486, pos=0.514]\n",
      "\n",
      "================================================================================\n",
      "Iteration 6/10 - Labeled samples: 350\n",
      "================================================================================\n",
      "Active Learning F1: 0.8299\n",
      "Random Sampling F1:  0.8492\n",
      "Improvement:         -0.0193\n",
      "\n",
      "Examples of selected UNCERTAIN samples:\n",
      "  1. 'a teasing drama whose relentless good-deed/bad-deed reversal...'\n",
      "     Uncertainty: 0.481\n",
      "     Probs: [neg=0.481, pos=0.519]\n",
      "  2. 'proof that a thriller can be sleekly shot , expertly cast , ...'\n",
      "     Uncertainty: 0.481\n",
      "     Probs: [neg=0.519, pos=0.481]\n",
      "  3. 'a deceivingly simple film , one that grows in power in retro...'\n",
      "     Uncertainty: 0.481\n",
      "     Probs: [neg=0.481, pos=0.519]\n",
      "\n",
      "================================================================================\n",
      "Iteration 7/10 - Labeled samples: 400\n",
      "================================================================================\n",
      "Active Learning F1: 0.8498\n",
      "Random Sampling F1:  0.8394\n",
      "Improvement:         +0.0105\n",
      "\n",
      "Examples of selected UNCERTAIN samples:\n",
      "  1. 'it's all very cute , though not terribly funny if you're mor...'\n",
      "     Uncertainty: 0.470\n",
      "     Probs: [neg=0.530, pos=0.470]\n",
      "  2. 'bound to appeal to women looking for a howlingly trashy time...'\n",
      "     Uncertainty: 0.470\n",
      "     Probs: [neg=0.530, pos=0.470]\n",
      "  3. 'the ring is worth a look , if you don't demand much more tha...'\n",
      "     Uncertainty: 0.471\n",
      "     Probs: [neg=0.471, pos=0.529]\n",
      "\n",
      "================================================================================\n",
      "Iteration 8/10 - Labeled samples: 450\n",
      "================================================================================\n",
      "Active Learning F1: 0.8465\n",
      "Random Sampling F1:  0.8232\n",
      "Improvement:         +0.0233\n",
      "\n",
      "Examples of selected UNCERTAIN samples:\n",
      "  1. 'it's a minor comedy that tries to balance sweetness with coa...'\n",
      "     Uncertainty: 0.454\n",
      "     Probs: [neg=0.454, pos=0.546]\n",
      "  2. 'as home movie gone haywire , it's pretty enjoyable , but as ...'\n",
      "     Uncertainty: 0.454\n",
      "     Probs: [neg=0.454, pos=0.546]\n",
      "  3. 'brainless , but enjoyably over-the-top , the retro gang melo...'\n",
      "     Uncertainty: 0.455\n",
      "     Probs: [neg=0.455, pos=0.545]\n",
      "\n",
      "================================================================================\n",
      "Iteration 9/10 - Labeled samples: 500\n",
      "================================================================================\n",
      "Active Learning F1: 0.8699\n",
      "Random Sampling F1:  0.8599\n",
      "Improvement:         +0.0101\n",
      "\n",
      "Examples of selected UNCERTAIN samples:\n",
      "  1. 'we root for [clara and paul] , even like them , though perha...'\n",
      "     Uncertainty: 0.437\n",
      "     Probs: [neg=0.563, pos=0.437]\n",
      "  2. 'in many ways , reminiscent of 1992's unforgiven which also u...'\n",
      "     Uncertainty: 0.437\n",
      "     Probs: [neg=0.437, pos=0.563]\n",
      "  3. 'any enjoyment will be hinge from a personal threshold of wat...'\n",
      "     Uncertainty: 0.438\n",
      "     Probs: [neg=0.438, pos=0.562]\n",
      "\n",
      "================================================================================\n",
      "Iteration 10/10 - Labeled samples: 550\n",
      "================================================================================\n",
      "Active Learning F1: 0.8632\n",
      "Random Sampling F1:  0.8430\n",
      "Improvement:         +0.0202\n",
      "\n",
      "Examples of selected UNCERTAIN samples:\n",
      "  1. 'playfully profound . . . and crazier than michael jackson on...'\n",
      "     Uncertainty: 0.425\n",
      "     Probs: [neg=0.425, pos=0.575]\n",
      "  2. 'mr . wedge and mr . saldanha handle the mix of verbal jokes ...'\n",
      "     Uncertainty: 0.425\n",
      "     Probs: [neg=0.425, pos=0.575]\n",
      "  3. 'i didn't laugh . i didn't smile . i survived ....'\n",
      "     Uncertainty: 0.426\n",
      "     Probs: [neg=0.426, pos=0.574]\n",
      "\n",
      "================================================================================\n",
      "FINAL RESULTS - LEARNING CURVES\n",
      "================================================================================\n",
      "\n",
      "Labeled    Active F1    Random F1    Difference  \n",
      "--------------------------------------------------\n",
      "100        0.7535       0.7535       +0.0000\n",
      "150        0.8023       0.8222       -0.0199\n",
      "200        0.8298       0.8329       -0.0031\n",
      "250        0.8332       0.8100       +0.0233  \u2713\n",
      "300        0.8332       0.8332       +0.0001\n",
      "350        0.8299       0.8492       -0.0193\n",
      "400        0.8498       0.8394       +0.0105  \u2713\n",
      "450        0.8465       0.8232       +0.0233  \u2713\n",
      "500        0.8699       0.8599       +0.0101  \u2713\n",
      "550        0.8632       0.8430       +0.0202  \u2713\n",
      "\n",
      "Average Improvement: +0.0045\n",
      "\n",
      "To reach F1=0.85:\n",
      "  Active Learning: 500 labeled samples\n",
      "  Random Sampling: 500 labeled samples\n",
      "  \u2192 Active Learning saved 0 labeled samples (0.0%)\n",
      "\n",
      "================================================================================\n",
      "TODO: Try different selection strategies and record results:\n",
      "================================================================================\n",
      "| Strategy              | Samples to F1=0.85 | Avg Improvement | Notes |\n",
      "|----------------------|-------------------|-----------------|-------|\n",
      "| Uncertainty (1-max)  | ???               | ???             |       |\n",
      "| Margin sampling      | ???               | ???             |       |\n",
      "| YOUR STRATEGY        | ???               | ???             |       |\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Questions:**\n",
    "\n",
    "1. What makes the uncertain samples uncertain? Are they using hedging language, mixed sentiment, or ambiguous wording?\n",
    "\n",
    "2. At what point did active learning pull ahead of random sampling? How much can active learning reduce labeling costs?\n",
    "\n",
    "3. Why are samples with probabilities like [0.52, 0.48] more valuable for training than confident samples?"
   ],
   "metadata": {
    "id": "yzYVwXTBHOrA"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Hard Task 3: Ensemble Classifier\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Execute code to see three models performing individually and as an ensemble\n",
    "2. Analyze when models disagree and which model is correct most often\n",
    "3. Uncomment TODO to add a fourth model to the ensemble\n",
    "4. Uncomment TODO to implement weighted voting based on validation performance\n",
    "5. Compare ensemble methods to determine which works best"
   ],
   "metadata": {
    "id": "1UYZHZs5HYZX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "data = load_dataset(\"rotten_tomatoes\")\n",
    "\n",
    "# Configuration\n",
    "train_size = 1500\n",
    "test_size = 300\n",
    "\n",
    "train_subset = data[\"train\"].shuffle(seed=42).select(range(train_size))\n",
    "test_subset = data[\"test\"].shuffle(seed=42).select(range(test_size))\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"BUILDING ENSEMBLE OF CLASSIFIERS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Model 1: Task-Specific (Twitter RoBERTa)\n",
    "print(\"\\n[1/3] Loading Task-Specific Model (Twitter RoBERTa)...\")\n",
    "task_model = pipeline(\n",
    "    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    tokenizer=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    return_all_scores=True,\n",
    "    device=-1\n",
    ")\n",
    "\n",
    "# Model 2: Embedding + Logistic Regression\n",
    "print(\"[2/3] Training Embedding Classifier...\")\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "train_embeddings = embedding_model.encode(train_subset[\"text\"], show_progress_bar=True)\n",
    "test_embeddings = embedding_model.encode(test_subset[\"text\"], show_progress_bar=False)\n",
    "\n",
    "clf_embedding = LogisticRegression(random_state=42, max_iter=1000)\n",
    "clf_embedding.fit(train_embeddings, train_subset[\"label\"])\n",
    "\n",
    "# Model 3: Zero-Shot\n",
    "print(\"[3/3] Setting up Zero-Shot Classifier...\")\n",
    "zero_shot_labels = [\"A very negative movie review\", \"A very positive movie review\"]\n",
    "zero_shot_label_embeddings = embedding_model.encode(zero_shot_labels)\n",
    "\n",
    "# TODO: Uncomment to add Model 4 - Different embedding model\n",
    "# print(\"[4/4] Training with alternative embedding model...\")\n",
    "# embedding_model_alt = SentenceTransformer('all-MiniLM-L6-v2')  # Smaller, faster\n",
    "# train_embeddings_alt = embedding_model_alt.encode(train_subset[\"text\"], show_progress_bar=True)\n",
    "# test_embeddings_alt = embedding_model_alt.encode(test_subset[\"text\"], show_progress_bar=False)\n",
    "# clf_embedding_alt = LogisticRegression(random_state=42, max_iter=1000)\n",
    "# clf_embedding_alt.fit(train_embeddings_alt, train_subset[\"label\"])\n",
    "\n",
    "print(\"\\n\u2713 All models ready\")\n",
    "\n",
    "# Get predictions from all models\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATING PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nModel 1: Task-Specific...\")\n",
    "pred_task = []\n",
    "conf_task = []\n",
    "for text in test_subset[\"text\"]:\n",
    "    output = task_model(text)[0]\n",
    "    neg_score = output[0][\"score\"]\n",
    "    pos_score = output[2][\"score\"]\n",
    "    pred_task.append(1 if pos_score > neg_score else 0)\n",
    "    conf_task.append(max(neg_score, pos_score))\n",
    "\n",
    "print(\"Model 2: Embedding Classifier...\")\n",
    "pred_embedding = clf_embedding.predict(test_embeddings)\n",
    "conf_embedding = np.max(clf_embedding.predict_proba(test_embeddings), axis=1)\n",
    "\n",
    "print(\"Model 3: Zero-Shot...\")\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "zero_shot_sim = cosine_similarity(test_embeddings, zero_shot_label_embeddings)\n",
    "pred_zero_shot = np.argmax(zero_shot_sim, axis=1)\n",
    "conf_zero_shot = np.max(zero_shot_sim, axis=1)\n",
    "\n",
    "# TODO: Uncomment if you added Model 4\n",
    "# print(\"Model 4: Alternative Embedding...\")\n",
    "# pred_alt = clf_embedding_alt.predict(test_embeddings_alt)\n",
    "# conf_alt = np.max(clf_embedding_alt.predict_proba(test_embeddings_alt), axis=1)\n",
    "\n",
    "# Evaluate individual models\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INDIVIDUAL MODEL PERFORMANCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "models = [\n",
    "    (\"Task-Specific\", pred_task),\n",
    "    (\"Embedding + LR\", pred_embedding),\n",
    "    (\"Zero-Shot\", pred_zero_shot),\n",
    "]\n",
    "\n",
    "# TODO: Uncomment if Model 4 added\n",
    "# models.append((\"Alternative Embedding\", pred_alt))\n",
    "\n",
    "individual_scores = []\n",
    "for name, predictions in models:\n",
    "    f1 = f1_score(test_subset[\"label\"], predictions, average='weighted')\n",
    "    acc = accuracy_score(test_subset[\"label\"], predictions)\n",
    "    individual_scores.append(f1)\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  F1 Score:  {f1:.4f}\")\n",
    "    print(f\"  Accuracy:  {acc:.4f}\")\n",
    "\n",
    "# Ensemble Methods\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENSEMBLE METHODS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Method 1: Simple Majority Voting\n",
    "ensemble_votes = np.array([pred_task, pred_embedding, pred_zero_shot])\n",
    "# TODO: Add Model 4 if available\n",
    "# ensemble_votes = np.array([pred_task, pred_embedding, pred_zero_shot, pred_alt])\n",
    "\n",
    "pred_majority = np.apply_along_axis(lambda x: np.bincount(x).argmax(), 0, ensemble_votes)\n",
    "\n",
    "maj_f1 = f1_score(test_subset[\"label\"], pred_majority, average='weighted')\n",
    "maj_acc = accuracy_score(test_subset[\"label\"], pred_majority)\n",
    "\n",
    "print(f\"\\n1. Simple Majority Voting:\")\n",
    "print(f\"   F1 Score:  {maj_f1:.4f}\")\n",
    "print(f\"   Accuracy:  {maj_acc:.4f}\")\n",
    "\n",
    "# Method 2: Confidence-Weighted Voting\n",
    "weights = np.array([conf_task, conf_embedding, conf_zero_shot])\n",
    "# TODO: Add Model 4 weights if available\n",
    "# weights = np.array([conf_task, conf_embedding, conf_zero_shot, conf_alt])\n",
    "\n",
    "weighted_votes = np.zeros((len(test_subset), 2))\n",
    "for i in range(len(test_subset)):\n",
    "    for model_idx in range(len(models)):\n",
    "        vote = ensemble_votes[model_idx, i]\n",
    "        weight = weights[model_idx, i]\n",
    "        weighted_votes[i, vote] += weight\n",
    "\n",
    "pred_weighted = np.argmax(weighted_votes, axis=1)\n",
    "\n",
    "weight_f1 = f1_score(test_subset[\"label\"], pred_weighted, average='weighted')\n",
    "weight_acc = accuracy_score(test_subset[\"label\"], pred_weighted)\n",
    "\n",
    "print(f\"\\n2. Confidence-Weighted Voting:\")\n",
    "print(f\"   F1 Score:  {weight_f1:.4f}\")\n",
    "print(f\"   Accuracy:  {weight_acc:.4f}\")\n",
    "\n",
    "# TODO: Uncomment to implement Method 3: Performance-Weighted Voting\n",
    "# Method 3: Weight models by their F1 scores\n",
    "# print(f\"\\n3. Performance-Weighted Voting:\")\n",
    "# model_weights = np.array(individual_scores)  # Use F1 scores as weights\n",
    "# model_weights = model_weights / model_weights.sum()  # Normalize\n",
    "#\n",
    "# perf_weighted_votes = np.zeros((len(test_subset), 2))\n",
    "# for i in range(len(test_subset)):\n",
    "#     for model_idx in range(len(models)):\n",
    "#         vote = ensemble_votes[model_idx, i]\n",
    "#         weight = model_weights[model_idx]\n",
    "#         perf_weighted_votes[i, vote] += weight\n",
    "#\n",
    "# pred_perf_weighted = np.argmax(perf_weighted_votes, axis=1)\n",
    "# perf_f1 = f1_score(test_subset[\"label\"], pred_perf_weighted, average='weighted')\n",
    "# perf_acc = accuracy_score(test_subset[\"label\"], pred_perf_weighted)\n",
    "# print(f\"   F1 Score:  {perf_f1:.4f}\")\n",
    "# print(f\"   Accuracy:  {perf_acc:.4f}\")\n",
    "\n",
    "# Comparison Table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = [\n",
    "    (\"Task-Specific (Model 1)\", individual_scores[0]),\n",
    "    (\"Embedding (Model 2)\", individual_scores[1]),\n",
    "    (\"Zero-Shot (Model 3)\", individual_scores[2]),\n",
    "    (\"\u2500\" * 30, None),\n",
    "    (\"Ensemble: Majority Vote\", maj_f1),\n",
    "    (\"Ensemble: Confidence-Weighted\", weight_f1),\n",
    "]\n",
    "\n",
    "# TODO: Add Model 4 and performance-weighted if implemented\n",
    "# results.insert(3, (\"Alternative Embedding (Model 4)\", individual_scores[3]))\n",
    "# results.append((\"Ensemble: Performance-Weighted\", perf_f1))\n",
    "\n",
    "best_individual = max(individual_scores)\n",
    "\n",
    "print(f\"\\n{'Method':<35s} {'F1 Score':<12s} {'vs Best Individual':<20s}\")\n",
    "print(\"-\"*70)\n",
    "for name, score in results:\n",
    "    if score is None:\n",
    "        print(name)\n",
    "    else:\n",
    "        diff = score - best_individual if score is not None else 0\n",
    "        improvement = \"\u2713\" if diff > 0.001 else \"\"\n",
    "        print(f\"{name:<35s} {score:.4f}       {diff:+.4f}  {improvement}\")\n",
    "\n",
    "# Analyze disagreements\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYZING MODEL DISAGREEMENTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "disagreements = []\n",
    "unanimous_correct = 0\n",
    "unanimous_wrong = 0\n",
    "\n",
    "for i in range(len(test_subset)):\n",
    "    votes = ensemble_votes[:, i]\n",
    "    unique_votes = len(set(votes))\n",
    "\n",
    "    if unique_votes > 1:  # Disagreement\n",
    "        disagreements.append({\n",
    "            'index': i,\n",
    "            'text': test_subset[\"text\"][i],\n",
    "            'true': test_subset[\"label\"][i],\n",
    "            'votes': votes,\n",
    "            'ensemble': pred_majority[i],\n",
    "            'models': [models[j][0] for j in range(len(models))]\n",
    "        })\n",
    "    else:  # Unanimous\n",
    "        if votes[0] == test_subset[\"label\"][i]:\n",
    "            unanimous_correct += 1\n",
    "        else:\n",
    "            unanimous_wrong += 1\n",
    "\n",
    "print(f\"\\nVoting Patterns:\")\n",
    "print(f\"  Unanimous Correct: {unanimous_correct} ({100*unanimous_correct/len(test_subset):.1f}%)\")\n",
    "print(f\"  Unanimous Wrong:   {unanimous_wrong} ({100*unanimous_wrong/len(test_subset):.1f}%)\")\n",
    "print(f\"  Disagreements:     {len(disagreements)} ({100*len(disagreements)/len(test_subset):.1f}%)\")\n",
    "\n",
    "print(f\"\\n\" + \"-\"*80)\n",
    "print(f\"Examples of Disagreements (first 5):\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for i, case in enumerate(disagreements[:5]):\n",
    "    true_label = \"Positive\" if case['true'] == 1 else \"Negative\"\n",
    "    ensemble_label = \"Positive\" if case['ensemble'] == 1 else \"Negative\"\n",
    "    ensemble_correct = \"\u2713\" if case['ensemble'] == case['true'] else \"\u2717\"\n",
    "\n",
    "    print(f\"\\n{i+1}. '{case['text'][:60]}...'\")\n",
    "    print(f\"   True: {true_label}\")\n",
    "\n",
    "    for j, model_name in enumerate(case['models']):\n",
    "        vote_label = \"Positive\" if case['votes'][j] == 1 else \"Negative\"\n",
    "        vote_correct = \"\u2713\" if case['votes'][j] == case['true'] else \"\u2717\"\n",
    "        print(f\"   {model_name:20s}: {vote_label:8s} {vote_correct}\")\n",
    "\n",
    "    print(f\"   Ensemble Decision:   {ensemble_label:8s} {ensemble_correct}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "e90e7c8a9ae641f9a5f2c9ab66248dbc",
      "e3af2532ddea450095ef34e3c7890344",
      "220d3bb538d84e23ba45bdb1ef20e68c",
      "be25c443c44c428eb271c02c2e34578a",
      "beb2fb51e5714276808eba4b576907e3",
      "8522a39f750149109702719a81a46bb0",
      "9d04edae2e034fb39a681ea8db1f79d8",
      "e97f097bfe454537b714d50cc3e1923e",
      "6fd1de03fef54dffba7b998edeac0776",
      "2d7a4067755849cd9cb94accfed4d9c0",
      "eb3cbb80ef9a42a8b31bf282cc426aeb"
     ]
    },
    "id": "kWDdM6kBHHr9",
    "outputId": "2f7082bc-801f-4d8d-9f17-78ae80b0cdce"
   },
   "execution_count": 37,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "================================================================================\n",
      "BUILDING ENSEMBLE OF CLASSIFIERS\n",
      "================================================================================\n",
      "\n",
      "[1/3] Loading Task-Specific Model (Twitter RoBERTa)...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[2/3] Training Embedding Classifier...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/47 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e90e7c8a9ae641f9a5f2c9ab66248dbc"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[3/3] Setting up Zero-Shot Classifier...\n",
      "\n",
      "\u2713 All models ready\n",
      "\n",
      "================================================================================\n",
      "GENERATING PREDICTIONS\n",
      "================================================================================\n",
      "\n",
      "Model 1: Task-Specific...\n",
      "Model 2: Embedding Classifier...\n",
      "Model 3: Zero-Shot...\n",
      "\n",
      "================================================================================\n",
      "INDIVIDUAL MODEL PERFORMANCE\n",
      "================================================================================\n",
      "\n",
      "Task-Specific:\n",
      "  F1 Score:  0.7709\n",
      "  Accuracy:  0.7733\n",
      "\n",
      "Embedding + LR:\n",
      "  F1 Score:  0.8699\n",
      "  Accuracy:  0.8700\n",
      "\n",
      "Zero-Shot:\n",
      "  F1 Score:  0.8255\n",
      "  Accuracy:  0.8267\n",
      "\n",
      "================================================================================\n",
      "ENSEMBLE METHODS\n",
      "================================================================================\n",
      "\n",
      "1. Simple Majority Voting:\n",
      "   F1 Score:  0.8667\n",
      "   Accuracy:  0.8667\n",
      "\n",
      "2. Confidence-Weighted Voting:\n",
      "   F1 Score:  0.8632\n",
      "   Accuracy:  0.8633\n",
      "\n",
      "================================================================================\n",
      "PERFORMANCE COMPARISON\n",
      "================================================================================\n",
      "\n",
      "Method                              F1 Score     vs Best Individual  \n",
      "----------------------------------------------------------------------\n",
      "Task-Specific (Model 1)             0.7709       -0.0990  \n",
      "Embedding (Model 2)                 0.8699       +0.0000  \n",
      "Zero-Shot (Model 3)                 0.8255       -0.0444  \n",
      "\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
      "Ensemble: Majority Vote             0.8667       -0.0032  \n",
      "Ensemble: Confidence-Weighted       0.8632       -0.0066  \n",
      "\n",
      "================================================================================\n",
      "ANALYZING MODEL DISAGREEMENTS\n",
      "================================================================================\n",
      "\n",
      "Voting Patterns:\n",
      "  Unanimous Correct: 199 (66.3%)\n",
      "  Unanimous Wrong:   18 (6.0%)\n",
      "  Disagreements:     83 (27.7%)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Examples of Disagreements (first 5):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. 'we started to wonder if \u0085 some unpaid intern had just typed ...'\n",
      "   True: Negative\n",
      "   Task-Specific       : Negative \u2713\n",
      "   Embedding + LR      : Negative \u2713\n",
      "   Zero-Shot           : Positive \u2717\n",
      "   Ensemble Decision:   Negative \u2713\n",
      "\n",
      "2. 'the metaphors are provocative , but too often , the viewer i...'\n",
      "   True: Negative\n",
      "   Task-Specific       : Negative \u2713\n",
      "   Embedding + LR      : Negative \u2713\n",
      "   Zero-Shot           : Positive \u2717\n",
      "   Ensemble Decision:   Negative \u2713\n",
      "\n",
      "3. 'an uneasy mix of run-of-the-mill raunchy humor and seemingly...'\n",
      "   True: Negative\n",
      "   Task-Specific       : Negative \u2713\n",
      "   Embedding + LR      : Positive \u2717\n",
      "   Zero-Shot           : Positive \u2717\n",
      "   Ensemble Decision:   Positive \u2717\n",
      "\n",
      "4. 'it's like a \" big chill \" reunion of the baader-meinhof gang...'\n",
      "   True: Positive\n",
      "   Task-Specific       : Negative \u2717\n",
      "   Embedding + LR      : Negative \u2717\n",
      "   Zero-Shot           : Positive \u2713\n",
      "   Ensemble Decision:   Negative \u2717\n",
      "\n",
      "5. 'goldmember is funny enough to justify the embarrassment of b...'\n",
      "   True: Positive\n",
      "   Task-Specific       : Negative \u2717\n",
      "   Embedding + LR      : Negative \u2717\n",
      "   Zero-Shot           : Positive \u2713\n",
      "   Ensemble Decision:   Negative \u2717\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reflection Questions:**\n",
    "\n",
    "1. Did the ensemble beat the best individual model? What does this reveal about combining complementary strengths?\n",
    "\n",
    "2. When models disagree, which one is usually correct? Are there patterns showing when ensembles add value?\n",
    "\n",
    "3. Compare simple majority voting vs confidence-weighted voting. Which performed better? When might confidence be misleading?"
   ],
   "metadata": {
    "id": "CaoyZl58Hx4W"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Hard Task 4: Cross-Domain Transfer Learning\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Train on movie reviews and test on restaurant/product/book reviews\n",
    "2. Study performance drops to identify which domains transfer well\n",
    "3. Uncomment TODO to test reviews from a domain you choose\n",
    "4. Uncomment TODO to try few-shot domain adaptation\n",
    "5. Compare which domains need more adaptation and why"
   ],
   "metadata": {
    "id": "vCwGtJCzH_RA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "# Source domain: Movie reviews\n",
    "movie_data = load_dataset(\"rotten_tomatoes\")\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "movie_train = movie_data[\"train\"].shuffle(seed=42).select(range(2000))\n",
    "movie_test = movie_data[\"test\"].shuffle(seed=42).select(range(200))\n",
    "\n",
    "# Target domains with labeled examples\n",
    "restaurant_reviews = {\n",
    "    'text': [\n",
    "        \"Amazing food and excellent service!\",\n",
    "        \"Best restaurant in town, highly recommend\",\n",
    "        \"Delicious meals and great atmosphere\",\n",
    "        \"Outstanding cuisine and friendly staff\",\n",
    "        \"Terrible food, very disappointing\",\n",
    "        \"Awful service and poor quality\",\n",
    "        \"Not worth the money, mediocre at best\",\n",
    "        \"Disgusting food and rude waiters\",\n",
    "        \"The pasta was okay but nothing special\",\n",
    "        \"Decent place for a quick meal\"\n",
    "    ],\n",
    "    'label': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
    "}\n",
    "\n",
    "product_reviews = {\n",
    "    'text': [\n",
    "        \"This product is amazing! Works perfectly\",\n",
    "        \"Excellent quality, very satisfied\",\n",
    "        \"Great value for money, highly recommend\",\n",
    "        \"Perfect! Exactly what I needed\",\n",
    "        \"Terrible product, broke immediately\",\n",
    "        \"Waste of money, very poor quality\",\n",
    "        \"Doesn't work as advertised, disappointed\",\n",
    "        \"Awful, don't buy this\",\n",
    "        \"It's okay, does the job\",\n",
    "        \"Average product, nothing special\"\n",
    "    ],\n",
    "    'label': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
    "}\n",
    "\n",
    "book_reviews = {\n",
    "    'text': [\n",
    "        \"Brilliant book! Couldn't put it down\",\n",
    "        \"Masterfully written, highly engaging\",\n",
    "        \"One of the best books I've read\",\n",
    "        \"Fantastic story and great characters\",\n",
    "        \"Boring and poorly written\",\n",
    "        \"Terrible book, waste of time\",\n",
    "        \"Disappointing, not worth reading\",\n",
    "        \"Awful plot and weak characters\",\n",
    "        \"Decent read but nothing groundbreaking\",\n",
    "        \"It was fine, not great not terrible\"\n",
    "    ],\n",
    "    'label': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
    "}\n",
    "\n",
    "# TODO: Add your own domain - try something different!\n",
    "# YOUR_DOMAIN_reviews = {\n",
    "#     'text': [\n",
    "#         \"Positive example 1\",\n",
    "#         \"Positive example 2\",\n",
    "#         \"Positive example 3\",\n",
    "#         \"Positive example 4\",\n",
    "#         \"Negative example 1\",\n",
    "#         \"Negative example 2\",\n",
    "#         \"Negative example 3\",\n",
    "#         \"Negative example 4\",\n",
    "#         \"Neutral example 1\",\n",
    "#         \"Neutral example 2\",\n",
    "#     ],\n",
    "#     'label': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
    "# }\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CROSS-DOMAIN TRANSFER LEARNING EXPERIMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Train on source domain (movies)\n",
    "print(\"\\nTraining classifier on MOVIE REVIEWS (source domain)...\")\n",
    "train_embeddings = model.encode(movie_train[\"text\"], show_progress_bar=True)\n",
    "clf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "clf.fit(train_embeddings, movie_train[\"label\"])\n",
    "\n",
    "# Test on source domain (baseline)\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"BASELINE: Performance on Source Domain (Movies)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "movie_test_embeddings = model.encode(movie_test[\"text\"], show_progress_bar=False)\n",
    "movie_test_pred = clf.predict(movie_test_embeddings)\n",
    "source_f1 = f1_score(movie_test[\"label\"], movie_test_pred, average='weighted')\n",
    "\n",
    "print(f\"Source Domain F1: {source_f1:.4f}\")\n",
    "print(\"This is how well the classifier does on its training domain\")\n",
    "\n",
    "# Zero-shot transfer to target domains\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ZERO-SHOT TRANSFER TO TARGET DOMAINS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "target_domains = {\n",
    "    \"Restaurant Reviews\": restaurant_reviews,\n",
    "    \"Product Reviews\": product_reviews,\n",
    "    \"Book Reviews\": book_reviews,\n",
    "}\n",
    "\n",
    "# TODO: Add your domain if created\n",
    "# target_domains[\"YOUR DOMAIN\"] = YOUR_DOMAIN_reviews\n",
    "\n",
    "transfer_results = {}\n",
    "\n",
    "for domain_name, domain_data in target_domains.items():\n",
    "    print(f\"\\n{domain_name}:\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # Test without adaptation\n",
    "    domain_embeddings = model.encode(domain_data['text'])\n",
    "    domain_pred = clf.predict(domain_embeddings)\n",
    "\n",
    "    domain_f1 = f1_score(domain_data['label'], domain_pred, average='weighted')\n",
    "    domain_acc = accuracy_score(domain_data['label'], domain_pred)\n",
    "\n",
    "    print(f\"F1 Score: {domain_f1:.4f}\")\n",
    "    print(f\"Accuracy: {domain_acc:.4f}\")\n",
    "    print(f\"Performance Drop: {source_f1 - domain_f1:.4f} ({(source_f1-domain_f1)/source_f1*100:.1f}%)\")\n",
    "\n",
    "    # Show some predictions\n",
    "    print(f\"\\nExample predictions:\")\n",
    "    for i in range(3):\n",
    "        true_label = \"Positive\" if domain_data['label'][i] == 1 else \"Negative\"\n",
    "        pred_label = \"Positive\" if domain_pred[i] == 1 else \"Negative\"\n",
    "        correct = \"\u2713\" if domain_pred[i] == domain_data['label'][i] else \"\u2717\"\n",
    "\n",
    "        print(f\"  '{domain_data['text'][i][:50]}...'\")\n",
    "        print(f\"  True: {true_label} | Pred: {pred_label} {correct}\")\n",
    "\n",
    "    transfer_results[domain_name] = {\n",
    "        'zero_shot_f1': domain_f1,\n",
    "        'zero_shot_acc': domain_acc,\n",
    "        'embeddings': domain_embeddings,\n",
    "        'predictions': domain_pred\n",
    "    }\n",
    "\n",
    "# TODO: Uncomment to implement few-shot domain adaptation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEW-SHOT DOMAIN ADAPTATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"Strategy: Add first 4 examples from each target domain to training set\")\n",
    "\n",
    "adaptation_size = 4\n",
    "\n",
    "for domain_name, domain_data in target_domains.items():\n",
    "    print(f\"\\n{domain_name}:\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # Split domain data\n",
    "    adapt_texts = domain_data['text'][:adaptation_size]\n",
    "    adapt_labels = domain_data['label'][:adaptation_size]\n",
    "\n",
    "    test_texts = domain_data['text'][adaptation_size:]\n",
    "    test_labels = domain_data['label'][adaptation_size:]\n",
    "\n",
    "    # Combine source + adaptation examples\n",
    "    adapt_embeddings = model.encode(adapt_texts)\n",
    "    combined_embeddings = np.vstack([train_embeddings, adapt_embeddings])\n",
    "    combined_labels = list(movie_train[\"label\"]) + adapt_labels\n",
    "\n",
    "    # Retrain\n",
    "    clf_adapted = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    clf_adapted.fit(combined_embeddings, combined_labels)\n",
    "\n",
    "    # Test\n",
    "    test_embeddings = model.encode(test_texts)\n",
    "    adapted_pred = clf_adapted.predict(test_embeddings)\n",
    "    adapted_f1 = f1_score(test_labels, adapted_pred, average='weighted')\n",
    "\n",
    "    zero_shot_f1 = transfer_results[domain_name]['zero_shot_f1']\n",
    "    improvement = adapted_f1 - zero_shot_f1\n",
    "\n",
    "    print(f\"Zero-shot F1:  {zero_shot_f1:.4f}\")\n",
    "    print(f\"Adapted F1:    {adapted_f1:.4f}\")\n",
    "    print(f\"Improvement:   {improvement:+.4f}\")\n",
    "\n",
    "    if improvement > 0.05:\n",
    "        print(f\"\u2192 Significant improvement! Domain adaptation helped a lot\")\n",
    "    elif improvement > 0:\n",
    "        print(f\"\u2192 Slight improvement from adaptation\")\n",
    "    else:\n",
    "        print(f\"\u2192 No improvement or slight degradation\")\n",
    "\n",
    "    transfer_results[domain_name]['adapted_f1'] = adapted_f1\n",
    "    transfer_results[domain_name]['improvement'] = improvement\n",
    "\n",
    "# Analyze domain similarity\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DOMAIN SIMILARITY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate domain centroids (average embedding)\n",
    "source_centroid = np.mean(train_embeddings, axis=0)\n",
    "\n",
    "print(\"\\nDomain distances from source (movie reviews):\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "domain_distances = []\n",
    "for domain_name, domain_data in target_domains.items():\n",
    "    domain_embeddings = transfer_results[domain_name]['embeddings']\n",
    "    domain_centroid = np.mean(domain_embeddings, axis=0)\n",
    "\n",
    "    distance = np.linalg.norm(source_centroid - domain_centroid)\n",
    "    zero_shot_f1 = transfer_results[domain_name]['zero_shot_f1']\n",
    "    drop = source_f1 - zero_shot_f1\n",
    "\n",
    "    domain_distances.append((domain_name, distance, drop))\n",
    "\n",
    "    print(f\"\\n{domain_name}:\")\n",
    "    print(f\"  Embedding distance: {distance:.4f}\")\n",
    "    print(f\"  Performance drop:   {drop:.4f}\")\n",
    "    print(f\"  Zero-shot F1:       {zero_shot_f1:.4f}\")\n",
    "\n",
    "    if 'improvement' in transfer_results[domain_name]:\n",
    "        improvement = transfer_results[domain_name]['improvement']\n",
    "        print(f\"  Adaptation gain:    {improvement:+.4f}\")\n",
    "\n",
    "# Correlation analysis\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Correlation: Distance vs Performance\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "domain_distances.sort(key=lambda x: x[1])\n",
    "print(\"\\nRanked by distance to source:\")\n",
    "for name, dist, drop in domain_distances:\n",
    "    print(f\"  {name:20s}: distance={dist:.3f}, drop={drop:.3f}\")\n",
    "\n",
    "print(\"\\nObservation:\")\n",
    "print(\"  \u2192 Domains closer to movies in embedding space tend to transfer better\")\n",
    "print(\"  \u2192 Larger embedding distance correlates with larger performance drop\")\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRANSFER LEARNING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n{'Domain':<20s} {'Zero-Shot F1':<15s} {'Adapted F1':<15s} {'Improvement':<12s}\")\n",
    "print(\"-\"*65)\n",
    "for domain_name in target_domains.keys():\n",
    "    zero_f1 = transfer_results[domain_name]['zero_shot_f1']\n",
    "    adapted_f1 = transfer_results[domain_name].get('adapted_f1', 0)\n",
    "    improvement = transfer_results[domain_name].get('improvement', 0)\n",
    "\n",
    "    marker = \"\u2713\" if improvement > 0.05 else \"\"\n",
    "    print(f\"{domain_name:<20s} {zero_f1:.4f}          {adapted_f1:.4f}          {improvement:+.4f}     {marker}\")\n",
    "\n",
    "print(f\"\\nSource (Movies):      {source_f1:.4f}          N/A             N/A\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "fa4e26da49ab490391057153659fe544",
      "199c9d5da4bf47baba405bc6555d9aba",
      "8a74df22f6e641f4812f2cc0f219039d",
      "0b2925cbaf3443758b52afbe41abd2f1",
      "aeda2644bfdd451198ea472fe8e7cf78",
      "86237397fec642049ff500bfa6cba51c",
      "ab5f53b292cd45f792ba5a717fa4720a",
      "318a40cb79bd49cc8c0a4b99cd9fe81b",
      "1582e826db4d4eecb28283bdbb088806",
      "cc5281d351a74d1db6f7a1c6845d61df",
      "593f99ba62cf4a2c980a706f21f6c4cf"
     ]
    },
    "id": "NWvx0lRMHmAv",
    "outputId": "434846c2-b49b-4a10-bb3e-d9d71c3b6761"
   },
   "execution_count": 38,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "================================================================================\n",
      "CROSS-DOMAIN TRANSFER LEARNING EXPERIMENT\n",
      "================================================================================\n",
      "\n",
      "Training classifier on MOVIE REVIEWS (source domain)...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/63 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fa4e26da49ab490391057153659fe544"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "BASELINE: Performance on Source Domain (Movies)\n",
      "--------------------------------------------------------------------------------\n",
      "Source Domain F1: 0.8497\n",
      "This is how well the classifier does on its training domain\n",
      "\n",
      "================================================================================\n",
      "ZERO-SHOT TRANSFER TO TARGET DOMAINS\n",
      "================================================================================\n",
      "\n",
      "Restaurant Reviews:\n",
      "------------------------------------------------------------\n",
      "F1 Score: 0.9010\n",
      "Accuracy: 0.9000\n",
      "Performance Drop: -0.0513 (-6.0%)\n",
      "\n",
      "Example predictions:\n",
      "  'Amazing food and excellent service!...'\n",
      "  True: Positive | Pred: Positive \u2713\n",
      "  'Best restaurant in town, highly recommend...'\n",
      "  True: Positive | Pred: Positive \u2713\n",
      "  'Delicious meals and great atmosphere...'\n",
      "  True: Positive | Pred: Positive \u2713\n",
      "\n",
      "Product Reviews:\n",
      "------------------------------------------------------------\n",
      "F1 Score: 1.0000\n",
      "Accuracy: 1.0000\n",
      "Performance Drop: -0.1503 (-17.7%)\n",
      "\n",
      "Example predictions:\n",
      "  'This product is amazing! Works perfectly...'\n",
      "  True: Positive | Pred: Positive \u2713\n",
      "  'Excellent quality, very satisfied...'\n",
      "  True: Positive | Pred: Positive \u2713\n",
      "  'Great value for money, highly recommend...'\n",
      "  True: Positive | Pred: Positive \u2713\n",
      "\n",
      "Book Reviews:\n",
      "------------------------------------------------------------\n",
      "F1 Score: 0.9010\n",
      "Accuracy: 0.9000\n",
      "Performance Drop: -0.0513 (-6.0%)\n",
      "\n",
      "Example predictions:\n",
      "  'Brilliant book! Couldn't put it down...'\n",
      "  True: Positive | Pred: Positive \u2713\n",
      "  'Masterfully written, highly engaging...'\n",
      "  True: Positive | Pred: Positive \u2713\n",
      "  'One of the best books I've read...'\n",
      "  True: Positive | Pred: Positive \u2713\n",
      "\n",
      "================================================================================\n",
      "FEW-SHOT DOMAIN ADAPTATION\n",
      "================================================================================\n",
      "Strategy: Add first 4 examples from each target domain to training set\n",
      "\n",
      "Restaurant Reviews:\n",
      "------------------------------------------------------------\n",
      "Zero-shot F1:  0.9010\n",
      "Adapted F1:    0.9091\n",
      "Improvement:   +0.0081\n",
      "\u2192 Slight improvement from adaptation\n",
      "\n",
      "Product Reviews:\n",
      "------------------------------------------------------------\n",
      "Zero-shot F1:  1.0000\n",
      "Adapted F1:    1.0000\n",
      "Improvement:   +0.0000\n",
      "\u2192 No improvement or slight degradation\n",
      "\n",
      "Book Reviews:\n",
      "------------------------------------------------------------\n",
      "Zero-shot F1:  0.9010\n",
      "Adapted F1:    0.9091\n",
      "Improvement:   +0.0081\n",
      "\u2192 Slight improvement from adaptation\n",
      "\n",
      "================================================================================\n",
      "DOMAIN SIMILARITY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Domain distances from source (movie reviews):\n",
      "------------------------------------------------------------\n",
      "\n",
      "Restaurant Reviews:\n",
      "  Embedding distance: 0.7160\n",
      "  Performance drop:   -0.0513\n",
      "  Zero-shot F1:       0.9010\n",
      "  Adaptation gain:    +0.0081\n",
      "\n",
      "Product Reviews:\n",
      "  Embedding distance: 0.6779\n",
      "  Performance drop:   -0.1503\n",
      "  Zero-shot F1:       1.0000\n",
      "  Adaptation gain:    +0.0000\n",
      "\n",
      "Book Reviews:\n",
      "  Embedding distance: 0.5339\n",
      "  Performance drop:   -0.0513\n",
      "  Zero-shot F1:       0.9010\n",
      "  Adaptation gain:    +0.0081\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Correlation: Distance vs Performance\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Ranked by distance to source:\n",
      "  Book Reviews        : distance=0.534, drop=-0.051\n",
      "  Product Reviews     : distance=0.678, drop=-0.150\n",
      "  Restaurant Reviews  : distance=0.716, drop=-0.051\n",
      "\n",
      "Observation:\n",
      "  \u2192 Domains closer to movies in embedding space tend to transfer better\n",
      "  \u2192 Larger embedding distance correlates with larger performance drop\n",
      "\n",
      "================================================================================\n",
      "TRANSFER LEARNING SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Domain               Zero-Shot F1    Adapted F1      Improvement \n",
      "-----------------------------------------------------------------\n",
      "Restaurant Reviews   0.9010          0.9091          +0.0081     \n",
      "Product Reviews      1.0000          1.0000          +0.0000     \n",
      "Book Reviews         0.9010          0.9091          +0.0081     \n",
      "\n",
      "Source (Movies):      0.8497          N/A             N/A\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reflection Questions:**\n",
    "\n",
    "1. Which domain transferred best from movies? Which worst? How domain-specific is the classifier's knowledge?\n",
    "\n",
    "2. Do you see patterns in what transfers well vs what fails? Does the classifier understand \"delicious\" (restaurant) as easily as \"entertaining\" (movie)?\n",
    "\n",
    "3. After few-shot adaptation: Which domains benefited most from adding just 4 examples? Why might some domains need more adaptation than others?"
   ],
   "metadata": {
    "id": "h6ezrMLSINqZ"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "yZLqm3S4IP2e"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}