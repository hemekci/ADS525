{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Guide: API Configuration for ADS 525\n",
    "\n",
    "This notebook covers account creation and API setup for OpenAI and HuggingFace. Complete these steps before working on course notebooks.\n",
    "\n",
    "---\n",
    "\n",
    "## Which Environment Are You Using?\n",
    "\n",
    "Choose the authentication method based on your environment:\n",
    "\n",
    "| Environment | Best Method | Why |\n",
    "|-------------|-------------|-----|\n",
    "| **Google Colab** | Option C (Colab Secrets) | Persistent across sessions, most secure for Colab |\n",
    "| **Local Jupyter/JupyterLab** | Option B (Environment Variable) | Works across notebooks, doesn't expose keys in code |\n",
    "| **VSCode/PyCharm locally** | Option B (Environment Variable) | Standard practice for local development |\n",
    "| **Quick testing only** | Option A (Direct in code) | Fast but insecure, never commit to git |\n",
    "\n",
    "**Note:** If using Option A, never commit notebooks with API keys to GitHub. Always use .gitignore or secrets management.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. OpenAI API Setup\n",
    "\n",
    "OpenAI provides access to GPT models (GPT-3.5, GPT-4) through their API. You will need this for text generation and classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create an OpenAI Account\n",
    "\n",
    "1. Go to https://platform.openai.com/signup\n",
    "2. Sign up with email or Google/Microsoft account\n",
    "3. Verify your email address\n",
    "4. Add payment method (required for API access)\n",
    "\n",
    "**Note:** OpenAI provides $5 in free credits for new accounts. After that, you pay per token used.\n",
    "\n",
    "**Pricing:** https://openai.com/pricing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Generate API Key\n",
    "\n",
    "1. Log in to https://platform.openai.com\n",
    "2. Click your profile icon (top-right) > **API keys**\n",
    "3. Click **Create new secret key**\n",
    "4. Name your key (e.g., \"ADS525\")\n",
    "5. Copy the key immediately (you cannot view it again)\n",
    "6. Store it securely\n",
    "\n",
    "**Direct link:** https://platform.openai.com/api-keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Install OpenAI Library\n",
    "\n",
    "Run this cell to install the OpenAI Python library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Configure API Key\n",
    "\n",
    "**Choose the option that matches your environment (see table above).**\n",
    "\n",
    "**Option A: Direct in Code (Testing only - any environment)**\n",
    "\n",
    "Replace `YOUR_API_KEY_HERE` with your actual key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "client = openai.OpenAI(api_key=\"YOUR_API_KEY_HERE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option B: Environment Variable (Local Jupyter, VSCode, PyCharm)**\n",
    "\n",
    "Set your API key as an environment variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "# Set the API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY_HERE\"\n",
    "\n",
    "# Client will automatically use the environment variable\n",
    "client = openai.OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option C: Google Colab Secrets (Google Colab only)**\n",
    "\n",
    "1. In Colab, click the key icon (ðŸ”‘) in the left sidebar\n",
    "2. Click **Add new secret**\n",
    "3. Name: `OPENAI_API_KEY`\n",
    "4. Value: Your API key\n",
    "5. Toggle on notebook access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colab\n",
    "from google.colab import userdata\n",
    "import openai\n",
    "\n",
    "client = openai.OpenAI(api_key=userdata.get('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Test Your Setup\n",
    "\n",
    "Run this cell to verify your API key works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Say 'API setup successful'\"}],\n",
    "        max_tokens=10\n",
    "    )\n",
    "    print(\"Success:\", response.choices[0].message.content)\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Documentation\n",
    "\n",
    "- **API Reference:** https://platform.openai.com/docs/api-reference\n",
    "- **Quickstart Guide:** https://platform.openai.com/docs/quickstart\n",
    "- **Python Library:** https://github.com/openai/openai-python\n",
    "- **Usage Dashboard:** https://platform.openai.com/usage\n",
    "- **Rate Limits:** https://platform.openai.com/docs/guides/rate-limits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. HuggingFace Setup\n",
    "\n",
    "HuggingFace hosts thousands of pre-trained models and datasets. Most models are free to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create a HuggingFace Account\n",
    "\n",
    "1. Go to https://huggingface.co/join\n",
    "2. Sign up with email or Google/GitHub account\n",
    "3. Verify your email address\n",
    "\n",
    "**Note:** Account creation is free. No payment method required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Generate Access Token\n",
    "\n",
    "An access token allows you to:\n",
    "- Download private models\n",
    "- Access gated models (like Llama)\n",
    "- Upload models/datasets\n",
    "- Avoid rate limits\n",
    "\n",
    "**Steps:**\n",
    "1. Log in to https://huggingface.co\n",
    "2. Click your profile picture > **Settings**\n",
    "3. Go to **Access Tokens** in the left sidebar\n",
    "4. Click **New token**\n",
    "5. Name: \"ADS525\"\n",
    "6. Role: **Read** (sufficient for this course)\n",
    "7. Click **Generate**\n",
    "8. Copy the token\n",
    "\n",
    "**Direct link:** https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Install HuggingFace Libraries\n",
    "\n",
    "Run this cell to install required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets sentence-transformers huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Authenticate with HuggingFace\n",
    "\n",
    "**Choose the option that matches your environment (see table above).**\n",
    "\n",
    "**Option A: Login via Terminal (Local environments - one-time setup)**\n",
    "\n",
    "Run this once to store credentials permanently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# This will prompt for your token\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option B: Environment Variable (Local Jupyter, VSCode, PyCharm)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = \"YOUR_HF_TOKEN_HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option C: Google Colab Secrets (Google Colab only)**\n",
    "\n",
    "1. In Colab, click the key icon (ðŸ”‘)\n",
    "2. Click **Add new secret**\n",
    "3. Name: `HF_TOKEN`\n",
    "4. Value: Your token\n",
    "5. Toggle on notebook access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colab\n",
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=userdata.get('HF_TOKEN'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Test Your Setup\n",
    "\n",
    "Run this cell to verify access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "try:\n",
    "    # Load a small model\n",
    "    classifier = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "    result = classifier(\"HuggingFace setup successful\")\n",
    "    print(\"Success:\", result)\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFace Documentation\n",
    "\n",
    "- **Models Hub:** https://huggingface.co/models\n",
    "- **Datasets Hub:** https://huggingface.co/datasets\n",
    "- **Transformers Library:** https://huggingface.co/docs/transformers\n",
    "- **Datasets Library:** https://huggingface.co/docs/datasets\n",
    "- **Sentence Transformers:** https://www.sbert.net/\n",
    "- **Hub API:** https://huggingface.co/docs/huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Google Colab Specific Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable GPU\n",
    "\n",
    "Most course notebooks require a GPU for reasonable runtime.\n",
    "\n",
    "**Steps:**\n",
    "1. Go to **Runtime** > **Change runtime type**\n",
    "2. **Hardware accelerator:** Select **GPU**\n",
    "3. **GPU type:** Select **T4** (free tier)\n",
    "4. Click **Save**\n",
    "\n",
    "Verify GPU is available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"No GPU available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persistent Storage (Optional)\n",
    "\n",
    "Mount Google Drive to save models and data between sessions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "print(\"Drive mounted at /content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Troubleshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Errors\n",
    "\n",
    "**Error:** `AuthenticationError: Incorrect API key`\n",
    "- Check your API key is correct (no extra spaces)\n",
    "- Verify key has not been revoked at https://platform.openai.com/api-keys\n",
    "\n",
    "**Error:** `RateLimitError: Rate limit exceeded`\n",
    "- You are making too many requests\n",
    "- Add delays between API calls: `import time; time.sleep(1)`\n",
    "- Check rate limits: https://platform.openai.com/account/rate-limits\n",
    "\n",
    "**Error:** `InsufficientQuotaError: You exceeded your current quota`\n",
    "- Your free credits are exhausted\n",
    "- Add payment method at https://platform.openai.com/account/billing\n",
    "\n",
    "**Error:** `InvalidRequestError: max_tokens is too large`\n",
    "- Reduce `max_tokens` parameter\n",
    "- Check model limits: https://platform.openai.com/docs/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFace Errors\n",
    "\n",
    "**Error:** `OSError: [Errno 403] Forbidden`\n",
    "- Model requires authentication\n",
    "- Run `huggingface-cli login` or use token\n",
    "\n",
    "**Error:** `OSError: [Errno 404] Model not found`\n",
    "- Check model name spelling\n",
    "- Verify model exists: https://huggingface.co/models\n",
    "\n",
    "**Error:** `OutOfMemoryError: CUDA out of memory`\n",
    "- Model is too large for available GPU memory\n",
    "- Use a smaller model\n",
    "- Reduce batch size\n",
    "- Use `device=\"cpu\"` instead of GPU\n",
    "\n",
    "**Error:** `RepositoryNotFoundError`\n",
    "- Check repository name\n",
    "- Model may be private (requires authentication)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Python Errors\n",
    "\n",
    "**Error:** `ModuleNotFoundError: No module named 'X'`\n",
    "- Run `!pip install X`\n",
    "- Restart runtime after installation\n",
    "\n",
    "**Error:** `RuntimeError: CUDA error: device-side assert triggered`\n",
    "- Usually an indexing error in model code\n",
    "- Try running on CPU to get better error message: `device=\"cpu\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Cost Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Cost Tips\n",
    "\n",
    "- Use `gpt-3.5-turbo` instead of `gpt-4` (20x cheaper)\n",
    "- Set `max_tokens` to limit response length\n",
    "- Cache results to avoid repeated API calls\n",
    "- Monitor usage: https://platform.openai.com/usage\n",
    "- Set usage limits: https://platform.openai.com/account/limits\n",
    "\n",
    "**Example pricing (as of 2024):**\n",
    "- GPT-3.5-turbo: $0.0015 per 1K tokens\n",
    "- GPT-4: $0.03 per 1K tokens (input)\n",
    "\n",
    "**Estimate tokens:** ~750 words = 1000 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFace (Free)\n",
    "\n",
    "- All public models and datasets are free\n",
    "- No API costs\n",
    "- Only costs are compute (your GPU/CPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Additional Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Libraries\n",
    "\n",
    "- **transformers:** Pre-trained models (BERT, GPT, etc.)\n",
    "- **datasets:** Access to 10,000+ datasets\n",
    "- **sentence-transformers:** Sentence embeddings\n",
    "- **openai:** OpenAI API client\n",
    "- **langchain:** Framework for LLM applications (>=0.1.17)\n",
    "- **langchain-core:** Core LangChain components (prompts, messages)\n",
    "- **langchain-classic:** Legacy LangChain components (chains, memory)\n",
    "- **langchain-community:** Community integrations (tools, LLMs)\n",
    "- **langchain-openai:** OpenAI integration for LangChain\n",
    "- **tiktoken:** OpenAI tokenizer\n",
    "\n",
    "Install all at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets sentence-transformers openai langchain>=0.1.17 langchain-core langchain-classic langchain-community langchain-openai tiktoken accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. LangChain Migration Guide (Important!)\n",
    "\n",
    "### Understanding LangChain 1.x Changes\n",
    "\n",
    "LangChain has restructured its packages in version 1.x. The old imports no longer work, and you must use the new modular structure.\n",
    "\n",
    "**What changed:**\n",
    "- LangChain split into multiple packages: `langchain-core`, `langchain-classic`, `langchain-community`\n",
    "- Old imports like `from langchain import PromptTemplate` no longer work\n",
    "- Legacy components are now in `langchain_classic` package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct Imports for Common Components\n",
    "\n",
    "**Prompts (use langchain_core):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  CORRECT (LangChain 1.x)\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "#   INCORRECT (deprecated, will fail)\n",
    "# from langchain import PromptTemplate\n",
    "# from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Example usage\n",
    "template = \"Tell me about {topic}\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"topic\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chains and Memory (use langchain_classic):**\n",
    "\n",
    "These are legacy components being phased out, but still work via `langchain_classic`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  CORRECT (LangChain 1.x)\n",
    "from langchain_classic.chains import LLMChain\n",
    "from langchain_classic.memory import (\n",
    "    ConversationBufferMemory,\n",
    "    ConversationBufferWindowMemory,\n",
    "    ConversationSummaryMemory\n",
    ")\n",
    "\n",
    "#   INCORRECT (deprecated, will fail)\n",
    "# from langchain import LLMChain\n",
    "# from langchain.chains import LLMChain\n",
    "# from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Example usage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "chain = LLMChain(llm=llm, prompt=prompt, memory=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Agents (use langchain_classic):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  CORRECT (LangChain 1.x)\n",
    "from langchain_classic.agents import (\n",
    "    load_tools,\n",
    "    Tool,\n",
    "    AgentExecutor,\n",
    "    create_react_agent\n",
    ")\n",
    "\n",
    "#   INCORRECT (deprecated, will fail)\n",
    "# from langchain.agents import load_tools, Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tools and Community LLMs (use langchain_community):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  CORRECT (LangChain 1.x)\n",
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "#   INCORRECT (deprecated, will fail)\n",
    "# from langchain.tools import DuckDuckGoSearchResults\n",
    "# from langchain.llms import LlamaCpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OpenAI Integration (use langchain_openai):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  CORRECT (LangChain 1.x)\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "#  INCORRECT (deprecated, will fail)\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Example usage\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Reference: Import Migration\n",
    "\n",
    "| Component | Old Import (Deprecated) | New Import (LangChain 1.x) |\n",
    "|-----------|------------------------|---------------------------|\n",
    "| PromptTemplate | `from langchain import PromptTemplate` | `from langchain_core.prompts import PromptTemplate` |\n",
    "| LLMChain | `from langchain import LLMChain` | `from langchain_classic.chains import LLMChain` |\n",
    "| ConversationBufferMemory | `from langchain.memory import ConversationBufferMemory` | `from langchain_classic.memory import ConversationBufferMemory` |\n",
    "| Agents | `from langchain.agents import create_react_agent` | `from langchain_classic.agents import create_react_agent` |\n",
    "| ChatOpenAI | `from langchain.chat_models import ChatOpenAI` | `from langchain_openai import ChatOpenAI` |\n",
    "| Tools | `from langchain.tools import DuckDuckGoSearchResults` | `from langchain_community.tools import DuckDuckGoSearchResults` |\n",
    "| LlamaCpp | `from langchain.llms import LlamaCpp` | `from langchain_community.llms import LlamaCpp` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Working Example (Copy-Paste Ready)\n",
    "\n",
    "Here's a complete example using the correct imports for a simple chat with memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete working example with LangChain 1.x\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_classic.chains import LLMChain\n",
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "\n",
    "# Setup\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "\n",
    "# Create prompt with chat history placeholder\n",
    "template = \"\"\"Current conversation:\n",
    "{chat_history}\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"input\", \"chat_history\"]\n",
    ")\n",
    "\n",
    "# Create memory and chain\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "chain = LLMChain(llm=llm, prompt=prompt, memory=memory)\n",
    "\n",
    "# Use it\n",
    "response1 = chain.invoke({\"input\": \"Hi! My name is Alice.\"})\n",
    "print(response1)\n",
    "\n",
    "response2 = chain.invoke({\"input\": \"What's my name?\"})\n",
    "print(response2)  # Should remember \"Alice\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why These Changes Matter\n",
    "\n",
    "1. **Modularity:** New structure allows installing only what you need\n",
    "2. **Clarity:** Clear separation between core, legacy, and community components\n",
    "3. **Migration Path:** `langchain_classic` provides compatibility while moving to modern alternatives\n",
    "4. **Future-Proofing:** LangChain is moving toward LangGraph (see below)\n",
    "\n",
    "**Note:** The course notebooks use the legacy components (`LLMChain`, `ConversationBufferMemory`) because they're simpler for learning. In production, consider using LangGraph or LCEL (LangChain Expression Language)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Future: LangGraph (Optional Reading)\n",
    "\n",
    "LangChain is migrating users from `LLMChain` + `Memory` to **LangGraph**, which offers:\n",
    "- Better state management\n",
    "- Time-travel debugging\n",
    "- Interrupts and human-in-the-loop\n",
    "- More complex agent workflows\n",
    "\n",
    "**When to use each approach:**\n",
    "\n",
    "| Use Case | Recommended Approach |\n",
    "|----------|---------------------|\n",
    "| **Learning LLM basics** | `LLMChain` + `ConversationBufferMemory` (this course) |\n",
    "| **Simple chat applications** | LCEL with `RunnableWithMessageHistory` |\n",
    "| **Complex agent workflows** | LangGraph |\n",
    "| **Production applications** | LangGraph (more control and debugging) |\n",
    "\n",
    "**Resources:**\n",
    "- LangGraph Docs: https://langchain-ai.github.io/langgraph/\n",
    "- Migration Guide: https://python.langchain.com/docs/versions/migrating_memory/\n",
    "- LCEL Guide: https://python.langchain.com/docs/expression_language/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Community Resources\n",
    "\n",
    "- **HuggingFace Forums:** https://discuss.huggingface.co/\n",
    "- **OpenAI Community:** https://community.openai.com/\n",
    "- **Stack Overflow:** https://stackoverflow.com/questions/tagged/transformers\n",
    "- **LangChain Docs:** https://python.langchain.com/\n",
    "- **Course GitHub:** https://github.com/hemekci/ADS525"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup Complete\n",
    "\n",
    "If both test cells ran successfully, you are ready to start the course notebooks. \n",
    "\n",
    "**Important:** Review Section 7 (LangChain Migration Guide) before working on Chapter 7 notebooks to understand the correct import syntax.\n",
    "\n",
    "Refer back to this guide if you encounter authentication or API issues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
