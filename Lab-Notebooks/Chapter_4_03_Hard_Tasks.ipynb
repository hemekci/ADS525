{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iF8lDgqJnzyT"
   },
   "source": [
    "# Chapter 4: Text Classification - Hard Tasks\n\nThis notebook tackles advanced classification challenges. You'll implement hierarchical multi-level classifiers for complex taxonomies, use active learning to minimize labeling costs, build ensemble classifiers to improve robustness, and apply transfer learning across domains. These techniques are essential for production-level NLP systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFpMVC20nzyW"
   },
   "source": [
    "## Setup\n\nRun all cells in this section to set up the environment and load necessary data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LGW2SD-c864"
   },
   "source": [
    "### [Optional] - Installing Packages on <img src=\"https://colab.google/static/images/icons/colab.png\" width=100>\n\nIf you are viewing this notebook on Google Colab (or any other cloud vendor), you need to **uncomment and run** the following codeblock to install the dependencies for this chapter:\n\n **Note**: We will want to use a GPU to run the examples in this notebook. In Google Colab, go to\n**Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4**.\n\n---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N-PxmOIhc865"
   },
   "outputs": [],
   "source": " %%capture\n!pip install transformers sentence-transformers openai\n!pip install -U datasets"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJorXxPFnzyZ"
   },
   "source": [
    "### Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5phRS_z2U_3T",
    "outputId": "59cda1da-3535-4be9-ddca-57b95f391188"
   },
   "outputs": [],
   "source": "from datasets import load_dataset\n# Load our data\ndata = load_dataset(\"rotten_tomatoes\")\ndata"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GS7ZhZfGnzyc"
   },
   "source": [
    "### Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X0KyKHtqyjn3"
   },
   "outputs": [],
   "source": "from sklearn.metrics import classification_report\ndef evaluate_performance(y_true, y_pred):\n    \"\"\"Create and print the classification report\"\"\"\n    performance = classification_report(\n        y_true, y_pred,\n        target_names=[\"Negative Review\", \"Positive Review\"]\n    )\n    print(performance)"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Your Turn - Text Classification Experiments\n\nRun each task first to see the baseline results. Follow the instructions to modify and experiment."
   ],
   "metadata": {
    "id": "NKYNfoaVC4hU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This section is divided into EASY, MEDIUM, & HARD."
   ],
   "metadata": {
    "id": "hHVONn85DElL"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpiSzld6nzyh"
   },
   "source": [
    "## Hard Tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hard Tasks - Advanced Classification Challenges\n\nThese tasks require significant modifications and deeper understanding. Take your time and experiment"
   ],
   "metadata": {
    "id": "sOcbrBwdGZbi"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F649-TFqnzyi"
   },
   "source": [
    "#### Hard Task 1: Hierarchical Multi-Level Classifier\nInstead of flat classification (choosing from all categories at once), hierarchical classification makes decisions in steps: first broad categories, then fine-grained ones. This mirrors how humans often reason.\nInstructions:\n1. Run the 2-level classifier (Sentiment \u2192 Specific Aspect)\n2. Compare with flat classification to see confidence differences\n3. Try adding a 3rd level by uncommenting the code\n4. Analyze whether breaking decisions into steps helps or hurts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rYMpBGqonzyj"
   },
   "outputs": [],
   "source": "# Level 1: Broad sentiment\nlevel1_labels = [\n    \"negative sentiment review\",\n    \"positive sentiment review\"\n]\n# Level 2: Specific aspects (conditional on Level 1)\nlevel2_negative = [\n    \"review criticizing entertainment value and pacing\",\n    \"review criticizing technical quality and production\"\n]\nlevel2_positive = [\n    \"review praising technical quality and artistry\",\n    \"review praising entertainment value and enjoyment\"\n]\n# Add Level 3 for even finer granularity\n# level3_positive_quality = [...]\n# level3_positive_entertainment = [...]"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DFgHMLAqnzyk"
   },
   "source": "from sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\nmodel = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n\ntest_reviews = [\n    \"Amazing cinematography and brilliant direction!\",\n    \"Terrible pacing, very boring throughout\",\n    \"Excellent acting but weak storyline\",\n    \"Poor production quality, disappointing visuals\"\n]",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PbnbmKYOnzyk"
   },
   "source": "def hierarchical_classify_2level(review):\n    \"\"\"\n    Classify review using 2-level hierarchy:\n    Level 1: Sentiment (positive/negative)\n    Level 2: Specific aspect (quality/entertainment)\n    \"\"\"\n    # Level 1: Determine sentiment\n    level1_emb = model.encode(level1_labels)\n    review_emb = model.encode([review])\n    level1_sim = cosine_similarity(review_emb, level1_emb)[0]\n\n    level1_pred = np.argmax(level1_sim)\n    level1_conf = level1_sim[level1_pred]\n    level1_label = level1_labels[level1_pred]\n\n    # Level 2: Conditional on Level 1\n    if level1_pred == 0:  # Negative\n        level2_labels = level2_negative\n        path = \"Negative \u2192 \"\n    else:  # Positive\n        level2_labels = level2_positive\n        path = \"Positive \u2192 \"\n\n    level2_emb = model.encode(level2_labels)\n    level2_sim = cosine_similarity(review_emb, level2_emb)[0]\n\n    level2_pred = np.argmax(level2_sim)\n    level2_conf = level2_sim[level2_pred]\n    level2_label = level2_labels[level2_pred]\n    path += level2_label\n\n    return {\n        \"level1_label\": level1_label,\n        \"level1_conf\": level1_conf,\n        \"level2_label\": level2_label,\n        \"level2_conf\": level2_conf,\n        \"path\": path\n    }",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bm1JW3gBnzym"
   },
   "source": [
    "Classify the reviews:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Hg7sMmhnzym",
    "outputId": "70765e3a-d2fd-4ded-fd44-6e0fc4a82db6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Hierarchical Classification (2 LEVELS)\")\nfor i, review in enumerate(test_reviews):\n    result = hierarchical_classify_2level(review)\n    print(f\"\\nReview {i+1}: '{review}'\")\n    print(f\"\\n  Level 1 (Sentiment):\")\n    print(f\"     {result['level1_label']}\")\n    print(f\"     Confidence: {result['level1_conf']:.3f}\")\n    print(f\"\\n  Level 2 (Specific Aspect):\")\n    print(f\"     {result['level2_label']}\")\n    print(f\"     Confidence: {result['level2_conf']:.3f}\")\n    print(f\"\\n  Final Classification Path:\")\n    print(f\"     {result['path']}\")\n# Compare with flat classification\nprint()\nprint(\"Comparison: Hierarchical vs Flat Classification\")\n# Flat: All 4 categories at once\nflat_labels = [\n    \"review criticizing entertainment value and pacing\",      # 0\n    \"review criticizing technical quality and production\",    # 1\n    \"review praising technical quality and artistry\",         # 2\n    \"review praising entertainment value and enjoyment\"       # 3\n]\nflat_embeddings = model.encode(flat_labels)\nreview_embeddings = model.encode(test_reviews)\nflat_sim = cosine_similarity(review_embeddings, flat_embeddings)\nprint(\"\\nShowing first 3 reviews:\")\nfor i in range(min(3, len(test_reviews))):\n    hier_result = hierarchical_classify_2level(test_reviews[i])\n    flat_pred = np.argmax(flat_sim[i])\n    flat_conf = flat_sim[i][flat_pred]\n    print(f\"\\nReview: '{test_reviews[i][:50]}...'\")\n    print(f\"  Hierarchical: {hier_result['level2_label']}\")\n    print(f\"     Confidence: {hier_result['level2_conf']:.3f}\")\n    print(f\"  Flat:         {flat_labels[flat_pred]}\")\n    print(f\"     Confidence: {flat_conf:.3f}\")\n    print(f\"  Confidence Diff: {hier_result['level2_conf'] - flat_conf:+.3f}\")\n# After implementing 3-level, uncomment to test it:\n# print()\n# print(\"Testing 3-LEVEL Hierarchical Classification\")\n# for i, review in enumerate(test_reviews):\n#     result = hierarchical_classify_3level(review)\n#     print(f\"\\n{i+1}. '{review[:60]}...'\")\n#     print(f\"   Path: {result['path']}\")\n#     print(f\"   Level 3 confidence: {result['level3_conf']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The hierarchical approach makes decisions in stages:\n1. Level 1: Determines if the review is positive or negative\n2. Level 2: Based on that sentiment, classifies the specific aspect\n\nNotice the confidence scores at each level. The hierarchical classifier's final confidence is often higher than flat classification because it's making simpler decisions at each step.\n\nLooking at the comparison between hierarchical and flat approaches:\n- Hierarchical often has higher confidence (makes easier per-step decisions)\n- But if Level 1 is wrong, Level 2 has no chance to correct it\n- Flat classification considers all options at once but may be less confident"
   ],
   "metadata": {
    "id": "DOgvT7Ffnzyn"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wea-7Goznzyo"
   },
   "source": [
    "#### Hard Task 2: Active Learning to Minimize Labeling Costs\nLabeling data is expensive. Active learning strategically selects the most informative samples to label, potentially saving 50%+ of labeling effort compared to random selection. Run the simulation to see active learning vs random sampling, observe which samples the model finds \"uncertain\", and track how many samples each approach needs to reach F1=0.85. Try implementing the alternative selection strategy (margin sampling) and compare labeling cost savings."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CHKseH6znzyo",
    "outputId": "19d5c2f9-05f5-4a35-96a2-9d5c4772dc8f",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "from sentence_transformers import SentenceTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score, accuracy_score\nimport numpy as np\n\n# Simulate active learning vs random sampling\nprint(\"Active Learning Simulation\")\nprint(\"=\"*60)\n\n# Setup\nmodel = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\ntrain_data = data[\"train\"].shuffle(seed=42).select(range(1000))\ntest_data = data[\"test\"].shuffle(seed=42).select(range(200))\n\n# Start with small labeled set\nlabeled_size = 50\ntarget_f1 = 0.85\n\nprint(f\"Starting with {labeled_size} labeled samples\")\nprint(f\"Target F1: {target_f1}\")\nprint(f\"\\nNote: This is a simplified simulation\")\nprint(\"In real active learning, you'd query an oracle (human) for labels\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6gNoUfj4tQUU",
    "outputId": "82863c05-19f1-4f52-cde6-d05524c167ed",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": "# Encode all training and test data upfront\nall_train_embeddings = model.encode(train_data[\"text\"], show_progress_bar=False)\nall_test_embeddings = model.encode(test_data[\"text\"], show_progress_bar=False)\n\nprint(f\"Encoded {len(train_data)} training samples\")\nprint(f\"Encoded {len(test_data)} test samples\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4sI4kQktQUV"
   },
   "source": [
    "Start with a small labeled set and keep rest unlabeled:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3c-AdbT1tQUW",
    "outputId": "677a79ff-31fc-49b4-c4c7-b77b5bc85628",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": "labeled_indices = list(range(labeled_size))  # First 50 samples\nunlabeled_indices = list(range(labeled_size, len(train_data)))  # Rest are unlabeled\n\nprint(f\"Starting labeled set: {len(labeled_indices)} samples\")\nprint(f\"Unlabeled pool: {len(unlabeled_indices)} samples\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVeK6HAttQUX"
   },
   "source": [
    "Run active learning iterations:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AEzaSddptQUX",
    "outputId": "b3f1c66e-ae3a-4d67-f87c-b051e5855326",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "print(\"\\nIteration | Labeled Size | F1 Score\")\nprint(\"-\" * 40)\n\nfor iteration in range(5):\n    # Train on current labeled data\n    X_train = all_train_embeddings[labeled_indices]\n    y_train = [train_data[\"label\"][i] for i in labeled_indices]\n\n    clf = LogisticRegression(random_state=42, max_iter=1000)\n    clf.fit(X_train, y_train)\n\n    # Evaluate on test set\n    y_pred = clf.predict(all_test_embeddings)\n    current_f1 = f1_score(test_data[\"label\"], y_pred, average='weighted')\n\n    print(f\"{iteration:9d} | {len(labeled_indices):12d} | {current_f1:.4f}\")\n\n    if current_f1 >= target_f1:\n        print(f\"\\nReached target F1={target_f1}!\")\n        break\n\n    # Complete this: Select most uncertain samples from unlabeled pool\n    if len(unlabeled_indices) > 0:\n        unlabeled_embeddings = all_train_embeddings[unlabeled_indices]\n        probs = clf.predict_proba(unlabeled_embeddings)\n\n        # Calculate uncertainty for each unlabeled sample\n        # Hint: Uncertainty = 1 - max(probability)\n        uncertainties = None  # calculate this\n\n        # Select 20 most uncertain samples\n        # Hint: Use argsort to find indices of highest uncertainties\n        n_select = min(20, len(unlabeled_indices))\n        most_uncertain = None  # find n_select most uncertain indices\n        selected_indices = [unlabeled_indices[idx] for idx in most_uncertain] if most_uncertain is not None else []\n\n        # Move to labeled set\n        labeled_indices.extend(selected_indices)\n        unlabeled_indices = [idx for idx in unlabeled_indices if idx not in selected_indices]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Active learning iteratively:\n1. Trains on currently labeled data\n2. Finds the most uncertain unlabeled samples\n3. \"Labels\" those samples (adds them to training set)\n4. Repeats\n\nThe uncertain samples being selected typically have prediction probabilities close to 50-50. These are the most informative because they lie near the decision boundary.\n\nCompare the learning curves to see how quickly each approach improves. Active learning often reaches target performance with fewer labeled samples, saving significant labeling costs."
   ],
   "metadata": {
    "id": "yzYVwXTBHOrA"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment: Try margin sampling as an alternative selection strategy (selects samples where top 2 predictions are close):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Experiment: Alternative selection strategy - Margin Sampling\n# Uncomment to try margin sampling instead of uncertainty sampling:\n#\n# # Reset for fair comparison\n# labeled_indices = list(range(labeled_size))\n# unlabeled_indices = list(range(labeled_size, len(train_data)))\n#\n# print(\"\\nMargin Sampling Strategy:\")\n# print(\"-\" * 40)\n#\n# for iteration in range(5):\n#     X_train = all_train_embeddings[labeled_indices]\n#     y_train = [train_data[\"label\"][i] for i in labeled_indices]\n#     \n#     clf = LogisticRegression(random_state=42, max_iter=1000)\n#     clf.fit(X_train, y_train)\n#     \n#     y_pred = clf.predict(all_test_embeddings)\n#     current_f1 = f1_score(test_data[\"label\"], y_pred, average='weighted')\n#     \n#     print(f\"{iteration:9d} | {len(labeled_indices):12d} | {current_f1:.4f}\")\n#     \n#     if current_f1 >= target_f1:\n#         print(f\"\\nReached target F1={target_f1}!\")\n#         break\n#     \n#     if len(unlabeled_indices) > 0:\n#         unlabeled_embeddings = all_train_embeddings[unlabeled_indices]\n#         probs = clf.predict_proba(unlabeled_embeddings)\n#         \n#         # Margin = difference between top 2 probabilities\n#         sorted_probs = np.sort(probs, axis=1)\n#         margins = sorted_probs[:, -1] - sorted_probs[:, -2]  # Smaller margin = more uncertain\n#         \n#         n_select = min(20, len(unlabeled_indices))\n#         smallest_margins = np.argsort(margins)[:n_select]  # Get smallest margins\n#         selected_indices = [unlabeled_indices[idx] for idx in smallest_margins]\n#         \n#         labeled_indices.extend(selected_indices)\n#         unlabeled_indices = [idx for idx in unlabeled_indices if idx not in selected_indices]\n#\n# print(\"\\nCompare: Did margin sampling reach target faster or slower than uncertainty sampling?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R98vphjNnzyp"
   },
   "source": [
    "#### Hard Task 3: Ensemble Classifier for Improved Robustness\n\nEnsemble methods combine multiple models to reduce individual biases and improve reliability. The wisdom of crowds principle: multiple imperfect models together often beat any single model.\n\nInstructions:\n1. Run to see 3 individual models compared to ensemble methods\n2. Compare simple majority voting vs confidence-weighted voting\n3. Examine disagreement cases - when models disagree, which is usually right?\n4. Optionally add a 4th model and performance-weighted voting\n5. Determine if the ensemble beats the best individual model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WD4KOy8jnzyq",
    "outputId": "765b12ba-118e-4469-d817-4e013993bf58",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "from sentence_transformers import SentenceTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score, accuracy_score\nimport numpy as np\n\nprint(\"Ensemble CLASSIFIER Comparison\")\nprint(\"=\"*60)\n\n# Load 3 different models\nmodel1 = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\nmodel2 = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\nmodel3 = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L3-v2')\n\nmodels = [model1, model2, model3]\nmodel_names = [\"MPNet\", \"MiniLM-L6\", \"MiniLM-L3\"]\n\nprint(f\"Using {len(models)} models:\\n\")\nfor name in model_names:\n    print(f\"  - {name}\")\n\n# Train each model\ntrain_subset = data[\"train\"].shuffle(seed=42).select(range(1000))\ntest_subset = data[\"test\"].shuffle(seed=42).select(range(200))\n\nprint(f\"\\nTraining on {len(train_subset)} samples...\")\nprint(\"Note: In practice, ensemble diversity comes from:\")\nprint(\"  - Different model architectures\")\nprint(\"  - Different training data subsets\")\nprint(\"  - Different hyperparameters\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7NXodRdVtQUb"
   },
   "source": [
    "Train each model individually and track performance:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2Z7qnySgtQUc",
    "outputId": "c9b22b11-112b-44c6-a7d2-6b3f74d23bb1",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": "classifiers = []\nindividual_f1s = []\n\nprint(\"Training individual models...\\n\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iLXRUDIJtQUd",
    "outputId": "56e6a8ca-a120-42b6-bdda-4c5ea4e08a18",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": "for i, (model, name) in enumerate(zip(models, model_names)):\n    # Encode with this model\n    train_emb = model.encode(train_subset[\"text\"], show_progress_bar=False)\n    test_emb = model.encode(test_subset[\"text\"], show_progress_bar=False)\n\n    # Train classifier\n    clf = LogisticRegression(random_state=42, max_iter=1000)\n    clf.fit(train_emb, train_subset[\"label\"])\n\n    # Evaluate\n    y_pred = clf.predict(test_emb)\n    f1 = f1_score(test_subset[\"label\"], y_pred, average='weighted')\n\n    classifiers.append((clf, test_emb))\n    individual_f1s.append(f1)\n\n    print(f\"{name:12s}: F1 = {f1:.4f}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbGPrIsktQUd"
   },
   "source": [
    "Collect predictions from all models for ensemble voting:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Aj9MEcB3tQUf",
    "outputId": "d84b05a2-8a4c-4526-a854-e9a368a7222e",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": "all_predictions = []\n\nfor clf, test_emb in classifiers:\n    preds = clf.predict(test_emb)\n    all_predictions.append(preds)\n\nall_predictions = np.array(all_predictions)\nprint(f\"\\nCollected predictions from {len(all_predictions)} models\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8WbCrwltQUf"
   },
   "source": [
    "Apply majority voting to get ensemble predictions:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "X7ieksGgtQUf",
    "outputId": "6ec5ffed-d1a8-4c06-dada-3f6f97e47bfb",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": "ensemble_predictions = []\n\nfor i in range(len(test_subset)):\n    votes = all_predictions[:, i]\n    unique, counts = np.unique(votes, return_counts=True)\n    majority = unique[np.argmax(counts)]  # Pick most common vote\n    ensemble_predictions.append(majority)\n\nensemble_f1 = f1_score(test_subset[\"label\"], ensemble_predictions, average='weighted')\nprint(f\"Ensemble F1: {ensemble_f1:.4f}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OcnCWwR0tQUf"
   },
   "source": [
    "Compare ensemble vs individual models:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_oOfUqkJtQUg",
    "outputId": "7f321d64-461a-4e30-9e45-95831624f3e3",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "print(\"\\nResults Comparison\")\nprint(\"-\" * 40)\n\nfor name, f1 in zip(model_names, individual_f1s):\n    print(f\"{name:12s}: {f1:.4f}\")\n\nprint(f\"\\nEnsemble:     {ensemble_f1:.4f}\")\n\nbest_individual = max(individual_f1s)\nif ensemble_f1 > best_individual:\n    gain = ensemble_f1 - best_individual\n    print(f\"\\n\u2713 Ensemble wins by {gain:.4f}!\")\nelse:\n    diff = best_individual - ensemble_f1\n    print(f\"\\nBest individual model wins by {diff:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try implementing confidence-weighted voting where each model's vote is weighted by its prediction confidence:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Complete this: Confidence-weighted ensemble\n# Instead of simple majority voting, weight each vote by confidence\n\n# Get prediction probabilities from all models\nall_probabilities = []\n\nfor clf, test_emb in classifiers:\n    # Hint: Use predict_proba() to get probabilities instead of predict()\n    probs = None  # get probabilities for each prediction\n    all_probabilities.append(probs)\n\nall_probabilities = np.array(all_probabilities) if all_probabilities[0] is not None else None\n\n# Calculate weighted predictions\nweighted_predictions = []\n\nfor i in range(len(test_subset)):\n    # For each sample, sum the confidence-weighted votes\n    # Hint: For each class, sum probabilities across all models using all_probabilities[:, i, :]\n    # Then use np.sum() along axis 0 to get scores for each class\n    class_scores = None  # calculate weighted scores for each class\n    \n    # Pick class with highest weighted score\n    # Hint: Use np.argmax()\n    weighted_pred = None  # argmax of class_scores\n    weighted_predictions.append(weighted_pred if weighted_pred is not None else 0)\n\nweighted_f1 = f1_score(test_subset[\"label\"], weighted_predictions, average='weighted')\n\nprint(f\"\\nConfidence-weighted ensemble F1: {weighted_f1:.4f}\")\nprint(f\"Simple majority F1: {ensemble_f1:.4f}\")\nprint(f\"Difference: {weighted_f1 - ensemble_f1:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGa8u5Ghnzyq"
   },
   "source": [
    "#### Hard Task 4: Cross-Domain Transfer Learning\n\nCan a classifier trained on movie reviews work on restaurant, product, or book reviews? Observe zero-shot transfer (no adaptation) performance on each domain, see which domains transfer well and which don't, then try few-shot adaptation (adding just 4 examples from target domain). Analyze domain similarity using embedding distances and optionally add your own custom domain to test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhueGby3nzyr"
   },
   "source": [
    "Load dependencies and set up source/target domains:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "574s5OB8nzys"
   },
   "source": "from sentence_transformers import SentenceTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score, accuracy_score\nimport numpy as np",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9m7SiDmknzys"
   },
   "source": [
    "Define source domain (movies) and target domains:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7NsPi1yMnzys"
   },
   "source": "movie_data = load_dataset(\"rotten_tomatoes\")\nmodel = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n\nmovie_train = movie_data[\"train\"].shuffle(seed=42).select(range(2000))\nmovie_test = movie_data[\"test\"].shuffle(seed=42).select(range(200))",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XmFlUawpnzyt"
   },
   "source": "restaurant_reviews = {\n    'text': [\n        \"Amazing food and excellent service!\",\n        \"Best restaurant in town, highly recommend\",\n        \"Delicious meals and great atmosphere\",\n        \"Outstanding cuisine and friendly staff\",\n        \"Terrible food, very disappointing\",\n        \"Awful service and poor quality\",\n        \"Not worth the money, mediocre at best\",\n        \"Disgusting food and rude waiters\",\n        \"The pasta was okay but nothing special\",\n        \"Decent place for a quick meal\"\n    ],\n    'label': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n}\n\nproduct_reviews = {\n    'text': [\n        \"This product is amazing! Works perfectly\",\n        \"Excellent quality, very satisfied\",\n        \"Great value for money, highly recommend\",\n        \"Perfect! Exactly what I needed\",\n        \"Terrible product, broke immediately\",\n        \"Waste of money, very poor quality\",\n        \"Doesn't work as advertised, disappointed\",\n        \"Awful, don't buy this\",\n        \"It's okay, does the job\",\n        \"Average product, nothing special\"\n    ],\n    'label': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n}\n\nbook_reviews = {\n    'text': [\n        \"Brilliant book! Couldn't put it down\",\n        \"Masterfully written, highly engaging\",\n        \"One of the best books I've read\",\n        \"Fantastic story and great characters\",\n        \"Boring and poorly written\",\n        \"Terrible book, waste of time\",\n        \"Disappointing, not worth reading\",\n        \"Awful plot and weak characters\",\n        \"Decent read but nothing groundbreaking\",\n        \"It was fine, not great not terrible\"\n    ],\n    'label': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n}",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJ7yP7KVnzyu"
   },
   "source": [
    "Train classifier on source domain (movies):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NhM4esP1nzyv",
    "outputId": "36fa06d1-1963-470e-f3a0-eddfbdd38902",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102,
     "referenced_widgets": [
      "e9caf6e017e04589936dafdffae40f46",
      "418b51b010ef4f8fa97762d37b330528",
      "eb3bb31177fe4e0fb912ef193d8691c1",
      "233e24a784594a0a8980af6691e60b7d",
      "d19c6fcb37724ceba5a120d4b5a13af0",
      "617815fce9d5410b8b3397d319dc8c9a",
      "a619de040e5040cb99a1e8dc38b022a4",
      "b68bc7e78b9e4440a4ae3b1f980d0e82",
      "78d1bf6648ba4c7380d79b009ffc4c04",
      "7d2af482ad91462a9bec2fdd5d5bf9d7",
      "f52286206367426b915795a27c6f235c"
     ]
    }
   },
   "source": "print(\"Training on movie reviews...\")\ntrain_embeddings = model.encode(movie_train[\"text\"], show_progress_bar=True)\n\nclf = LogisticRegression(random_state=42, max_iter=1000)\nclf.fit(train_embeddings, movie_train[\"label\"])\n\n# Test on source domain\nmovie_test_embeddings = model.encode(movie_test[\"text\"], show_progress_bar=False)\nmovie_test_pred = clf.predict(movie_test_embeddings)\nsource_f1 = f1_score(movie_test[\"label\"], movie_test_pred, average='weighted')\n\nprint(f\"\\nSource domain F1: {source_f1:.4f}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yEr0prtsnzyw"
   },
   "source": [
    "Test zero-shot transfer to target domains (no adaptation):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VPD8PXiZnzyx",
    "outputId": "5125406a-66d0-4620-a723-702ee1e35974",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": "target_domains = {\n    \"Restaurant\": restaurant_reviews,\n    \"Product\": product_reviews,\n    \"Book\": book_reviews,\n}\n\ntransfer_results = {}\n\nprint(\"\\nZero-shot transfer results:\")\nfor domain_name, domain_data in target_domains.items():\n    domain_embeddings = model.encode(domain_data['text'])\n    domain_pred = clf.predict(domain_embeddings)\n    domain_f1 = f1_score(domain_data['label'], domain_pred, average='weighted')\n    domain_acc = accuracy_score(domain_data['label'], domain_pred)\n\n    print(f\"\\n{domain_name}: F1={domain_f1:.4f}, Acc={domain_acc:.4f}\")\n    print(f\"  Drop from source: {source_f1 - domain_f1:.4f}\")\n\n    transfer_results[domain_name] = {\n        'zero_shot_f1': domain_f1,\n        'embeddings': domain_embeddings\n    }",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EEPIxwrLnzyy"
   },
   "source": [
    "Few-shot adaptation (add 4 examples from target domain):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3SRItXyjnzyy",
    "outputId": "2549d5ac-c6e9-469b-f788-5535573b9370",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "adaptation_size = 4\n\n# Experiment: Try different adaptation sizes\n# Uncomment to see how performance changes with more examples:\n# adaptation_size = 2   # Minimal adaptation\n# adaptation_size = 8   # More examples\n# adaptation_size = 16  # Even more examples\n\nprint(f\"\\nFew-shot adaptation ({adaptation_size} examples):\")\nfor domain_name, domain_data in target_domains.items():\n    # Split: first N for adaptation, rest for testing\n    adapt_texts = domain_data['text'][:adaptation_size]\n    adapt_labels = domain_data['label'][:adaptation_size]\n    test_texts = domain_data['text'][adaptation_size:]\n    test_labels = domain_data['label'][adaptation_size:]\n\n    # Combine source + adaptation\n    adapt_embeddings = model.encode(adapt_texts)\n    combined_embeddings = np.vstack([train_embeddings, adapt_embeddings])\n    combined_labels = list(movie_train[\"label\"]) + adapt_labels\n\n    # Retrain\n    clf_adapted = LogisticRegression(random_state=42, max_iter=1000)\n    clf_adapted.fit(combined_embeddings, combined_labels)\n\n    # Test\n    test_embeddings = model.encode(test_texts)\n    adapted_pred = clf_adapted.predict(test_embeddings)\n    adapted_f1 = f1_score(test_labels, adapted_pred, average='weighted')\n\n    zero_shot_f1 = transfer_results[domain_name]['zero_shot_f1']\n    improvement = adapted_f1 - zero_shot_f1\n\n    print(f\"\\n{domain_name}:\")\n    print(f\"  Zero-shot: {zero_shot_f1:.4f}\")\n    print(f\"  Adapted:   {adapted_f1:.4f}\")\n    print(f\"  Gain:      {improvement:+.4f}\")\n\n    transfer_results[domain_name]['adapted_f1'] = adapted_f1\n    transfer_results[domain_name]['improvement'] = improvement"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yc6UDdfinzyz"
   },
   "source": [
    "Analyze domain similarity (embedding distance):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4YMFCMYUnzyz",
    "outputId": "9806a500-49c3-441c-c90b-bf9fdad0c082",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": "source_centroid = np.mean(train_embeddings, axis=0)\n\nprint(\"\\nDomain distances from source:\")\nfor domain_name in target_domains.keys():\n    domain_embeddings = transfer_results[domain_name]['embeddings']\n    domain_centroid = np.mean(domain_embeddings, axis=0)\n    distance = np.linalg.norm(source_centroid - domain_centroid)\n\n    zero_f1 = transfer_results[domain_name]['zero_shot_f1']\n    drop = source_f1 - zero_f1\n\n    print(f\"\\n{domain_name:12s}: distance={distance:.3f}, drop={drop:.3f}\")\n\nprint(\"\\nObservation: Smaller distance = better transfer\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBJkCQOGnzy0"
   },
   "source": [
    "### Questions\n\n1. Which domain transferred best from movies? Which worst?\n\n2. Do you see patterns in what transfers well vs what fails?\n\n3. After few-shot adaptation: Which domains benefited most from just 4 examples?\n"
   ]
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "id": "yZLqm3S4IP2e"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}