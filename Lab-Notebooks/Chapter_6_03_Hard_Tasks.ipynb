{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Prompt Engineering - Hard Tasks\n",
    "\n",
    "This notebook covers advanced reasoning techniques: Tree-of-Thought, chain prompting, output verification, and multi-stage reasoning with self-reflection.\n",
    "\n",
    "**Note on Code Organization**: In previous notebooks, we sometimes wrote prompts directly. Here we use functions extensively because:\n",
    "- **Reusability**: Call the same logic multiple times without copying code\n",
    "- **Modularity**: Each function handles one specific task (generate options, evaluate, verify, etc.)\n",
    "- **Real-world practice**: Production systems always use functions for maintainability\n",
    "- **Testing**: Easy to test individual components independently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run all cells in this section to set up the environment and load the model.\n",
    "\n",
    "Before running these cells, review the concepts from the main Chapter 6 notebook (00_Start_Here.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional] - Installing Packages on Google Colab\n",
    "\n",
    "If you are viewing this notebook on Google Colab, uncomment and run the following code to install dependencies.\n",
    "\n",
    "**Note**: Use a GPU for this notebook. In Google Colab, go to Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install --upgrade transformers>=4.40.0 torch accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "model_path = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=False,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, temperature=0.7, max_tokens=500):\n",
    "    \"\"\"Generate text with specified parameters\"\"\"\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        return_full_text=False,\n",
    "        max_new_tokens=max_tokens,\n",
    "        do_sample=True if temperature > 0 else False,\n",
    "        temperature=temperature if temperature > 0 else None,\n",
    "    )\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    output = pipe(messages)\n",
    "    return output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "\n",
    "Complete the following tasks by implementing the starter code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Level: Hard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About This Task:**\n",
    "Tree-of-Thought generates multiple options at each step and evaluates which paths look most promising. We break this into modular functions instead of one big script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hard Task 1: Tree-of-Thought\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Run baseline CoT to see linear reasoning\n",
    "2. Study how ToT functions work together (generate → evaluate → explore)\n",
    "3. Improve `generate_next_steps` prompt to get better options\n",
    "4. Modify `evaluate_option` to use different criteria (safety vs progress)\n",
    "5. Test on a different strategic problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = \"\"\"You need to transport a fox, a chicken, and grain across a river.\n",
    "Your boat can only carry you and one item.\n",
    "If left alone: fox eats chicken, chicken eats grain.\n",
    "How do you get everything across safely?\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: Linear CoT\n",
    "\n",
    "This is the simple approach - ask the model to think step-by-step in one shot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_cot_solve(problem):\n",
    "    \"\"\"Simple linear chain-of-thought - one path, no exploration\"\"\"\n",
    "    prompt = f\"{problem}\\n\\nLet's think step-by-step:\"\n",
    "    return generate_text(prompt, temperature=0, max_tokens=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear CoT:\n",
      " 1. Take the chicken across the river first and leave it on the other side.\n",
      "2. Go back and take the fox across the river.\n",
      "3. Leave the fox on the other side, but take the chicken back with you.\n",
      "4. Leave the chicken on the starting side and take the grain across the river.\n",
      "5. Leave the grain with the fox on the other side and go back to get the chicken.\n",
      "6. Finally, take the chicken across the river.\n",
      "\n",
      "Now, all three items (fox, chicken, and grain) are safely on the other side of the river.\n"
     ]
    }
   ],
   "source": [
    "print(\"Linear CoT:\")\n",
    "solution_cot = linear_cot_solve(problem)\n",
    "print(solution_cot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how linear CoT might miss better solutions or get stuck. Now we'll use Tree-of-Thought to explore multiple paths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree-of-Thought Implementation\n",
    "\n",
    "We break ToT into separate functions. Each function has a single responsibility - this is how real systems are built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function 1: Generate Options**\n",
    "\n",
    "Your task: Improve this prompt to generate more specific, actionable next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_steps(current_state, problem, num_options=3):\n",
    "    \"\"\"\n",
    "    Generate possible next moves from current state.\n",
    "    \n",
    "    This function uses a prompt to brainstorm options. In real systems,\n",
    "    this allows you to:\n",
    "    - Call it repeatedly at each decision point\n",
    "    - Adjust num_options based on problem complexity\n",
    "    - Cache and reuse results for similar states\n",
    "    \n",
    "    Args:\n",
    "        current_state: Description of where we are now\n",
    "        problem: The full problem description for context\n",
    "        num_options: How many alternatives to generate\n",
    "    \n",
    "    Returns:\n",
    "        List of possible next actions\n",
    "    \"\"\"\n",
    "    # Improve this prompt to get better, more specific options\n",
    "    prompt = f\"\"\"{problem}\n",
    "\n",
    "Current situation: {current_state}\n",
    "\n",
    "What are {num_options} possible next moves? List them:\n",
    "1.\"\"\"\n",
    "    \n",
    "    output = generate_text(prompt, temperature=0.7, max_tokens=200)\n",
    "    \n",
    "    # Parse the output to extract individual options\n",
    "    # This is why functions are useful - parsing logic is isolated here\n",
    "    lines = output.strip().split('\\n')\n",
    "    options = []\n",
    "    \n",
    "    for line in lines[:num_options]:\n",
    "        clean = line.strip()\n",
    "        # Remove common prefixes like \"1.\", \"2.\", \"-\", etc.\n",
    "        for prefix in ['1.', '2.', '3.', '4.', '-', '•']:\n",
    "            if clean.startswith(prefix):\n",
    "                clean = clean[len(prefix):].strip()\n",
    "        if clean:\n",
    "            options.append(clean)\n",
    "    \n",
    "    return options[:num_options]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function 2: Evaluate Options**\n",
    "\n",
    "This function scores how good each option is. Notice how we can easily swap evaluation criteria by modifying just this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_option(option, problem, criterion=\"progress\"):\n",
    "    \"\"\"\n",
    "    Score how promising this option is (0-10).\n",
    "    \n",
    "    Using a function here means we can:\n",
    "    - Change evaluation criteria without touching tree exploration code\n",
    "    - Test different scoring strategies independently  \n",
    "    - Add logging or caching if needed\n",
    "    \n",
    "    Args:\n",
    "        option: The action to evaluate\n",
    "        problem: Full problem for context\n",
    "        criterion: What to optimize for (\"progress\" or \"safety\")\n",
    "    \n",
    "    Returns:\n",
    "        Score from 0-10\n",
    "    \"\"\"\n",
    "    if criterion == \"safety\":\n",
    "        # Your task: Write a prompt that focuses on safety\n",
    "        prompt = f\"\"\"{problem}\n",
    "\n",
    "Proposed move: {option}\n",
    "\n",
    "On a scale of 0-10, how SAFE is this move?\n",
    "Consider: Does it prevent anything from being eaten?\n",
    "\n",
    "Score (0-10):\"\"\"\n",
    "    else:\n",
    "        # Default: Focus on progress\n",
    "        prompt = f\"\"\"{problem}\n",
    "\n",
    "Proposed move: {option}\n",
    "\n",
    "On a scale of 0-10, how promising is this move?\n",
    "Consider: Does it make progress? Does it violate constraints?\n",
    "\n",
    "Score (0-10):\"\"\"\n",
    "    \n",
    "    output = generate_text(prompt, temperature=0, max_tokens=50)\n",
    "    \n",
    "    # Extract score from text - regex makes this robust to different formats\n",
    "    match = re.search(r'(\\d+)', output)\n",
    "    if match:\n",
    "        score = int(match.group(1))\n",
    "        return min(max(score, 0), 10)  # Clamp to 0-10 range\n",
    "    return 5  # Default middle score if parsing fails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Structure: Tree Node**\n",
    "\n",
    "We use a class to represent each decision point. This keeps state organized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    \"\"\"\n",
    "    Represents one state in the decision tree.\n",
    "    \n",
    "    Using a class instead of just variables makes it easy to:\n",
    "    - Track parent-child relationships\n",
    "    - Reconstruct the path to any node\n",
    "    - Store metadata like scores\n",
    "    \"\"\"\n",
    "    def __init__(self, state, action=None, parent=None):\n",
    "        self.state = state          # Current situation description\n",
    "        self.action = action        # What action led here\n",
    "        self.parent = parent        # Previous node (for path reconstruction)\n",
    "        self.children = []          # Possible next nodes\n",
    "        self.score = 0              # Evaluation score\n",
    "    \n",
    "    def get_path(self):\n",
    "        \"\"\"Reconstruct the sequence of actions from root to this node\"\"\"\n",
    "        path = []\n",
    "        node = self\n",
    "        # Walk backwards through parent links\n",
    "        while node.parent:\n",
    "            path.append(node.action)\n",
    "            node = node.parent\n",
    "        return list(reversed(path))  # Reverse to get root-to-leaf order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main Algorithm: Tree Search**\n",
    "\n",
    "This orchestrates the generate → evaluate → explore loop. Notice how clean it is because we separated concerns into functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_of_thought_solve(problem, max_depth=3, branch_factor=2, criterion=\"progress\"):\n",
    "    \"\"\"\n",
    "    Explore multiple reasoning paths to find the best solution.\n",
    "    \n",
    "    This function shows why modular design matters:\n",
    "    - We call generate_next_steps() for brainstorming\n",
    "    - We call evaluate_option() for scoring\n",
    "    - We use TreeNode to manage state\n",
    "    - The search logic is separate from prompt engineering\n",
    "    \n",
    "    Args:\n",
    "        problem: Problem to solve\n",
    "        max_depth: How many steps ahead to plan\n",
    "        branch_factor: How many options to consider at each step\n",
    "        criterion: What to optimize (\"progress\" or \"safety\")\n",
    "    \n",
    "    Returns:\n",
    "        Best solution node found\n",
    "    \"\"\"\n",
    "    # Start with initial state\n",
    "    initial_state = \"Everything is on the starting side\"\n",
    "    root = TreeNode(initial_state)\n",
    "    \n",
    "    print(\"Tree-of-Thought exploration:\")\n",
    "    print(f\"Criterion: {criterion}\")\n",
    "    \n",
    "    # Breadth-first search using a queue\n",
    "    queue = deque([(root, 0)])\n",
    "    best_solution = None\n",
    "    best_score = -1\n",
    "    explored_nodes = 0\n",
    "    \n",
    "    # Explore tree level by level\n",
    "    while queue and explored_nodes < 10:  # Limit exploration for demo\n",
    "        node, depth = queue.popleft()\n",
    "        explored_nodes += 1\n",
    "        \n",
    "        print(f\"\\nDepth {depth}, Node {explored_nodes}\")\n",
    "        print(f\"State: {node.state}\")\n",
    "        \n",
    "        # At max depth, evaluate final state\n",
    "        if depth >= max_depth:\n",
    "            score = evaluate_option(node.state, problem, criterion)\n",
    "            node.score = score\n",
    "            print(f\"Final score: {score}/10\")\n",
    "            \n",
    "            # Track best solution\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_solution = node\n",
    "            continue\n",
    "        \n",
    "        # Generate possible next moves - using our function!\n",
    "        next_steps = generate_next_steps(node.state, problem, num_options=branch_factor)\n",
    "        \n",
    "        if next_steps:\n",
    "            print(f\"\\nExploring {len(next_steps)} options:\")\n",
    "            \n",
    "            for i, step in enumerate(next_steps, 1):\n",
    "                # Evaluate each option - using our function!\n",
    "                score = evaluate_option(step, problem, criterion)\n",
    "                print(f\"  {i}. {step} (score: {score}/10)\")\n",
    "                \n",
    "                # Create child node\n",
    "                new_state = f\"{node.state}, then {step}\"\n",
    "                child = TreeNode(new_state, step, node)\n",
    "                child.score = score\n",
    "                node.children.append(child)\n",
    "                \n",
    "                # Pruning: Only explore promising branches (score >= 5)\n",
    "                if score >= 5:\n",
    "                    queue.append((child, depth + 1))\n",
    "                else:\n",
    "                    print(f\"       → Pruned (low score)\")\n",
    "    \n",
    "    # Display best solution found\n",
    "    if best_solution:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"Best solution path:\")\n",
    "        for i, action in enumerate(best_solution.get_path(), 1):\n",
    "            print(f\"{i}. {action}\")\n",
    "        print(f\"\\nFinal score: {best_score}/10\")\n",
    "        return best_solution\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the tree search. Watch how it explores multiple paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree-of-Thought exploration:\n",
      "Criterion: progress\n",
      "\n",
      "Depth 0, Node 1\n",
      "State: Everything is on the starting side\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exploring 2 options:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1. Take the chicken across the river. (score: 0/10)\n",
      "       → Pruned (low score)\n",
      "  2. Return alone to the original side. (score: 0/10)\n",
      "       → Pruned (low score)\n"
     ]
    }
   ],
   "source": [
    "solution_tot = tree_of_thought_solve(problem, max_depth=3, branch_factor=2, criterion=\"progress\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Different Criteria\n",
    "\n",
    "See how easy it is to swap evaluation criteria? Just change one parameter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your task: Run with safety criterion and compare results\n",
    "# solution_safety = tree_of_thought_solve(problem, max_depth=3, branch_factor=2, criterion=\"safety\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1. How many branches were pruned (score < 5)? What does pruning save in terms of LLM calls?\n",
    "\n",
    "2. Compare the solution paths from \"progress\" vs \"safety\" criteria. Did they differ?\n",
    "\n",
    "3. Why is using separate functions better than one big script with all prompts and logic mixed together?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About This Task:**\n",
    "Chain prompting breaks complex tasks into sequential stages. Each stage is a separate function call - this is standard practice in production systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hard Task 2: Chain Prompting\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Run single-prompt baseline to see limitations\n",
    "2. Study the 4-stage chain - notice how each function builds on previous outputs\n",
    "3. Improve Stage 3 prompt to generate better response strategies\n",
    "4. Test what happens if you skip a stage\n",
    "5. Try a different customer review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_review = \"\"\"I've been using your product for 3 months. Setup was confusing\n",
    "and took 2 hours with no clear instructions. Once I figured it out, the features\n",
    "are quite powerful. Performance is good, but the mobile app crashes occasionally.\n",
    "Support was helpful. For $49/month, I expected better documentation.\n",
    "I'm considering upgrading to premium.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: Single Prompt\n",
    "\n",
    "The naive approach - ask the model to do everything at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_prompt_analysis(review):\n",
    "    \"\"\"One-shot analysis - tries to do everything in one call\"\"\"\n",
    "    prompt = f\"\"\"Analyze this customer review and generate a response:\n",
    "\n",
    "Review: {review}\n",
    "\n",
    "Provide:\n",
    "1. Sentiment analysis\n",
    "2. Key issues\n",
    "3. A professional response\n",
    "\"\"\"\n",
    "    return generate_text(prompt, temperature=0, max_tokens=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single prompt approach:\n",
      " 1. Sentiment Analysis:\n",
      "\n",
      "   - The customer has mixed feelings about the product. They are satisfied with the features and performance but are unhappy with the setup process and the mobile app's stability.\n",
      "   - The customer appreciates the support but is disappointed with the documentation.\n",
      "   - The customer is contemplating an upgrade to the premium version, indicating a potential interest in the product despite the issues.\n",
      "\n",
      "2. Key Issues:\n",
      "\n",
      "   - Confusing setup process\n",
      "   - Lack of clear instructions\n",
      "   - Mobile app crashes occasionally\n",
      "   - Inadequate documentation\n",
      "\n",
      "3. Professional Response:\n",
      "\n",
      "   Dear [Customer's Name],\n",
      "\n",
      "   Thank you for taking the time to share your feedback with us. We are glad to hear that you find our product's features and performance to be powerful and that our support team has been helpful to you.\n",
      "\n",
      "   We apologize for the inconvenience you experienced during the setup process. We understand that it can be frustrating when instructions are not clear, and we appreciate your patience. We are continuously working to improve our documentation and will take your feedback into consideration.\n",
      "\n",
      "   We are also sorry to hear about the occasional crashes you've experienced with our mobile app. Our development team is actively working on resolving this issue, and we will ensure that you receive a stable and reliable app in the future.\n",
      "\n",
      "   We value your opinion and would like to offer you a free month of premium service to enhance your experience with our product. If you decide to upgrade, you will have access to additional features and benefits that may address your concerns.\n",
      "\n",
      "   Once again, thank you for your feedback, and we hope to continue serving you with our products.\n",
      "\n",
      "   Best regards,\n",
      "\n",
      "   [Your Name]\n",
      "   [Your Position]\n",
      "   [Company Name]\n"
     ]
    }
   ],
   "source": [
    "print(\"Single prompt approach:\")\n",
    "single_result = single_prompt_analysis(customer_review)\n",
    "print(single_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the single prompt might miss nuances or produce a generic response. Now we'll use a 4-stage chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain Prompting: 4 Specialized Functions\n",
    "\n",
    "Each stage is a focused function. This is how real systems work - specialized components that you can test and improve independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stage 1: Extract Information**\n",
    "\n",
    "First, we pull out facts. This function only does extraction - nothing else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_information(review):\n",
    "    \"\"\"\n",
    "    Extract structured facts from unstructured review text.\n",
    "    \n",
    "    Why separate this?\n",
    "    - Easy to test extraction accuracy independently\n",
    "    - Can reuse extracted facts for multiple analyses\n",
    "    - Can cache results to avoid re-extraction\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Extract key information from this review:\n",
    "\n",
    "Review: {review}\n",
    "\n",
    "List:\n",
    "- Issues mentioned:\n",
    "- Positive aspects:\n",
    "- Duration of usage:\n",
    "- Price mentioned:\n",
    "\"\"\"\n",
    "    return generate_text(prompt, temperature=0, max_tokens=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1: Extraction\n",
      " - Issues mentioned:\n",
      "  - Confusing setup process\n",
      "  - No clear instructions\n",
      "  - Occasional mobile app crashes\n",
      "  - Poor documentation\n",
      "\n",
      "- Positive aspects:\n",
      "  - Powerful features\n",
      "  - Good performance\n",
      "  - Helpful support\n",
      "\n",
      "- Duration of usage:\n",
      "  - 3 months\n",
      "\n",
      "- Price mentioned:\n",
      "  - $49/month\n"
     ]
    }
   ],
   "source": [
    "print(\"Stage 1: Extraction\")\n",
    "stage1_output = extract_information(customer_review)\n",
    "print(stage1_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stage 2: Analyze Sentiment**\n",
    "\n",
    "Now we analyze the extracted facts. See how we pass stage1_output as input?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(extracted_info):\n",
    "    \"\"\"\n",
    "    Determine overall sentiment from extracted facts.\n",
    "    \n",
    "    Benefits of separating:\n",
    "    - Can swap sentiment analysis methods without changing extraction\n",
    "    - Can test sentiment accuracy on known examples\n",
    "    - Clearer what this function is responsible for\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Based on this information:\n",
    "\n",
    "{extracted_info}\n",
    "\n",
    "Determine:\n",
    "1. Overall sentiment (positive/negative/mixed)\n",
    "2. Satisfaction level (1-10)\n",
    "3. Main concerns\n",
    "\"\"\"\n",
    "    return generate_text(prompt, temperature=0, max_tokens=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 2: Sentiment Analysis\n",
      " 1. Overall sentiment: Mixed\n",
      "2. Satisfaction level: 6\n",
      "3. Main concerns:\n",
      "   - Confusing setup process\n",
      "   - No clear instructions\n",
      "   - Occasional mobile app crashes\n",
      "   - Poor documentation\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStage 2: Sentiment Analysis\")\n",
    "stage2_output = analyze_sentiment(stage1_output)\n",
    "print(stage2_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stage 3: Plan Response Strategy**\n",
    "\n",
    "Your task: Improve this prompt to create a more detailed response plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_response_strategy(sentiment_analysis, extracted_info):\n",
    "    \"\"\"\n",
    "    Decide what to say in the response.\n",
    "    \n",
    "    This function takes both sentiment and facts to create a strategy.\n",
    "    Separating strategy planning from actual writing allows:\n",
    "    - Human review of strategy before writing\n",
    "    - A/B testing different strategies\n",
    "    - Consistent application of business rules\n",
    "    \"\"\"\n",
    "    # Improve this prompt to be more specific about strategy\n",
    "    prompt = f\"\"\"Given this sentiment:\n",
    "\n",
    "{sentiment_analysis}\n",
    "\n",
    "And these facts:\n",
    "\n",
    "{extracted_info}\n",
    "\n",
    "Plan the response:\n",
    "- What to acknowledge\n",
    "- What to apologize for\n",
    "- What actions to offer\n",
    "\"\"\"\n",
    "    return generate_text(prompt, temperature=0, max_tokens=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 3: Response Strategy\n",
      " Acknowledgment:\n",
      "\n",
      "Thank you for taking the time to share your experience with our product. We're glad to hear that you've found the powerful features and good performance to be beneficial during your 3-month usage. Our support team has been working hard to assist our customers, and we're pleased to know that you found them helpful.\n",
      "\n",
      "Apology:\n",
      "\n",
      "We sincerely apologize for the issues you've encountered with the confusing setup process, lack of clear instructions, occasional mobile app crashes, and poor documentation. We understand how frustrating these problems can be, and we're committed to making things right.\n",
      "\n",
      "Actions Offered:\n",
      "\n",
      "1. We will work on improving the setup process and providing clearer instructions to ensure a smoother experience for our users.\n",
      "2. Our development team will investigate the cause of the occasional mobile app crashes and implement necessary fixes to prevent them from happening in the future.\n",
      "3. We will review our documentation and update it to be more user-friendly and comprehensive, making it easier for our customers to understand and use our product effectively.\n",
      "4. As a gesture of goodwill, we would\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStage 3: Response Strategy\")\n",
    "stage3_output = plan_response_strategy(stage2_output, stage1_output)\n",
    "print(stage3_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stage 4: Write Final Response**\n",
    "\n",
    "Finally, we execute the strategy to produce the actual response text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_response(strategy):\n",
    "    \"\"\"\n",
    "    Generate the actual response text following the strategy.\n",
    "    \n",
    "    Why separate this?\n",
    "    - Can change tone/style without changing strategy logic\n",
    "    - Can generate multiple versions for A/B testing\n",
    "    - Can add templates or brand voice guidelines here\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Write a professional customer service response following this strategy:\n",
    "\n",
    "{strategy}\n",
    "\n",
    "Tone: Professional, empathetic, solution-focused\n",
    "Length: 2-3 paragraphs\n",
    "\n",
    "Response:\n",
    "\"\"\"\n",
    "    return generate_text(prompt, temperature=0, max_tokens=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 4: Final Response\n",
      " Dear [Customer Name],\n",
      "\n",
      "\n",
      "Thank you for taking the time to share your experience with our product. We're glad to hear that you've found the powerful features and good performance to be beneficial during your 3-month usage. Our support team has been working hard to assist our customers, and we're pleased to know that you found them helpful.\n",
      "\n",
      "\n",
      "We sincerely apologize for the issues you've encountered with the confusing setup process, lack of clear instructions, occasional mobile app crashes, and poor documentation. We understand how frustrating these problems can be, and we're committed to making things right.\n",
      "\n",
      "\n",
      "To address these concerns, we will work on improving the setup process and providing clearer instructions to ensure a smoother experience for our users. Our development team will investigate the cause of the occasional mobile app crashes and implement necessary fixes to prevent them from happening in the future. We will also review our documentation and update it to be more user-friendly and comprehensive, making it easier for our customers to understand and use our product effectively. As a gesture of goodwill, we would like to offer you a complimentary one-month extension to your current subscription.\n",
      "\n",
      "\n",
      "We value your feedback and appreciate your patience as we work to enhance your experience with our product. If you have any further concerns or questions, please don't hesitate to reach out to our support team\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStage 4: Final Response\")\n",
    "stage4_output = write_response(stage3_output)\n",
    "print(stage4_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how each stage built on the previous one? This is modular design - each function has one job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1. What information from Stage 1 was used in Stage 3? What would happen if you skipped Stage 1?\n",
    "\n",
    "2. Which stage added the most value compared to the single-prompt approach?\n",
    "\n",
    "3. Why is it better to have 4 functions instead of copying the prompts 4 times in a row?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About This Task:**\n",
    "Output verification uses one prompt to generate, another to check, and a third to fix. Functions make this generate-verify-correct loop clean and testable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hard Task 3: Output Verification Loop\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Study the 3-function pattern: generate → verify → correct\n",
    "2. Run to see which requirements the model might miss\n",
    "3. Improve the verification prompt to catch more issues\n",
    "4. Test what happens if you run verify → correct twice\n",
    "5. Try different code generation tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "requirements = {\n",
    "    'task': 'Write a function that calculates a discount',\n",
    "    'name': 'calculate_discount',\n",
    "    'parameters': ['price', 'discount_percent'],\n",
    "    'must_have': ['docstring', 'input validation', 'return statement']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function 1: Generate Code**\n",
    "\n",
    "First attempt at generating code from requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_function(requirements):\n",
    "    \"\"\"\n",
    "    Generate code from requirements.\n",
    "    \n",
    "    Why use a function?\n",
    "    - Can be called multiple times for different requirements\n",
    "    - Easy to test with various requirement sets\n",
    "    - Can log all generated code for analysis\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Write a Python function:\n",
    "\n",
    "Task: {requirements['task']}\n",
    "Function name: {requirements['name']}\n",
    "Parameters: {', '.join(requirements['parameters'])}\n",
    "Must include: {', '.join(requirements['must_have'])}\n",
    "\n",
    "Write the complete function:\n",
    "\"\"\"\n",
    "    return generate_text(prompt, temperature=0, max_tokens=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated code:\n",
      " ```python\n",
      "\n",
      "def calculate_discount(price, discount_percent):\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    Calculate the discounted price of an item.\n",
      "\n",
      "\n",
      "    Parameters:\n",
      "\n",
      "    price (float): The original price of the item.\n",
      "\n",
      "    discount_percent (float): The discount percentage to apply to the price.\n",
      "\n",
      "\n",
      "    Returns:\n",
      "\n",
      "    float: The discounted price after applying the discount.\n",
      "\n",
      "\n",
      "    Raises:\n",
      "\n",
      "    ValueError: If the price or discount_percent is negative, or if discount_percent is not between 0 and 100.\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "\n",
      "    # Input validation\n",
      "\n",
      "    if price < 0 or discount_percent < 0:\n",
      "\n",
      "        raise ValueError(\"Price and discount percent must be non-negative.\")\n",
      "\n",
      "    if discount_percent < 0 or discount_percent > 100:\n",
      "\n",
      "        raise ValueError(\"Discount percent must be between 0 and 100.\")\n",
      "\n",
      "\n",
      "    # Calculate the discount amount\n",
      "\n",
      "    discount_amount = (dis\n"
     ]
    }
   ],
   "source": [
    "print(\"Generated code:\")\n",
    "code = generate_function(requirements)\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function 2: Verify Code**\n",
    "\n",
    "Check if the generated code meets all requirements. Your task: Improve this to catch more issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_code(code, requirements):\n",
    "    \"\"\"\n",
    "    Use a prompt to verify if code meets requirements.\n",
    "    \n",
    "    Separating verification into its own function:\n",
    "    - Allows testing verification logic independently\n",
    "    - Can swap between prompt-based and code-based verification\n",
    "    - Makes it easy to add human-in-the-loop approval\n",
    "    \"\"\"\n",
    "    # Improve this prompt to be more thorough in checking\n",
    "    prompt = f\"\"\"Check if this code meets the requirements:\n",
    "\n",
    "Code:\n",
    "{code}\n",
    "\n",
    "Requirements:\n",
    "- Function name: {requirements['name']}\n",
    "- Must have: {', '.join(requirements['must_have'])}\n",
    "\n",
    "List any issues found (or write 'No issues'):\n",
    "\"\"\"\n",
    "    return generate_text(prompt, temperature=0, max_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verification:\n",
      " No issues.\n",
      "\n",
      "\n",
      "The provided code snippet meets the specified requirements. It defines a function named `calculate_discount` with a docstring that explains the purpose, parameters, and return value of the function. The function includes input validation to ensure that the `price` and `discount_percent` are non-negative and that `discount_percent` is within the range of 0 to 100. If the input validation fails, the function raises a `ValueError` with an appropriate message. The function also calculates the discounted price and returns it.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nVerification:\")\n",
    "verification = verify_code(code, requirements)\n",
    "print(verification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function 3: Request Corrections**\n",
    "\n",
    "If verification found issues, ask the model to fix them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_correction(code, verification_feedback):\n",
    "    \"\"\"\n",
    "    Request fixes based on verification results.\n",
    "    \n",
    "    Using a function here:\n",
    "    - Can iterate the verify-correct loop multiple times\n",
    "    - Can add retry logic with exponential backoff\n",
    "    - Can track how many correction rounds were needed\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"This code has issues:\n",
    "\n",
    "Code:\n",
    "{code}\n",
    "\n",
    "Issues found:\n",
    "{verification_feedback}\n",
    "\n",
    "Fix these issues. Provide the complete corrected function:\n",
    "\"\"\"\n",
    "    return generate_text(prompt, temperature=0, max_tokens=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the verify-correct loop. Watch how the functions work together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No issues found!\n"
     ]
    }
   ],
   "source": [
    "if 'no issues' not in verification.lower():\n",
    "    print(\"\\nIssues found - requesting corrections:\")\n",
    "    corrected_code = request_correction(code, verification)\n",
    "    print(corrected_code)\n",
    "    \n",
    "    # Your task: Try verifying again to see if issues were fixed\n",
    "    # second_check = verify_code(corrected_code, requirements)\n",
    "    # print(\"\\nSecond verification:\")\n",
    "    # print(second_check)\n",
    "else:\n",
    "    print(\"\\nNo issues found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1. What issues did verification catch? How would you catch these with code-based checks instead of prompts?\n",
    "\n",
    "2. If you run verify → correct twice, does it improve quality further? When should you stop?\n",
    "\n",
    "3. Why is it useful to have separate verify and correct functions instead of one \"fix_code\" function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About This Task:**\n",
    "Multi-stage reasoning uses a 4-function pipeline: reason → reflect → revise → finalize. Each stage improves on the previous one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hard Task 4: Self-Reflection Pipeline\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Study the 4-stage pipeline and how each function builds on previous outputs\n",
    "2. Run to see how self-reflection catches initial mistakes\n",
    "3. Improve the reflection prompt to ask better critical questions\n",
    "4. Test what happens if you skip the reflection stage\n",
    "5. Try a different decision-making problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = \"\"\"A company is choosing between two pricing strategies:\n",
    "\n",
    "Strategy A: $10/month, expect 10,000 customers\n",
    "Strategy B: $25/month, expect 5,000 customers\n",
    "\n",
    "Customer support costs $2 per customer per month.\n",
    "Which strategy maximizes profit?\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stage 1: Initial Reasoning**\n",
    "\n",
    "First pass at solving the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_reasoning(problem):\n",
    "    \"\"\"\n",
    "    First attempt at solving the problem.\n",
    "    \n",
    "    Why separate this?\n",
    "    - Can compare initial vs final reasoning\n",
    "    - Can measure how often initial reasoning is wrong\n",
    "    - Can use initial reasoning as a baseline for testing\n",
    "    \"\"\"\n",
    "    prompt = f\"{problem}\\n\\nLet's think step-by-step:\"\n",
    "    return generate_text(prompt, temperature=0, max_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1: Initial Reasoning\n",
      " Step 1: Calculate the revenue for each strategy\n",
      "\n",
      "For Strategy A:\n",
      "Revenue = Price per customer * Number of customers\n",
      "Revenue = $10 * 10,000 = $100,000\n",
      "\n",
      "For Strategy B:\n",
      "Revenue = Price per customer * Number of customers\n",
      "Revenue = $25 * 5,000 = $125,000\n",
      "\n",
      "Step 2: Calculate the customer support costs for each strategy\n",
      "\n",
      "For Strategy A:\n",
      "Customer support cost = Cost per customer * Number of customers\n",
      "Customer support cost = $2 * 10,000 = $20,000\n",
      "\n",
      "For Strategy B:\n",
      "Customer support cost = Cost per customer * Number of customers\n",
      "Customer support cost = $2 * 5,000 = $10,000\n",
      "\n",
      "Step 3: Calcul\n"
     ]
    }
   ],
   "source": [
    "print(\"Stage 1: Initial Reasoning\")\n",
    "initial = initial_reasoning(problem)\n",
    "print(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stage 2: Self-Reflection**\n",
    "\n",
    "Your task: Improve this prompt to ask more critical questions about the reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_reflect(problem, initial_reasoning):\n",
    "    \"\"\"\n",
    "    Critically examine the initial reasoning.\n",
    "    \n",
    "    This function demonstrates meta-cognition - thinking about thinking.\n",
    "    Benefits:\n",
    "    - Can test if reflection improves accuracy\n",
    "    - Can tune reflection questions for different problem types\n",
    "    - Can add domain-specific reflection guidelines\n",
    "    \"\"\"\n",
    "    # Improve these critical questions\n",
    "    prompt = f\"\"\"{problem}\n",
    "\n",
    "Here was my initial reasoning:\n",
    "{initial_reasoning}\n",
    "\n",
    "Now critically examine this:\n",
    "- Did I consider all factors?\n",
    "- Are calculations correct?\n",
    "- Did I miss anything?\n",
    "\n",
    "Critical reflection:\n",
    "\"\"\"\n",
    "    return generate_text(prompt, temperature=0, max_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 2: Self-Reflection\n",
      " Your initial reasoning is mostly correct, but there are a few things to consider to ensure a comprehensive analysis.\n",
      "\n",
      "1. Calculate the profit for each strategy:\n",
      "\n",
      "For Strategy A:\n",
      "Profit = Revenue - Customer support cost\n",
      "Profit = $100,000 - $20,000 = $80,000\n",
      "\n",
      "For Strategy B:\n",
      "Profit = Revenue - Customer support cost\n",
      "Profit = $125,000 - $10,000 = $115,000\n",
      "\n",
      "2. Compare the profits:\n",
      "\n",
      "Strategy A profit: $80,000\n",
      "Strategy B profit: $115,000\n",
      "\n",
      "Based on the calculations, Strategy B maximizes profit with a profit of $115,000 compared to Strategy A's profit of $80,\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStage 2: Self-Reflection\")\n",
    "reflection = self_reflect(problem, initial)\n",
    "print(reflection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what the model caught when reviewing its own reasoning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stage 3: Revised Reasoning**\n",
    "\n",
    "Incorporate insights from reflection to improve the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revised_reasoning(problem, initial, reflection):\n",
    "    \"\"\"\n",
    "    Generate improved reasoning based on reflection.\n",
    "    \n",
    "    This function shows the value of iteration:\n",
    "    - Initial attempt → reflection → revision is a powerful pattern\n",
    "    - Keeping them separate means we can inspect each stage\n",
    "    - Can measure improvement from initial to revised\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"{problem}\n",
    "\n",
    "My initial reasoning:\n",
    "{initial}\n",
    "\n",
    "After reflection:\n",
    "{reflection}\n",
    "\n",
    "Now provide improved reasoning that addresses the concerns:\n",
    "\"\"\"\n",
    "    return generate_text(prompt, temperature=0, max_tokens=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 3: Revised Reasoning\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 7.62 GiB of which 9.25 MiB is free. Including non-PyTorch memory, this process has 7.59 GiB memory in use. Of the allocated memory 7.44 GiB is allocated by PyTorch, and 30.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStage 3: Revised Reasoning\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m revised \u001b[38;5;241m=\u001b[39m \u001b[43mrevised_reasoning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproblem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreflection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(revised)\n",
      "Cell \u001b[0;32mIn[38], line 20\u001b[0m, in \u001b[0;36mrevised_reasoning\u001b[0;34m(problem, initial, reflection)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    Generate improved reasoning based on reflection.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m    - Can measure improvement from initial to revised\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproblem\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;124mMy initial reasoning:\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124mNow provide improved reasoning that addresses the concerns:\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgenerate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m, in \u001b[0;36mgenerate_text\u001b[0;34m(prompt, temperature, max_tokens)\u001b[0m\n\u001b[1;32m      3\u001b[0m pipe \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     temperature\u001b[38;5;241m=\u001b[39mtemperature \u001b[38;5;28;01mif\u001b[39;00m temperature \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m messages \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}]\n\u001b[0;32m---> 14\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/ADS525/ads525/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:325\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first_item, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mdict\u001b[39m)):\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;66;03m# We have one or more prompts in list-of-dicts format, so this is chat mode\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first_item, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 325\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mChat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    327\u001b[0m         chats \u001b[38;5;241m=\u001b[39m (Chat(chat) \u001b[38;5;28;01mfor\u001b[39;00m chat \u001b[38;5;129;01min\u001b[39;00m text_inputs)  \u001b[38;5;66;03m# 🐈 🐈 🐈\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/ADS525/ads525/lib/python3.10/site-packages/transformers/pipelines/base.py:1467\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1459\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1460\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1461\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1464\u001b[0m         )\n\u001b[1;32m   1465\u001b[0m     )\n\u001b[1;32m   1466\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ADS525/ads525/lib/python3.10/site-packages/transformers/pipelines/base.py:1474\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1472\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1473\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1474\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1475\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1476\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/Desktop/ADS525/ads525/lib/python3.10/site-packages/transformers/pipelines/base.py:1374\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1372\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1373\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1374\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1375\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/ADS525/ads525/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:432\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[1;32m    430\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[0;32m--> 432\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[1;32m    435\u001b[0m     generated_sequence \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msequences\n",
      "File \u001b[0;32m~/Desktop/ADS525/ads525/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ADS525/ads525/lib/python3.10/site-packages/transformers/generation/utils.py:2564\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2561\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39muse_cache\n\u001b[1;32m   2563\u001b[0m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[0;32m-> 2564\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2565\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2566\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2570\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2571\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2572\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2574\u001b[0m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[1;32m   2575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2576\u001b[0m     generation_config\u001b[38;5;241m.\u001b[39mreturn_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2577\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2578\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result\u001b[38;5;241m.\u001b[39mpast_key_values, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_legacy_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2579\u001b[0m ):\n",
      "File \u001b[0;32m~/Desktop/ADS525/ads525/lib/python3.10/site-packages/transformers/generation/utils.py:2787\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2785\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2786\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2787\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2789\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   2790\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   2791\u001b[0m     outputs,\n\u001b[1;32m   2792\u001b[0m     model_kwargs,\n\u001b[1;32m   2793\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2794\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/ADS525/ads525/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ADS525/ads525/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/ADS525/ads525/lib/python3.10/site-packages/transformers/utils/generic.py:918\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict_passed\n\u001b[0;32m--> 918\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    920\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/Desktop/ADS525/ads525/lib/python3.10/site-packages/transformers/models/phi3/modeling_phi3.py:465\u001b[0m, in \u001b[0;36mPhi3ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Unpack[TransformersKwargs],\n\u001b[1;32m    447\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CausalLMOutputWithPast:\n\u001b[1;32m    448\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;124;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;124;03m    ```\"\"\"\u001b[39;00m\n\u001b[0;32m--> 465\u001b[0m     outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    476\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/ADS525/ads525/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ADS525/ads525/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/ADS525/ads525/lib/python3.10/site-packages/transformers/utils/generic.py:1064\u001b[0m, in \u001b[0;36mcheck_model_inputs.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1061\u001b[0m                 monkey_patched_layers\u001b[38;5;241m.\u001b[39mappend((module, original_forward))\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1064\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m     kwargs_without_recordable \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "File \u001b[0;32m~/Desktop/ADS525/ads525/lib/python3.10/site-packages/transformers/models/phi3/modeling_phi3.py:401\u001b[0m, in \u001b[0;36mPhi3Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb(hidden_states, position_ids)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers]:\n\u001b[0;32m--> 401\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(hidden_states)\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[1;32m    413\u001b[0m     last_hidden_state\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    414\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    415\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/ADS525/ads525/lib/python3.10/site-packages/transformers/modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ADS525/ads525/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ADS525/ads525/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/ADS525/ads525/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ADS525/ads525/lib/python3.10/site-packages/transformers/models/phi3/modeling_phi3.py:263\u001b[0m, in \u001b[0;36mPhi3DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    261\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[0;32m--> 263\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresid_attn_dropout(hidden_states)  \u001b[38;5;66;03m# main diff with Llama\u001b[39;00m\n\u001b[1;32m    275\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/Desktop/ADS525/ads525/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ADS525/ads525/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/ADS525/ads525/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ADS525/ads525/lib/python3.10/site-packages/transformers/models/phi3/modeling_phi3.py:192\u001b[0m, in \u001b[0;36mPhi3Attention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;66;03m# sin and cos are specific to RoPE models; cache_position needed for the static cache\u001b[39;00m\n\u001b[1;32m    191\u001b[0m     cache_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msin\u001b[39m\u001b[38;5;124m\"\u001b[39m: sin, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcos\u001b[39m\u001b[38;5;124m\"\u001b[39m: cos, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_position\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_position}\n\u001b[0;32m--> 192\u001b[0m     key_states, value_states \u001b[38;5;241m=\u001b[39m \u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m attention_interface: Callable \u001b[38;5;241m=\u001b[39m eager_attention_forward\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meager\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/ADS525/ads525/lib/python3.10/site-packages/transformers/cache_utils.py:776\u001b[0m, in \u001b[0;36mCache.update\u001b[0;34m(self, key_states, value_states, layer_idx, cache_kwargs)\u001b[0m\n\u001b[1;32m    773\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdefault_stream(key_states\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mwait_stream(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprefetch_stream)\n\u001b[1;32m    774\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprefetch(layer_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39monly_non_sliding)\n\u001b[0;32m--> 776\u001b[0m keys, values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffloading:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffload(layer_idx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39monly_non_sliding)\n",
      "File \u001b[0;32m~/Desktop/ADS525/ads525/lib/python3.10/site-packages/transformers/cache_utils.py:205\u001b[0m, in \u001b[0;36mDynamicSlidingWindowLayer.update\u001b[0;34m(self, key_states, value_states, cache_kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# Compute the full states\u001b[39;00m\n\u001b[1;32m    204\u001b[0m full_key_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys, key_states], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 205\u001b[0m full_value_states \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# Only cache the last `self.sliding_window - 1` tokens (or all of them if lower than that)\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys \u001b[38;5;241m=\u001b[39m full_key_states[:, :, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msliding_window \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m :, :]\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 7.62 GiB of which 9.25 MiB is free. Including non-PyTorch memory, this process has 7.59 GiB memory in use. Of the allocated memory 7.44 GiB is allocated by PyTorch, and 30.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "print(\"\\nStage 3: Revised Reasoning\")\n",
    "revised = revised_reasoning(problem, initial, reflection)\n",
    "print(revised)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stage 4: Final Answer with Confidence**\n",
    "\n",
    "Produce a clean final answer with confidence assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_answer_with_confidence(problem, revised_reasoning):\n",
    "    \"\"\"\n",
    "    Extract final answer and assess confidence.\n",
    "    \n",
    "    Separating this allows:\n",
    "    - Consistent answer formatting across problems\n",
    "    - Tracking confidence scores over time\n",
    "    - Filtering low-confidence answers for human review\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"{problem}\n",
    "\n",
    "After careful analysis:\n",
    "{revised_reasoning}\n",
    "\n",
    "Provide:\n",
    "1. Final answer (which strategy?)\n",
    "2. Key reasoning (2-3 sentences)\n",
    "3. Confidence level (0-100%)\n",
    "4. Main uncertainty\n",
    "\"\"\"\n",
    "    return generate_text(prompt, temperature=0, max_tokens=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStage 4: Final Answer\")\n",
    "final = final_answer_with_confidence(problem, revised)\n",
    "print(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the 4-stage pipeline produced a well-reasoned, confident answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare: With vs Without Reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your task: Run without reflection stage to see the difference\n",
    "# skipped_reflection = final_answer_with_confidence(problem, initial)\n",
    "# print(\"\\nWithout reflection stage:\")\n",
    "# print(skipped_reflection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1. What mistake did self-reflection catch that initial reasoning missed?\n",
    "\n",
    "2. Compare the final answer with vs without the reflection stage. Was reflection worth the extra LLM call?\n",
    "\n",
    "3. Why is having 4 separate functions better than one big function that does all stages? List 3 specific advantages."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ads525",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
